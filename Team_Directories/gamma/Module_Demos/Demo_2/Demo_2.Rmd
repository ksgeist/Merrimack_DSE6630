---
title: "Demo 2: Public Health & Epidemiology"
subtitle: "Merrimack College DSE6630: Healthcare & Life Sciences Analytics"
author: "Team Gamma, Collins Njagi"
date: "05 june 2025"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    fig_crop: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      cache = FALSE,
                      cache.comments = FALSE,
                      size = 13)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Turn off scientific notation
options(scipen=999)

# Set seed
set.seed(98501)


# Clean, set up, and load
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))

pacman::p_load(tidyverse,
               maps,
               zipcodeR,
               ggplot2, 
               RColorBrewer,
               gridExtra,
               ggrepel,
               kableExtra,
               e1071,
               moments,
               tmap,
               sf,
               spdep,
               spData,
               spatialreg,
               gtsummary
)

skittles <- c("#314070","#627fde", "#a83448", "#075557", "#38878a", "#cb3200", "#db6f4b","#45012d","#b27891", "#213b20", "#3a693a")
```

# Introduction: Epidemiology of pneumonia-related hospital readmissions

One thing that we largely glossed over as you worked through the hospital readmissions data was that there was actually a strong __spatial component__. Yes, we address state using frequency encoding, but we actually could have chosen to focus more heavily on these spatial components. As data scientists of public health and epidemiology, we likely would not have chosen to ignore it - and in fact one of the __most powerful__ tools in our toolkit for analysis are maps. 

So, in this demo, we are going to take advantage of the fact that there is a lot of spatial data in the pneumonia-related hospital readmissions dataset that we *could* use: _address_, _county_, _state_, _zip code_... wow! We will first on the __state level__ data together and then we will take a more granular look using county-level data. We will also discuss __spatial statistics__ we would calculate and even include in our machine learning models if desired!

# 1 State-level Analysis of Pneuomonia-related Hospital Readmissions {#section1}

## 1.1 Make a base map 

Let's practice bring up a very simple little map of the US from `maps` and `ggplot2`. The `maps` package provides latitude and longitude data for various in the package. The vignette for the package can be found [here](https://cran.r-project.org/web/packages/maps/maps.pdf) if you'd more information on the package. Although we will do something much more complicated for our Project 2, for this little exploration let's just keep it simple by taking advantage of everything that is pre-loaded in `maps`.

First, we need to extract state-level geographic information using the `map_data()` function. Similar functions exist for other geographic levels of data, including world (country), county, or other regions of the world, but we will be using `state`. 

```{r}
states <- map_data("state")
```

```{r, echo = FALSE}
head(states) %>% 
  kable(
    format = "html",
    caption = "Table 1. The first 6 rows of the states dataframe") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
  )
```

#### The 6 columns in the `states` variables are:

1. `long` = **longitude** in unprojected geographic coordinates (i.e., longitude and latitude on a sphere or ellipsoid like WGS84); ranges from -180 to 180

2. `lat` = **latitude** in unprojected geographic coordinates (i.e., longitude and latitude on a sphere or ellipsoid like WGS84); ranges from -90 to 90

3. `group` = grouping variable for polygon boundaries where all points in a given group represent a single polygon, like parts of a country, state or county. E.g., here Alabama is treated as a single group (labeled as 1 because its the first state alphabetically

4. `order` = indicates the order in which points should be connected to draw the polygon; helps when connecting `long` and `lat` coordinates to ensure the shape is drawn in the correct sequence

5. `region` = represents the primary geographic unit requested, e.g., here, we requested "state" so the region is takes the state names as labels

6. `subregion` = refers to a subdivision within a region, but is only present for detailed maps like "county" not "state", hence we have no subregion here

#### **Question 1A**: [0.5 points]

Now, let's make a base map leveraging the `ggplot2` package using the `states` dataset. Notice that `x` will always be longitude, `y` will always be latitude, and that we must also pass it a grouping variable. Why do you think we need the `group` variable here?

```{r, echo=FALSE}
                ## x = longitude, y = latitude, group must be set!
ggplot(states, aes(x = long, y = lat, group = group)) + 
  ## geom_polygon draws the states 
  geom_polygon(color = "darkgray",    ## border color
               fill = skittles[1], ## inside of polygon color
               size = 0.2, alpha = 1) +
  ## Makes a completely blank map background
  theme(
    panel.grid = element_blank(),
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white"),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank()
  ) +
  ## Sets the aspect ratio to roughly match US map proportions
  coord_fixed(1.3) +
  ## Creates a title
  ggtitle("A beautiful, blank map of the continental United States")
```

Great, we've made our first map of the continental US! It's pretty uninteresting just as-is, so let's use this map to overlay our pneumonia-related hospital readmissions data.

## 1.2 Prepare the pneumonia-related hospital readmissions data

### 1.2.1 Bring in the hospital readmissions data

```{r}
load ("C:/data_science/DSE6003OM_SP2025R1/Merrimack_DSE6630/Team_Directories/gamma/pneumoniaFull.Rdata")  #("../../I. Biomedical & Clinical Informatics/Demo_1/pneumoniaFull.Rdata")
## Let's change some key data types as well:
pneumoniaFull <- pneumoniaFull %>% 
  mutate_at(c("Score_Death rate for pneumonia patients", 
              "Score_Medicare spending per patient", 
              "Score_Hospital return days for pneumonia patients"), 
            as.numeric)
summary(pneumoniaFull)
```

Notice that I brought in our **fully merged, but otherwise unmodified (non-encoded, non-transformed)** dataset from Demo 1 called `pneumoniaAnalyzeNoEncoding2024.Rdata`!

#### **Question 1B**: [0.5 points]

Why didn't I choose to bring in the fully-processed dataset from Project 1? What does this suggest about our choice of data for spatial analyses versus machine learning models like elastic net?

**Hint**: What information was deleted and/or changes were made downstream that would make those later datasets poor choices for spatial analysis?

> Your answer here. By Collins
Raw data is best preferred for Spatial analaysis as it preserves spatial relationships and patterns such as address, citytown, and zipcode features we had deleted earlier. On the other hand machine learning models like Elastic Net require structured and cleaned data to learn from.

### 1.2.2 Make sure the geographic levels match

Now, take a look at the names of the states in the `states` dataset (from the `region`) column and compare that with the names of the states in our dataset. How do they differ? *You may choose to answer this with code or by otherwise visually inspecting the dataset.*

#### **Question 2A**: [0.25 points]

```{r}
# Your code here. 
str(states$region)
str(pneumoniaFull$State)
```

> Your answer here.By Collins
states in pneumoniafull dataset is in abbreviation while region (state) in state dataset is in full name


I am sure you've now realized that before we can merge them with the state data that we extracted for the map we must make the state names match. This is because `State` in our pneumonia-related readmissions data is an abbreviation (e.g., "AL") but it's the name of the state in the `map_data` ("alabama"). **How can we fix that?**

#### **Question 2B**: [0.75 points]

Explore how you can use the `state.fips` packaged dataset from the `maps` package to do the conversion for you.

Here are the first 6 rows of the the `state.fips` dataset to get you started.

```{r, echo=FALSE}
head(state.fips) %>% 
  kable(
    format = "html",
    caption = "Table 2. The first 6 rows of the state.fips dataframe") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
  )
```

Now, write code that would allow you to replace the two-letter abbreviation for `State` in the `pneumoniaFull` dataset with the full-name of the state in `states$region` to allow merging.

```{r}
# Your code here.
state_map <- setNames(state.fips$polyname, state.fips$abb) #this code creates a named vector state_map, where the names are state abb and the values are the state full name
pneumoniaFull$State <- state_map[pneumoniaFull$State] # we use the vector state_map to replace the abb in the pneumoniafull dataset with full names

unique(pneumoniaFull$State) #double check the changes
```

Sadly, sometimes this can fall short. For example, take a look at the state of Washington:
```{r, echo=FALSE}
state.fips %>% 
  filter(abb == "WA") %>% 
  kable(
    format = "html",
    caption = "Table 3. The state.fips dataframe for just Washington state") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
  )
```

States with islands, like WA state or NY, can have additional mappings that we'd miss if we weren't aware of this issue.

#### **Question 3**: [1 point]

What could we do to `state.fips` to fix this problem? Would your solution become tedious if, for example, we were doing counties instead?

> Your answer here.By Collins
I think to address this problem, we will need to modify state.fips to include these island mappings. If we were to do this solution when dealing with counties, our solution could be tedious because there would be many more counties to account for.

An alternative solution (and not the only one by any means) could be to write a function to do the mapping ourselves. It's easier in this particular case to write our own little function, I think. Notice also that I am choosing to name the new column in `pneumoniaFull.Rdata` as `region`; this is to match the column name in the `states` dataframe that we use for mapping so we can merge the tables more easily.

```{r, echo = TRUE}
## Use this to see which states we have in our dataset, if needed
# unique(states$region)

## Set up a lookup table that goes abbr = state_name
stateNames <- c(
  "AL" = "alabama", "AK" = "alaska", "AZ" = "arizona", "AR" = "arkansas", 
  "CA" = "california", "CO" = "colorado", "CT" = "connecticut", 
  "DC" = "district of columbia", "DE" = "delaware", "FL" = "florida", 
  "GA" = "georgia", "HI" = "hawaii", "ID" = "idaho", "IL" = "illinois", 
  "IN" = "indiana", "IA" = "iowa", "KS" = "kansas", "KY" = "kentucky", 
  "LA" = "louisiana", "ME" = "maine", "MD" = "maryland", "MA" = "massachusetts", 
  "MI" = "michigan", "MN" = "minnesota", "MS" = "mississippi", "MO" = "missouri", 
  "MT" = "montana", "NE" = "nebraska", "NV" = "nevada", "NH" = "new hampshire", 
  "NJ" = "new jersey", "NM" = "new mexico", "NY" = "new york", 
  "NC" = "north carolina", "ND" = "north dakota", "OH" = "ohio", 
  "OK" = "oklahoma", "OR" = "oregon", "PA" = "pennsylvania", 
  "RI" = "rhode island", "SC" = "south carolina", "SD" = "south dakota", 
  "TN" = "tennessee", "TX" = "texas", "UT" = "utah", "VT" = "vermont", 
  "VA" = "virginia", "WA" = "washington", "WV" = "west virginia",
  "WI" = "wisconsin", "WY" = "wyoming")

## Function to look up the state name based on abbreviation
getStateName <- function(abbr) {
  stateNames[abbr]
}
```

#### **Question 4**: [1 point]

Show that you understand how the `getStateName` works by using it to replace the abbreviation with the statename into a new column you call `feature`(**I think you meant region**) in the `pneumoniaFull` dataframe.

**HINT**: You can use the `mutate()` function or you could just use the base `R` way, either one!

```{r}
# Your code here.
state_map_revert <- setNames(state.fips$abb, state.fips$polyname) #creating a new vector to convert full names into abbreviation as it was originally
pneumoniaFull$State <- state_map_revert[pneumoniaFull$State] #First to demonstrate this i have return pneumonia$state into abbr as it was

pneumoniaFull <- pneumoniaFull %>%
  mutate(region = sapply(State, getStateName))

unique(pneumoniaFull$region) #checking if feature variable is fullname
```

### 1.2.3 Aggregate `PredictedReadmissionsRate` by state

Now, before we add the pneumonia-related hospital readmissions to the map, we need to **aggregate** it at the state-level. We will want to do that by aggregating across states and taking the `mean()` and `median()`.

#### **Question 5**: [1 point]

Start from my code and finish the aggregation by state for the `mean()` and `median()` by filling in the blanks. Notice that we are making a separate dataset called `stateAggPneumonia`. Make sure to uncomment the code when you're ready to run it.

```{r}
 stateAggPneumonia <- pneumoniaFull %>%
   ## Grab readmissions and region
   select(PredictedReadmissionRate, region,
          `Score_Death rate for pneumonia patients`, 
          `Score_Hospital return days for pneumonia patients`, 
          `Score_Medicare spending per patient`) %>%  
   ## Change region to a factor
   mutate(region = as.factor(region)) %>% 
   ## Change any other columns from character to numeric
   mutate_if(is.character, as.numeric) %>% 
   ## Group by region
   group_by(region) %>%  
   ## Create the new aggregated variables for each measure
   summarize(meanReadmissions = mean(PredictedReadmissionRate, na.rm = TRUE),
             medianReadmissions = median(PredictedReadmissionRate, na.rm = TRUE),
             meanDeathRate = mean(`Score_Death rate for pneumonia patients`, 
               na.rm = TRUE),
             medianDeathRate = median(`Score_Death rate for pneumonia patients`, 
               na.rm = TRUE),
             meanHospitalReturnDays = mean(`Score_Hospital return days for pneumonia patients`, na.rm = TRUE),
             medianHospitalReturnDays = median(`Score_Hospital return days for pneumonia patients`, na.rm = TRUE),
             meanMedicareSpending = mean(`Score_Medicare spending per patient`, na.rm = TRUE),
             medianMedicareSpending = median(`Score_Medicare spending per patient`, na.rm = TRUE)) %>% 
   ## Remove the missing values in region
   filter(!is.na(region))

levels(stateAggPneumonia$region)
```

### 1.2.4 If you were not able to figure it out, load the state-level aggregated data set

```{r}
load(file = "stateAggPneumonia.Rdata")

levels(stateAggPneumonia$region)
```

#### **Question 6**: [1 point]
Are there any missing data? If so, what data are missing? Did you make a mistake?

> Your answer here.By Collins
Yes I had missing data, I had 49 observation rather than 51 observation. My dataset was missing Alaska and Hawaii. No I don't think I made a mistake. The removal of N/A (!is.na(region)) could have resulted to missing the two states

### 1.2.5 Means or medians? How do you choose?

Choosing the mean or median value here is going to be really important for anyone interpreting our maps, as well as spatial statistics we will calculate downstream. Recall that we can choose the **mean** when:

 - The data is symmetric (i.e., it is approximately normal or has that beautiful bell-shaped curve!)

 - There are no extreme outliers or heavy skew

 - You are working with data that were measured on a *truly continuous scale* (i.e., height, weight) OR can be approximated by a normal distribution because of a sufficiently large sample size (see the [Central Limit Theorem](https://math.mit.edu/~dav/05.dir/class6-prep.pdf))

 - You want to describe an "average"

 - You are using statistical methods that assume normality (i.e., **parametric** tests)

On the other hand, we will choose the **median** when:

 - The data are skewed (i.e., there is long tail on one side of the curve) OR the data are lepto- or platykurtic (overly "peaked" or flattened, respectively)

  - **Skewness** Guidelines:

    - Skewness > 0 $\rightarrow$ right-skewed $\rightarrow$ prefer __median__

    - Skewness $\approx$ 0 $\rightarrow$ symmetric $\rightarrow$ prefer __mean__

    - Skewness < 0 $\rightarrow$ left-skewed $\rightarrow$ prefer __median__

  - **Kurtosis** Guidelines:

    - Kurtosis > 3 $\rightarrow$ Leptokurtic (sharp peak, heavy tails) $\rightarrow$ prefer __median__

    - Kurtosis $\approx$ 3 $\rightarrow$ Mesokurtic (bell-shaped) $\rightarrow$ prefer __mean__

    - Kurtosis < 3 $\rightarrow$ Platykurtic (flat peak, light tails) $\rightarrow$ prefer __median__

 - There are outliers that could distort the mean

 - You want to describe the "typical value" or "middle case"

 - You have ordinal data (e.g., patient satisfaction ratings)

 - You are using **non-parametric** statistical methods that do not assume normality because you found that your data were not normally distributed

#### **Question 7**: [1 point]
You may have to do some research if you do not recall, but why would showing someone a mean when we should have showed them a median, or vice versa, potentially create interpretation problems for us or our stakeholders?

> Your answer here.By Collins
Mean is very sensitive to extreme values (outliers). If they are very high or low values in a dataset, then mean can be skewed leading to a misleading interpretation. On the other hand median is very robust with outliers, therefore a good representation of a typical value when the data is skewed.
>Interpretation Problems:
1. Masking inequality: If you show mean in a skewed data then it might musk underlying inequalities. For example, if you are analyzing healthcare outcomes, a mean value may not reveal a small group of patients that has extremly poor outcomes (right skew) while the majority has good outcomes. In this case Median would be the best to represent the typical value as its robust to outliers.
2. overemphasizing extremes: conversely, if you show stakeholders a median value it might downplay the impact of extreme values. For example, in finance, a median return on investment may not capture the potential for extremly high returns that can occur with certain investments. In this example assume you have ROI: 5%, 7%, 10%, 12% & 1000%. in this case our median would be 10% ignoring the presence of 1000%. This is because median focuses on the central position of the ordered data rather than the weight of the values in the data.
3.Misleading comparisons: When comparing groups or datasets, using wrong measure of central tendency may lead to incorrect conclusions. For instance, if you comparing the average income of two cities, using the mean may make one city appear more affluent that it actually if there are a few extremely high-income individuals.
   

#### 1.2.5.1 Using boxplots embedded in violin plots to assess distribution symmetry

#### **Question 8A**: [1 points]
Practice reading boxplots to decide, for each measure, whether you should use the mean or median, or whether it matters. Need some help with boxplots? This [article](https://www.labxchange.org/library/items/lb:LabXchange:d8863c77:html:1) breaks it down nicely.

**Based on Figure 1 below**, which measure(s) do you think would benefit from the median over the mean? Why?

> Your answer here.
1. medical spending per patient seems a little bit skewed to the right. even thogh the skew is not prominent I would prefer using median
2. Pneumonia death rate shows a normally distributed (symmetrical) type of data indicating no presence of skweness. For death rate I would use mean as my measure of central tendency
3.Pneumonia Hospital return days and Pneumonia readmission rate are both prominently right skewed thus median would be the best. In addtion, pneumonia readmission rate is ordinal therefore, median would still the best to interprete it.

```{r, echo = FALSE, fig.height=8, fig.width=10, fig.cap="Figure 1. Violin and boxplots to assess distribution symmetry"}
pneumoniaFull %>% 
 select(PredictedReadmissionRate,
         `Score_Death rate for pneumonia patients`, 
         `Score_Hospital return days for pneumonia patients`, 
         `Score_Medicare spending per patient`) %>%  
  rename(`Pneumonia Readmission Rate` =  PredictedReadmissionRate,
         `Pneumonia Hospital Return Days` = `Score_Hospital return days for pneumonia patients`,
         `Pneumonia Death Rate` = `Score_Death rate for pneumonia patients`,
         `Medicare Spending per Patient` = `Score_Medicare spending per patient`) %>% 
    ## Change any other columns from character to numeric
  mutate_if(is.character, as.numeric) %>% 
  pivot_longer(cols = 1:4, names_to = "Measure", values_to = "Value") %>% 
  ggplot(aes(y = Value, x = Measure, fill = Measure)) + 
  geom_violin(alpha = 0.3) + 
  geom_boxplot(width = 0.25, alpha = 0.6, fill = "white") +
  scale_fill_manual(values = skittles[c(7,5,3,2)]) +
  theme_bw() +
  facet_wrap(~Measure, scales = "free", nrow = 1, shrink = TRUE) + 
  theme(legend.position = "bottom",
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  ggtitle("Figure 1. Mean or median? Which is skewed?")
```

#### 1.2.5.2 D'Agostino's $K^2$ Skewness Test to assess distribution symmetry

In Project 1, I introduced the Shapiro-Wilk test for normality for those who were not familiar with it. As a reminder, the Shapiro-Wilk test is a goodness-of-fit test to assess whether an approximately normal transformation matches our data, with the null hypothesis that it does. It does this by computing a $W$ statistic compares ordered sample values to the corresponding normal distribution quantiles. As I mentioned before, it typically has high power to detect deviations from normality in most situations unless $N>5000$.

```{r}
shapiro.test(pneumoniaFull$`Score_Medicare spending per patient`) # Just wanted to check with shapiro_wilk test
```
```{r}
shapiro.test(pneumoniaFull$`Score_Death rate for pneumonia patients`)
```

But normality tests (and Shapiro-Wilk is not the only one!) is not a sufficient test of __skewness__ or __kurtosis__. E.g., we can have enough skew or kurtosis that our distribution would be better described by a median over a mean, yet it does not deviate at the 5% significance threshold from a normal approximation! In other words, it can look normal, not deviate from normal per a Shapiro-Wilk test, and _yet_ still be sufficiently skewed that the median is a better choice. One reason this can happen, for example, is if you have a lot of repeats of the same value, as seen in count data or some quantitative measures that have been rounded to integers. 

__Enter [D'Agostino's $K^2$ or Skewness Test](https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test).__ Also sometimes referred to as the "D'Agostino-Pearson Test" in some statistics sources, the D'Agostino test was also designed as a goodness-of-fit test for distribution departures from normality. However, it simply doesn't perform quite as well overall as a Shapiro-Wilk for tests of deviation from normality BUT it is superior for assessing differences in skewness or kurtosis! What the test does is computes two components, (1) _Skewness_ to check for asymmetry and (2) _Kurtosis_ to check for peakedness or flatness, and combines these components into a $K^2$ test statistic, a version of the $\chi^2$ statistic. 

Let's use the `agostino.test()` from the `moments` package to apply this to our data. For our first example, I will use `PredictedReadmissionRate`. Remember, the null hypothesis, $H_0$, is that there is **no** skew. So, a $p < 0.05$ means we __reject the null hypothesis of symmetry__ and conclude that the data are significantly skewed. However, is $p \ge 0.05$, then there is no significant skewness to the data.

```{r}
agostino.test(pneumoniaFull$PredictedReadmissionRate)
```

___Interpretation___: Because $p<0.05$, we reject the hypothesis of symmetry and conclude that predicted readmissions are __significantly skewed__. We should use the median to aggregate this variable.

#### **Question 8B**: [1 point]
Your task is to repeat this test, with interpretations, for `Score_Death rate for pneumonia patients`, `Score_Hospital return days for pneumonia patients`, and `Score_Medicare spending per patient`:

```{r}
# Your code here.By Collins
agostino.test(pneumoniaFull$`Score_Death rate for pneumonia patients`)
```

___Interpretation___: By Collins. Am surprised because to my naked eyes pneumonia death rate violin showed normal distribution. But after running Agostino test and shipiro-wilk test, the data shows existence of skewness that is p-value of 0.0000000000001165 < 0.05. Therefore, we should use median 

```{r}
# Your code here. By collins
agostino.test(pneumoniaFull$`Score_Hospital return days for pneumonia patients`)

```

___Interpretation___: Because p-value =  0.00000000000000022 is less than 0.05, we reject null hypotheis and conclude that Score_Hospital return days for pneumonia patients is skewed. we should therefore use median 

```{r}
# Your code here.By Collins
agostino.test(pneumoniaFull$ `Score_Medicare spending per patient`)
```

___Interpretation___: By Collins: Because  p-value < 0.00000000000000022, $p<0.05$, we reject the hypothesis of symmetry and conclude that Score_Medicare spending per patient are __significantly skewed__. We should use the median to aggregate this variable.

#### 1.2.5.3 Assessing stability of central tendency estimates

One final thing we can do is assess the stability of the estimates of central tendency we've calculated, the __mean__ and the __median__ to ensure that, post-aggregation, we don't see any issues in the resulting distribution. If we did, we would want to consider another estimate we haven't yet explored - e.g., another percentile or even a weighted-mean. 

#### **Question 8C**: [1 points]
**Based on Figure 2 below**, and using your choices from questions 8A and 8B, assess the __stability__ (i.e., symmetry) of the estimates you selected as the best (either mean or median) for each of the four variables. Do you see any cause for concern with your choice, or do you feel comfortable to proceed with what you've selected?

```{r, echo = FALSE, fig.height=10, fig.width=5, fig.cap="Figure 2. Compare stability of mean vs. median after aggregation"}
stateAggPneumonia %>% 
  select(region, contains(c("mean", "median"))) %>% 
  distinct() %>% 
  pivot_longer(cols = -1, names_to = "Measure", values_to = "Estimate") %>% 
  mutate(Group = case_when(grepl("Readmissions", Measure)~ "Predicted Pneumonia Readmission Rate",
                           grepl("Hospital", Measure)~ "Pneumonia Hospital Return Days",
                           grepl("DeathRate", Measure)~ "Pneumonia Death Rate",
                           grepl("Medicare", Measure)~ "Medicare Spending per Patient")) %>%
  mutate(Measure = case_when(grepl("mean", Measure) ~ "Mean",
                             grepl("median", Measure) ~ "Median")) %>% 
ggplot(aes(y = Estimate, x = Measure, fill = Measure)) + 
  geom_boxplot(width = 0.25, alpha = 0.6) +
  theme_bw() +
  scale_fill_manual(values = c(skittles[2], skittles[3])) +
  facet_wrap(~Group, scales = "free", nrow = 4, shrink = TRUE) + 
  ggtitle("Figure 2. Assessing stability of central tendency \nestimates")
```

#### 1.2.5.4 Final selection of mean vs. median {#choice}

The time has come to make your final selection! Your job is to __remove__ the columns from `stateAggPneumonia` that do not correspond with your final decisions of mean or median for questions 8A-C. 

#### **Question 8D**: [1 points]
```{r}
# Your code here. By Collins
stateAggPneumonia <- stateAggPneumonia %>%
  select(-c("meanReadmissions", "meanDeathRate", "meanHospitalReturnDays", "meanMedicareSpending"))
```


### 1.3 Merge `stateAggPneumonia` with the `states` data for mapping.

Let's use a `left_join` on `stateAggPneumonia` on the `states` dataframe we made in [Section 1](#section1). __Make sure to join by the shared geographic identifier__; here that is `region`.

```{r}
mergedStateAggPneumonia <- left_join(stateAggPneumonia, states, by = "region")
```

### 1.4 Chloropeth map: predicted pneumonia readmissions

Recall from lecture that a choropleth map is a type of thematic map where areas (such as states, counties, or other geographic regions) are shaded or colored in relative proportion to the quantative value of a variable.

What follows is code to perform chloropeth mapping in `ggplot2` building on our base map from [Section 1](#section1). I will show you how to do the first one for __median__ Predicted Readmission Rate. You will be mapping the subsequent variables based on your choices of mean or median! 

**NOTE**: At this stage, you will get an error if you did NOT choose to retain `medianReadmissions`! If you made that mistake, return to this [step](#choice)!  

```{r, echo = FALSE}
## Store the plot into a variable called p1
p1 <- ggplot(mergedStateAggPneumonia, aes(x = long, y = lat, group = group, 
                               ## !!Update the fill to the variable you want
                               fill = medianReadmissions)) +  
  geom_polygon(color = "grey20", size = 0.2, alpha = 1) +
  ## !!Update this change the legend and color scale
  scale_fill_continuous(name="Median predicted % readmissions within 30 days", 
            low = "white", high = skittles[1]) +       ## !!Update color here   
            ## Leave low as white; only !!update high!
  
  ## !!Update the title to reflect a better map title
  labs(title = "Median pneumonia-related hospital readmissions by state",
   ## Leave the caption as-is
   caption = "Source: Centers for Medicare & Medicaid Services (CMS), 2024") +
  
  ## -- This makes the map pretty. No need to change any of this -- ##
  ## Makes a completely blank map background
  theme(
    panel.grid = element_blank(),
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white"),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom"
  ) +
  ## Sets the aspect ratio to roughly match US map proportions
  coord_fixed(1.3)

## Displays the map
p1
```

#### **Question 9A**: [0.25 points] {#table4}
Do you notice any trends that could be worth testing? If you're not sure, take a look at this sorted table showing the top 10 states for median pneumonia-related hospital readmissions:

#By Collins, southern states, western states (California and nevada), and northeastern states (Michigan, Illinois). These regions exhibit a higher median predicted percentage of readmissions within 30 days. this suggest there may be regional factors contributing to these higher rates

```{r, echo = FALSE, message=FALSE, warning=FALSE}
stateAggPneumonia %>% 
  arrange(desc(medianReadmissions)) %>%
  select(region, medianReadmissions) %>% 
  rename(Region = region, 
         `Median Predicted Pneumonia Readmissions` = medianReadmissions) %>% 
  top_n(10) %>% 
   kable(digits = 2,
    format = "html",
    caption = "Table 4. Top 10 states for pneumonia-related readmissions.") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
  )
```

**HINT**: It may not be readily obvious from the map or table! If you don't see a hypothesis, that's okay. But try to think about anything some of these states might have in common, if anything.

#By Collins:
#1. Socialeconomic Factors: states with lower resources access may experience higher readmission rates
#2. Demographic Factors: presences of higher ageing population may influence higher readmissions backed by factors such chronic diseases.

#### **Question 9B**: [1.75 points]

Your task is to now update the code I used to make `p1` for the remaining three variables. I suggest copying the code and following the comments to update everywhere I have `!!Update` written. You should update per the following specification for __full credit__:

- `Pneumonia Death Rate` (whether mean or median, your choice from earlier) $\rightarrow$ color should be `skittles[3]`
  - Don't forget to update the axis labels and titles appropriately. Death rate was measured as number of deaths per pneumonia cases in 30 days.

```{r}
## Store the plot into a variable called p1
p2 <- ggplot(mergedStateAggPneumonia, aes(x = long, y = lat, group = group, 
                               ## !!Update the fill to the variable you want
                               fill = medianDeathRate)) +  
  geom_polygon(color = "grey20", size = 0.2, alpha = 1) +
  ## !!Update this change the legend and color scale
  scale_fill_continuous(name="Median Pneumonia Deaths within 30 days", 
            low = "white", high = skittles[3]) +       ## !!Update color here   
            ## Leave low as white; only !!update high!
  
  ## !!Update the title to reflect a better map title
  labs(title = "Median Pneumonia-related Deaths by state",
   ## Leave the caption as-is
   caption = "Source: Centers for Medicare & Medicaid Services (CMS), 2024") +
  
  ## -- This makes the map pretty. No need to change any of this -- ##
  ## Makes a completely blank map background
  theme(
    panel.grid = element_blank(),
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white"),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom"
  ) +
  ## Sets the aspect ratio to roughly match US map proportions
  coord_fixed(1.3)

## Displays the map
p2
```

- `Medicare Spending` (whether mean or median, your choice from earlier) $\rightarrow$ color should be `skittles[7]`
  - Don't forget to update the axis labels and titles appropriately. Medicare spending was measured at each hospital per patient.
  
```{r}
## Store the plot into a variable called p1
p3 <- ggplot(mergedStateAggPneumonia, aes(x = long, y = lat, group = group, 
                               ## !!Update the fill to the variable you want
                               fill = medianMedicareSpending)) +  
  geom_polygon(color = "grey20", size = 0.2, alpha = 1) +
  ## !!Update this change the legend and color scale
  scale_fill_continuous(name="Median Medicare Spending per Patient within 30 days", 
            low = "white", high = skittles[7]) +       ## !!Update color here   
            ## Leave low as white; only !!update high!
  
  ## !!Update the title to reflect a better map title
  labs(title = "Median Medicare Spending Per patient by state",
   ## Leave the caption as-is
   caption = "Source: Centers for Medicare & Medicaid Services (CMS), 2024") +
  
  ## -- This makes the map pretty. No need to change any of this -- ##
  ## Makes a completely blank map background
  theme(
    panel.grid = element_blank(),
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white"),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom"
  ) +
  ## Sets the aspect ratio to roughly match US map proportions
  coord_fixed(1.3)

## Displays the map
p3
```
  

- `Pneumonia Hospital Return Days` (whether mean or median, your choice from earlier) $\rightarrow$ color should be `skittles[5]`
  - Don't forget to update the axis labels and titles appropriately. Hospital return days are the number of days readmitted pneumonia patients spend in the hospital. Readmissions are only measured within 30 days of the original admission to the hospital for pneumonia.
  
```{r}
## Store the plot into a variable called p1
p4 <- ggplot(mergedStateAggPneumonia, aes(x = long, y = lat, group = group, 
                               ## !!Update the fill to the variable you want
                               fill = medianHospitalReturnDays)) +  
  geom_polygon(color = "grey20", size = 0.2, alpha = 1) +
  ## !!Update this change the legend and color scale
  scale_fill_continuous(name="Pneumonia Related: Median Hopsital Return Days within 30 days", 
            low = "white", high = skittles[5]) +       ## !!Update color here   
            ## Leave low as white; only !!update high!
  
  ## !!Update the title to reflect a better map title
  labs(title = "Median Hospital Return Days by state",
   ## Leave the caption as-is
   caption = "Source: Centers for Medicare & Medicaid Services (CMS), 2024") +
  
  ## -- This makes the map pretty. No need to change any of this -- ##
  ## Makes a completely blank map background
  theme(
    panel.grid = element_blank(),
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white"),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom"
  ) +
  ## Sets the aspect ratio to roughly match US map proportions
  coord_fixed(1.3)

## Displays the map
p4
```
  

# 2 State-level Spatial Analysis

To perform spatial analysis, one needs geographic-level aggregate data that has been joined with geometry. Although it might feel like we technically have everything we need, alas, we do not - quite. Remember that our ultimate goal for our stakeholder is __prediction__ - so what this means is that we need to determine whether spatial structure is a problem before we proceed with predictions. If it is, we can include spatially lagged variables in our analysis; if not, we can proceed with exactly what we pre-processed in Project 1. 

## 2.1 Spatial Analysis Pre-processing Steps

### 2.1.1 Identify your appropriate method of aggregation for each variable, but **especially** your outcome of interest! 

But because our ultimate goal is to try to tie this back with all of the work we did in Project 1, I will actually give you data, which you get by running this code chunk. This returns both the training and testing sets, aggregated but not yet merged with geometries, so we don't lose our other encoding and especially our transformations for OLS. They are going to be called `stateAggTrain` and `stateAggTest`, respectively.

```{r}
source(file = "loadTrainTest.R")
         
#"C:/data_science/DSE6003OM_SP2025R1/Merrimack_DSE6630/Team_Directories/gamma/Module_Demos/Demo_2/loadTrainTest.R")

```

**NOTE 1** that these new aggregated training and testing states have `region` in them instead of `state`. This is so that they are already cleaned up for merging with geometries.

**NOTE 2**: All of the raw variables have been named `Median_Raw...` to distinguish them from their transformed and scaled counterparts!

#### **Question 10A**: [0.25 points]

Check that all of the states (in `region`) made it into the training set. If you choose to do this by counting, say with `unique()` or `distinct()`, you should get __52__ if they are all present.

```{r}
# Your code here By Collins
unique(stateAggTrain$region)
```

> Your answer here. By Collins. I happen to have only 49. Am still missing Alaska and Hawaii

#### **Question 10B**: [0.25 points]

Why am I not asking you to check the states in the testing data?

> Your answer here. By Collins
The testing data is used to evaluate the performance of the model, and it is assumed that the it will have similar distribution of states as the training data. Therefore, checking the states in the testing test data is not as crucial as checking the states in the training set.

#### **Question 10C**: [0.25 points]

In the aggregation that I did for both `stateAggTrain` and `stateAggTest` I took the __median__ for everything. Why, when we just went through all the rigmarole of figuring out if we should display the mean or the median of variables like Predicted Pneumonia Readmissions? 

**HINT 1**: What have we already done to the training and testing sets that we had not done here because we went back to the raw data? 

**HINT 2**: Why is a median _or_ a mean acceptable if we're working with scaled and centered data?

> Your answer here. By Collins
Scaling and centering data is technically transforming data from asymmetry to symmetry. In short its a way of normalizing the data around the mean such that it follows a normal distribution. In this case using either mean or medium will be acceptable.

### 2.1.2 Identify your geographic level of aggregation for each variable. 

__Here, we have selected state__. We already decided this going into the previous analysis; it only makes sense to continue that level of spatial analysis first. 

### 2.1.3 Get the right geometries for spatial analysis. 

Sadly, what we used for mapping will not be sufficient. Recall that, in lecture, I kept referring to something called __shapefiles__ and I said we wouldn't have to worry about them a ton (yet) but that we were going to need them already. Well, here they are! 

A shapefile is a widely used geospatial vector data format for geographic information system (GIS) software, storing the location, shape, and attributes of geographic features such as points, lines, and polygons. This is typically across a set of related files with file extensions of `.shp`, `.shx`, or `.dbf`. Because __shapefiles are the gold-standard__ of spatial analysis, our spatial statistics packages in R similarly expect geometrics in __shapefile__ (often abbreviates as __sf__) format. We could either import shapefiles (which we will do in Project 2) or, here, we will simply convert the geometrics from the `maps` package into shapefile format so we can calculate our spatial statistics. 

Complicated, right? Thankfully, most of that magic will be performed for us using the `sf` package in R. If you want to learn more about the `sf` package, you can find that [here](https://r-spatial.github.io/sf/). 

_We can convert to shapefile format in a single line of code:_
```{r}
## Get state map geometries in shapefile format
states_sf <- st_as_sf(map("state", plot = FALSE, fill = TRUE))

```  

#### **Question 10D**: [0.25 points]

You may need to do a little digging, but what exactly is the `st_as_sf()` function from the `sf` package doing? Can you figure out why I didn't just use the `states` object we already used for mapping?

#Answer By Collins: `st_as_sf()` function is used to convert the map object ("state", plot = FALSE, fill = TRUE) into an (sf) object named states_sf. This allows for further analysis and manipulation using sf package
>states object was not used directly because it might not be in the format that is easily convertible to an sf object or might not have the necessary geometry information.


```{r}
?st_as_sf() #Convert stars object into an sf object
```


### 2.1.4 Calculate spatial weights for our target, predicted pneumonia readmission rate. {#findSW}

Now, we finally get to the meat of it! __Spatial weights__ represent the spatial relationships (or influence) between geographic units (here, states). In other words, spatial weights define how much nearby observations contribute to calculations for a given location. These weights are typically based on proximity, contiguity (shared borders), or distance, but it depends on the type of spatial autocorrelation or dependence in the data.

Recall that, in lecture, I mentioned that we were going to look at spatial autocorrelation. Spatial autocorrelation is the relative degree to which the value of a variable (e.g., hospital readmissions) at one location is similar to values at nearby locations. When nearby areas have similar values (e.g., high readmission rates clustered together), it's called _positive_ spatial autocorrelation (**clustering**); when nearby areas have very different values, it's _negative_ spatial autocorrelation (**dispersal**). Thus, you can think of **spatial weights** as measures of the spatial correlation in hospital readmissions between states, in our case!

We will use a package called spatial dependence (`spdep`, more information [here](https://r-spatial.github.io/spdep/index.html)) to calculate our spatial weights. We have a series of small sub-steps to go through to get there.

#### 2.1.4.1 Turn off S2 geometry engine, if needed (usually a good idea if you didn't import shapefiles directly or if they are older shapefiles)

By default, the `sf` package now uses Google's S2 geometry engine (a library for geometric calculations on the sphere developed by Google for their maps), which is strict and throws errors when geometries are even slightly invalid (like if edges should cross each other even slightly). We will choose to turn off the S2 engine just for this step because it will fail otherwise. This is often going to be the case _when you are not importing shapefiles directly or if your shapefiles are older and predate the S2 engine__. But, we will turn it back on when we're done. 

_We can disable the S2 engine like this:_
```{r}
## Disable Google S2 engine
sf_use_s2(FALSE)
```

We will also just do a quick cleanup using the `st_make_valid` function **just to be safe**. It never hurts! All it does is clean up any of those cases where the borders do cross (i.e., are invalid) so we don't get any errors.

_We can validate the geometry edges like this:_
```{r}
## Clean up the edges of `states_sf` to be safe
states_sf <- st_make_valid(states_sf)
```

**Find Neighboring States**

Next, we will use the `poly2nb()` function from the `spdep` package to create a _neighborhood list_ based on the polygon boundaries of our spatial objects (states). It is using the shapefile (sf) object we already made, `states_sf`, to do this. What it will do is defines which polygons (states) are neighbors by looking for which ones share borders with each other (called __contiguity__). By default, it uses what is called "queen" contiguity (derived from chess); it would define a neighboring state as one that shares any point (edge OR corner) as neighbors. 

As an example, Colorado and Arizona do not share a border only a corner, right? But according to "queen" contiguity, they are still neighbors. 

#### **Question 11A**: [0.5 points]
Look up "rook" contiguity and explain what (1) what it means, (2) how the neighbor relationship of Colorado and Arizona would change, if it would, and (3) how we change to this type of contiguity in the `poly2nb()` function. (You don't need to actually change it in the code, however.)

> Your answer here.By Collins:
>1. What is "rook" contiguity. It refers to a method of defining polygons based on shared edges. Unlike "queen" contiguity, which considers both edges and corners as points of contiguity, rook contiguity only considers polygons as neighbors if they share an edge (boundary line)
>2. how the neighbor relationship of Colorado and Arizona would change: Under "queen" contiguity, Colorado and Arizona are considered neighbors because they share a corner. However, under "rook" contiguity, they would not be considered neighbors since they do not share an edge (boundary line).
>3. how we change to this type of contiguity in the `poly2nb()` function. To implement "rook" contiguity in `poly2nb()`function, we would set the "queen" argument to FALSE

_We can find neighboring states using queen contiguity like this:_
```{r}
## Create neighbors using queen contiguity
states_nb <- poly2nb(states_sf, queen = TRUE)
```

**Calculate Spatial Weights Between Neighbors**

Next, we will use the `nb2listw()` function from `spdep` to convert the neighborhood list that we created with `poly2nb()` (called `states_nb`) into a spatial weights object that we can use in spatial analysis like Moranâ€™s $I$, LISA, and spatial regression. What `nb2listw()` does is assigns weights to the neighbors identified in the `nb` object, and those weights define how strongly each neighbor influences a spatial unit (state). 

There are actually several styles of weighting; we will be using row-standardizing (`style = "W"`) as it is the most appropriate for spatial autocorrelation. Row-standardized weights are spatial weights where each row sums to 1. This means the influence of all neighbors on a given spatial unit is normalized, so the total influence is consistent across all units regardless of how many neighbors they have. Without standardization, units with more neighbors could have a greater total weight, biasing spatial statistics. This means that states like Colorado, that have lots of neighbors, are not weighted more heavily than states like Alaska or Hawaii, which are effectively islands. **This will make Moran's $I$ more comparable across the states.** 

_We can calculate the spatial weights as folows:_
```{r}
## Calculate the state-level spatial weights
states_sw <- nb2listw(states_nb, style = "W")

## Turn Google S2 engine back on
sf_use_s2(TRUE)
```

#### **Question 11B**: [0.5 points]

The first state in our dataset is Alabama. Take a look at one of our maps if needed. How many neighbors do you expect it to have?

> Your answer here. By Collins
Alabama has four neighbors: Tennessee, Georgia, Florida, and Mississippi

Now investigate Alabama's spatial weights:

```{r}
states_sw$weights[[1]]
```

Does this feel consistent (1) with what you know about the number of neighbors Alabama has and (2) what I mentioned above about row-standardizing our weights? Why or why not?

> Your answer here. By Collins
Yes this is very consistent because each neighbor is allocated a weight of 0.25. This means the total weight of Alabama will be 1 just like any other state with less or more neighbors

Lastly, investigate the spatial weights for Massachusetts (number __22__). Does it fit with your expectations given the number of neighbors?

```{r}
# Your code here.
states_sw$weights[[22]]
```

> Your answer here. By Collins
Massachusetts has five neighbors: New Hampshire, vermont, New york, connecticut, and rhode Island. While the idea of normalized weight of 1 is consistent, I expected each neighbor to contribute equal weight of (1/5)

**Join the Spatial Weights back to our Training Data**

Our final step is to attach the neighbor lists and spatial weights back to the training data and the shapefile we already have. This way, we can proceed with the rest of our spatial analyses. 

**NOTE** that if the shapefile spatial names (`ID`) did not already match `region` in `stateAggTrain` we'd have to convert that first! Further, it is at this stage you would add FIPS to facilitate merging, if needed.

_We can calculate the spatial weights as folows:_
```{r}
## If we want to make sure that the shapefile ID matches region:
## unique(states_sf$ID)

## Join the shapefile with spatial weights and neighbor lists back to the 
## training data
states_sf_AggTrain <- left_join(states_sf, stateAggTrain, 
                                by = c("ID" = "region"))
```

#### **Question 12**: [1 point]

I am sure you caught this, but spatial weights and everything are being done on the **training** data only. What key issue am I trying to __avoid__ by performing all of my spatial pre-processing steps on the training data only? Why am I not using the full dataset for this?

**HINT**: It is the same reason we did all of our other pre-processing AFTER splitting the data!

> Your answer here. By Collins
The main reason to perform all spatial pre-processing steps on the training data only is to avoid data leakage. Data leakage occurs when information from test dataset is used to train the model, which can lead to overly optimistic results and poor performance on unseen data. 

## 2.2 Global Spatial Autocorrelation: Global Moranâ€™s $I$

Recall from lecture that I mentioned that our goal is to determine if there is clustering (positive spatial autocorrelation) or dispersal/dispersion (negative spatial autocorrelation). Global Moranâ€™s $I$ is a way to measure clustering and will tell us whether clustering exists overall. Moran's $I$ statistic ranges from -1 to +1, just like a typical correlation coefficient! Significant positive $I$ inidicates clustering of similar values (high with high, low with low) whereas significant negative $I$ indicates dispersion (high values near low values; or "opposites attract"). Just as with a canonical correlation, $p$-values < 0.05 suggest significant spatial autocorrelation (assuming we are using the 5% significance threshold). It's important to note that an $I$ = 0 would indicate randomness; thus, the null hypothesis ($H_0$) is that median pneumonia readmission rates are randomly distributed across the states.
  

### 2.2 1. Set the target {#setTarget}
We are going to set the target like this to facilitate making it easier to swap out at a later question. **NOTE** that we are centering but NOT scaling it - why? This is because we want it to be comparable to the lagged values; this will make a little more sense as we go.

```{r}
## Change this to change the target!

y <- scale(states_sf_AggTrain$Median_RawPredictedReadmissionRate,
           center = TRUE, scale = FALSE)

```



### 2.2.2 Calculate the global Moran's $I$
Using the `moran.test()` function from the spatial dependence (`spdep`) package, we will calculate the global Moran's $I$ statistic first. If you would like more details about this function, you can find that [here](https://r-spatial.github.io/spdep/reference/moran.test.html).

```{r}
## Calculate the global Moran's I for states on training data
## Use scaled y & apply spatial weights
global_moran <- moran.test(y, states_sw)

## View the results
global_moran

## Show a plot of the top most outlier states
moran.plot(as.vector(y), 
           listw = states_sw,
           ylab = "Spatially-lagged Median Pneumonia Readmissions",
           xlab = "Median Pneumonia Readmissions (No Lag)", pch = 16,
           col = skittles[2], xlim = c(-3,3), 
           main = "Figure 3. Global Moran's I (State-Level Aggregation)")
```

**Interpretation**:
Our global spatial autocorrelation test found a Moran's $I$ of 0.268 with a $p$-value of 0.0014, which is less than 0.05. Thus, we reject the $H_0$ of a random distribution and conclude that we have __significant positive spatial autocorrelation__ for median pneumonia-related readmissions! This suggests that states with similar median readmission rates _tend to be clustered geographically_ in the U.S.!

The accompanying scatterplot produced with `moran.plot()` confirms this. On the $x$-axis we have median pneumonia-related readmission rates for each spatial unit (state) and on the $y$-axis the spatially lagged version of readmission rates. __Spatial lags are the weighted average value of neighboring states (spatial units)__.
Thus, this plot shows that high readmission states tend to cluster with other high states and low states with other low states (i.e., positive spatial autocorrelation).
Outlier states in each of the quadrants of the graph are also identified.

- **High-high outliers** (Quadrant I)
  - These states have especially high readmission rates and are generally surrounded by neighboring states that are also high.
  
- **Low-high outliers**  (Quadrant II) 
  - These states have low readmission rates but are generally surrounded by neighboring states that are high.
  
- **Low-low outliers** (Quadrant III)
  - These states have especially low readmission rates and are generally surrounded by neighboring states that are also low.
  
- **High-low outliers**  (Quadrant IV) 
  - These states have high readmission rates but are generally surrounded by neighboring states that are low.

#### **Question 13**: [1 point]
Identify each of the outliers from the graph and interpret the type of outlier (high-high, low-high, etc.) that they are. 

> Your answer here. By Collins
1.quadrant 1 (High-High), West Virginia, District of Columbia & New Jersey
2.quadrant II (Low-High), Delaware
3.quadrant  III (Low-Low), Utah


#### **Question 14A**: [0.5 points]
**NOTICE** that I had you define the target as a _centered_ version of `Median_RawPredictedReadmissionRate`, which is the median of the _untransformed, unscaled_ readmissions. Shouldn't I have used the transformed and scaled version, `bc_PredictedReadmissionRate`? Can you think of any reason why I chose to use the centered median of the raw readmissions rather than the transformed version?

> Your answer here. By Collins
a transformed target (`bc_PredictedReadmissionRate) is not only centred but also scaled. That means it would be difficult to interpret its results. Therefore the main reason for using the centered median of raw readmissions instead of the transformed and scaled version is to maintain interpretability. By using the centered version, the results are more intuitive and easier to understand in the context of actual readmission rates. In addition, the results are simpler for stakeholders to understand.
>Centering involves subtracting a constant value from each data point, typically the mean or median to shift the data's central tendency. Scaling, on the other hand, involves multiplying each data point by a constant factor to adjust the data's range or variance. So, while scaling adjusts the range and variance of the data, centering focuses on aligning the data around a central point. Centering does not change the spread or dispersion of the data, whereas scaling does. Centering is particularly important when dealing with lagged values, as it ensures that the original and lagged values are directionally aligned. in our case this is very important in ensuring that both the original and lagged values belong in the same quadrant.


**HINT 1**: What does centering do that scaling does not? Could centering be important, for example, to make sure that $y$ is going in the same direction as lagged $y$?

**HINT 2**: Why do we need $y$ and lagged $y$ going in the same general directions? Could it have anything to do with identifying which __quadrant__ they fall into?

**HINT 3**: [Chapter 8](https://www.paulamoraga.com/book-spatial/spatial-autocorrelation.html) by Paul Moraga on Spatial Autocorrelation may be helpful, but it also gets a bit deep in the weeds too.

#### **Question 14B**: [0.5 points]

Now go back up to temporarily **reset the target** to `bc_PredictedReadmissionRate` [here](#setTarget). **There is no need to scale it again!** Then, re-run the Global Moran's $I$. make sure to update the limits of the $x$-axis or just comment that line out (`xlim(...)`) to look at the graph. Looking at the value of $I$, the $p$-value, and the graph output - does the conclusion change? Does it matter which value we choose for the analysis?

> Your answer here. By Collins
Moran I statistic = 0.288790751 and p-value = 0.0007019 the interpretation remains the same because the p-value = 0.0007019 < 0.05  and its still positive. But for the graph, utah and west virgin miss to show up in the graph

**NOTE**: Make sure to change $y$ back to the scaled version of `Median_RawPredictedReadmissionRate` before moving to the next analysis!


## 2.3 Local Spatial Autocorrelation (LISA): Local Moran's $I$

Now that we've gotten a sense for possible outlier states and we know that we have global spatial autocorrelation in median readmission rates, it would be nice to concretely identify hotspots and coldspots. Unlike the global Moran's $I$, which provides an overall measure of spatial autocorrelation across the entire nation, LISA (**L**ocal **I**ndicators of **S**patial **A**ssociation) helps pinpoint specific states that significantly contribute to that global pattern. LISA identifies clusters of similar values, such as high-high (hotspots) and low-low (coldspots). It can also be used to identify spatial high-low and low-high outliers, just as the global test did. This local approach is so powerful though because it __enables us to map and interpret where spatial clustering or dispersion is happening__, which is especially valuable for targeted policy intervention or resource allocation. So, even though it may not be as vital for our predictions, it can help us to better understand the spatial pattern of pneumonia-related hospital readmissions more generally.

## 2.3.1 Run local Moran's I for LISA

To perform the LISA, we will use the `localmoran()` function from `spdep`. More information on the function can be found [here](https://r-spatial.github.io/spdep/reference/localmoran.html). After, instead of printing the `summary()` I show you the `head()` of the first rows produced by the LISA.

```{r}
## Run local Moran's I for LISA on vectorized y & applying same spatial weights
local_moran <- localmoran(as.vector(y), states_sw)
```

```{r, echo=FALSE}
head(local_moran) %>% kable(digits = 2,
    format = "html",
    caption = "Table 5. First 6 rows of Local Indicators of Spatial Awareness (LISA) output") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
  )
```

- `Ii` = Stands for $I_i$, which is the local Moran's $I$ estimate for that state. $I_i$ is a measure of local spatial autocorrelation for the given state. If positive, it is similar to its neighbors, if negative it is dissimilar. 

- `E.Ii` = Stands for $\mathbb{E}(I_i)$, which is the expected value of $I_i$ under the null hypothesis of spatial randomness. This value is usually close to 0.

- `Var.Ii` = Stands for $Var(I_i)$, which is the variance of $I_i$ under the null hypothesis. It is used to assess the significance of $I_i$.

- `Z.Ii` = Stands for $Z_{I_i}$, which is the $Z$-score calculated as $Z = \frac{I_i âˆ’ \mathbb{E}(I_i)}{\sqrt{Var(I_i)}}$, which indicates how extreme $I_i$ is compared to the random expectation. Used to derive statistical significance.

- `Pr(z!=E(Ii))` = Stands for $Probability(Z_{I_i} \ne \mathbb{E}(I_i))$, which is the probability that  $I_i$ is not equal to the expected value $\mathbb{E}(I_i)$. In other words, it's the p-value associated with $Z_{I_i}$! So, it tells you whether the local spatial autocorrelation is statistically significant for any given state.

That might feel like a lot of confusing math, but let's start to put it together into something meaningful we can use: __HOTSPOTS__ vs. __COLDSPOTS__.

### 2.3.2 Criteria for hotspots vs. coldspots {#hotVScold}

- **Hotspots** (High-High). States with a high median readmission value surrounded by other high value states. As you get to the fringes of hotspots, other states that have high readmission rates but are surrounded by low states will not be labeled as part of the hotspot but could be labeled an outlier (high-low).

- **Coldspots** (Low-Low). States with a low median readmission value surrounded by other low value states. As you get to the fringes of coldspots, other states that have low readmission rates but are surrounded by high states will not be labeled as part of the coldspot but could be labeled an outlier (low-high).

We can actually break this down a bit to help us better understand the difference between hot/coldspots and _outliers_ - outliers are states with high readmission values that are near states really different from them in terms of readmission rates. 

\   

##### Table 6. Description of the quadrants and their corresponding LISA cluster categories.

|**Readmissions**|**Neighbors**|**LISA Category**|**Quadrant**| **Explanation**|
|:---------------:|:---------:|:----------------:|:--------:|:----------------------------------------------------------------|
| High	          | High      | Hotspot (H-H)    | I        | State with high readmissions is surrounded by other high states |
| Low	            | Low	      | Coldspot (L-L)   | III      | State with low readmissions is surrounded by other low states   |
| High	          | Low	      | Outlier (H-L)    | IV       | State with high readmissions is surrounded by low rate states   | 
| Low	            | High	    | Outlier (L-H)    | II       | State with low readmissions is surrounded by high rate states   | 
| High or Low     | Random    |	Not Sign. (N.S.) | -        | No difference from random (dispersed)                           |

This implies, then, that if we __identify the quadrants__ we identify the hot/coldspots! So, let's do that.

#### **Question 15A**: [1 point]

- Hotspots (Quadrant I, "High-High") will be states that have:

 - A significant $p$-value ($p < 0.05$), which suggests that local $I_i$ is significantly __positive__ ($I_i > 0$) 
 - A _centered_ median hospital pneumonia-related readmissions rate that is positive
 - A __spatially-lagged__ median hospital pneumonia-related readmissions rate that is positive

**What will the criteria for coldspot (quadrant III) states be?**

> Your answer here. By Collins
 - A significant $p$-value ($p < 0.05$), which suggests that local $I_i$ is significantly __negative__ ($I_i < 0$) 
 - A _centered_ median hospital pneumonia-related readmissions rate that is negative
 - A __spatially-lagged__ median hospital pneumonia-related readmissions rate that is negative


#### 2.3.2.1 Extract the spatially-lagged median hospital readmission rates
We can use the `lag.listw()` to extract the spatial lag of the target. So, these values are the __spatially-adjusted__ median pneumonia-related 

```{r}
lag_medianReadmissions <- lag.listw(states_sw, y)
## Run this if you want to see what they look like!
 #lag_medianReadmissions

## Just Massachusetts' value:
 lag_medianReadmissions[20] #initially we used 22 to represented Massachusetts, has it changed because we are missing Alask and Hawaii (islands)?
```

#### **Question 15B**: [0.5 points]
The raw target is the % of all pnuemonia patients discharged who are predicted to be readmitted within 30 days after being released from the hospital. So, e.g., the _raw_ median pneumonia-related readmissions rate for Massachusetts is 16.6789, meaning that $\approx$ 16.7% (no multiplication by 100 needed here!) are predicted to be readmitted based on a sliding-window average of 30-day readmissions. 

Yet, the _spatially-lagged_ pneumonia-readmissions for Massachusetts is only 0.2126469! 

**QUESTION**: Why do the spatially-lagged variables seem SO shifted compared to the original value?

**HINT**: What did we do to the raw hospital readmissions before we calculated the spatial weights in this [section](#findSW)?

> Your answer here.By Collins
we centered the raw hospital readmissions before we calculated the spatial weights

#### **Question 15C**: [0.5 points]
Show just Massachusetts' median readmissions to see its value after performing the step I am referring to in the hint. 

**HINT**: Massachusetts is at row 20 in the `states_sf_AggTrain` dataframe, which will make it easier to locate its value in the vector created by looking at just the `Median_RawPredictedReadmissionRate` column.

```{r}
## Your code here.

#hint 1 step being refered to "y <- scale(states_sf_AggTrain$Median_RawPredictedReadmissionRate,
           #center = TRUE, scale = FALSE)" we did centering but not scaling 

states_sf_AggTrain$Median_RawPredictedReadmissionRate[20]

  
```

**QUESTION**: Is this a lot closer to the spatially-lagged value?

> Your answer here. By Collins 
Yes it is, 16.67% is close to 21.26 %

#### 2.3.2.2 Convert the LISA results to a dataframe

```{r}
local_df <- local_moran %>% as_data_frame()
```

#### 2.3.2.3 Add the local $I_i$, $p$-value, and lagged readmissions to the `states_sf_AggTrain` dataset

```{r}
states_sf_AggTrain <- states_sf_AggTrain %>%
  mutate(
    ## adds the local I
    localI = local_df$Ii,
    ## adds the p-value
    pValue = local_df$`Pr(z != E(Ii))`,
    ## adds the lagged median readmissions
    lag_medianReadmissions = lag_medianReadmissions
  )
```

#### 2.3.2.4 Classify the LISA quadrants (hot/coldspots) based on the criteria 
The criteria we used (in case you already forgot!) were set [here]{#hotVScold}. All of the steps might make it easy to forget what we are trying to do, but we're finally at the important part: **identifying the quadrants**. 

#### **Question 15D**: [0.5 points]
The code below identifies the quadrants and adds them to the `states_sf_AggTrain` dataset. Your task is to add comments to the code below where indicated. Make sure to refer back to the criteria we set [here]{#hotVScold} if needed!

> Answer in the comments.

```{r}
## COMMENT HERE calling state_sf_AggTrain using pipe function
states_sf_AggTrain <- states_sf_AggTrain %>%
  mutate(
    ## COMMENT HERE mutate function add/ modifies columns in states_sf_AggTrain
    quadrant = case_when(
      ## COMMENT HERE case_when statement performs multiple conditional checks and returns different values based on those conditions. in this case it creates a new column quadrant
      pValue > 0.05 ~ "N.S.", #this line checks if the value in the Pvalue column is greater than 0.05. If true, it assigns the string "N.S" (Not Significant) in the quadrant column and stops evaluating further conditions
      ## COMMENT HERE - Why do I not have to specify p-value again? because the p-value condition pValue > 0.05 ~ "N.S." is being used as a filter to exclude non-significant results from further evaluation. once the condition is met it stops evaluating further conditions
      
      ## COMMENT HERE ABOUT LINE BELOW - INCLUDE QUADRANT!
      y > 0 & lag_medianReadmissions > 0 ~ "Hotspot", # Quadrant I, High readmissions with high neighboring states
      
      ## COMMENT HERE ABOUT LINE BELOW - INCLUDE QUADRANT!
      y < 0 & lag_medianReadmissions < 0 ~ "Coldspot", #Quadrant III, low readmissions with low neighboring states

      ## COMMENT HERE ABOUT LINE BELOW - INCLUDE QUADRANT!
      y > 0 & lag_medianReadmissions < 0 ~ "High-Low Outlier", #Quadrant iv, high readmissions and low neighboring states (outlier)

      ## COMMENT HERE ABOUT LINE BELOW - INCLUDE QUADRANT!
      y < 0 & lag_medianReadmissions > 0 ~ "Low-High Outlier", # Quadrant II, Low readmissions and high neighboring states (outlier)

      ## COMMENT HERE default case for the case_when statement. if non of the above conditions are met, then assighns N/A
      TRUE ~ NA
    )
  )
```

#### 2.3.2.5 Map the LISA cluster (quadrants)
Lastly, let's visualize our hot/cold spots on the map! Note that we are now using `geom_sf()` for mapping rather than `geom_polygon()`. This is because our dataframe now contains **shapefile** geometries instead of just the polygons that it did earlier, hence the change.

```{r}
ggplot(states_sf_AggTrain) +
  ## Now using geom_sf() because we have shapefile data rather than just
  ## polygons; geom_polygon() will no longer work here!
  geom_sf(aes(fill = quadrant), size = 0.2, 
          alpha = 1, color = "darkgray") +
  
  ## Fill based on the quadrants!
   scale_fill_manual(name = "LISA Cluster (Quadrant)",
    values = c("Hotspot" = skittles[6],
               "Coldspot" = skittles[1],
               "High-Low Outlier" = skittles[7],
               "Low-High Outlier" = skittles[2],
               "N.S." = "#dadbd7")) +
  
  ## Makes a completely blank map background
  theme(
    panel.grid = element_blank(),
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white"),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom"
  ) +
  labs(title = "LISA Clustering of Median Pneumonia-related Hospital Readmission Rates",
    subtitle = "Hotspots and coldspots show spatial clusters of similar values",
    caption = "Source: Centers for Medicare & Medicaid Services (CMS), 2024")
```

#### **Question 15E**: [0.5 points]
Interpret what it means if Indiana is a "low-high" outlier.

> Your answer here. By Collins
From The map we can see Indiana is in a quadrant low-high. That is Indiana has low readmission rates of pneumonia but is surrounded by states with high readmission rate.

#### **Question 15F**: [1 point]
Do the hot and coldspots make sense with your observations of graph `p1`? Why or why not? Can you explain why hot/coldspots will not necessarily correspond with __individual states'__ high and low median readmission rates?

> from P1, states like California, Nevada, Michigan, Florida, West virginia, and Illinois show high readmission rates but non of them made it to hotspots. However states with very low readmission rates like Wyoming and utah made it to coldspot quadrant.
>Hot and Coldspots somehow make sense with some of the observations in p1 because they represent spatial clusters of similar values. However, they will not necessarily correspond with indivual states "high and lwo median readmission rates due to the spatial nature of the analysis.

## 2.4 Spatial Regression Models

__Spatial regression models__ explicitly account for spatial relationships in data, which is something we encounter constantly in public health and epidemiology, as well as environmental studies (e.g., climate change, pollution, wildfire risk), agriculture (e.g., soil nutrients, water availability), sociology and social work (e.g., poverty, social justice), criminology (e.g., real-time crime predictions), and business (e.g., real-estate and housing prices). In other words, spatial regression gives us tools to analyze _how nearby locations influence each other_, especially when spatial relationships __violate the assumption of independence in OLS regression!__

There are two common issues that spatial regression addresses:

1. **Spatial dependence**, in which outcomes in nearby locations tend to be similar (e.g., rates of a transmissable disease tend to be higher in nearby areas vs. farther areas).

2. **Spatial heterogeneity**, in which relationships between variables may vary across space (e.g., housing prices, climate, or wildfire risk depend on geographic features of the landscape, sometimes features we have not even measured).

### 2.4.1 Spatial Lag/Autoregressive Regression (SAR)

There are three types of spatial regression models worth mentioning. The **spatial lag model (aka, Spatial Autoregressive Model or SAR)** is the one we will focus on first. In this regression model, the outcome depends on neighboring outcomes. This makes sense for us because we know from the LISA analysis that we have hot/coldspots of pneumonia-related readmissions! 

The general structure of the SAR model is:

$y = \rho Wy + \beta_1 X_1 + \beta_2 X_2 + ... \beta_n X_n +\epsilon$

where $y$ is the target variable (median pneumonia-related readmissions), $W$ is the spatial weights matrix, which defines the neighborhoods (stored in `states_sw`!), and $\rho$ is the _spatial lag coefficient__. This means that the first term of the equation, $\rho Wy$, measures the spatial lag of the target variable as the average of neighborsâ€™ outcomes multiplied by the spatial lag coefficient. It tells us __how much the outcome deviates just do to spatial relationships with neighbors!__. The remaining terms, $\beta_1 X_1 + \beta_2 X_2 + ... \beta_n X_n$ are just the standard regression coefficients for each of the $n$ predictors. $\epsilon$ is an unmeasured error term, which we also have in OLS regression. 

We are justified in proceeding with a SAR because, as you hopefully remember from the end of Project 1, pneumonia-related hospital readmissions could be predicted with an OLS regression! However, we also know we have spatial patterns here that we want to try account for in our predictive models, so let's do that now.

#### **Question 16**: [1 point]
Your data have already been partitioned into training and testing sets, preprocessed and normalized, and we just calculated the spatial weights matrix, $W$. What if we did NOT already have the data fully pre-processed? In a few sentences, describe the steps we would have to take if we were starting from the raw data.

**HINT**: Think of the data science lifecycle/flow we keep coming back to!

> Your answer here. By Collins
data importation > cleaning (tidy) > [Exploratory analysis (EDA) > Feature Engineering]. the next is model, its rfinement and then prediction.
> pre-processing (Train-test split > impute/remove missing values > encoding > arithment tranformation > center and scaling)


### 2.4.1.1 For our convenience, make a dataframe that contains just the information we want for fitting the SAR and store it into `sarTrain`

```{r}
            ## Start from the training data
sarTrain <- states_sf_AggTrain %>% 
  ## Make it a dataframe/tibble
  as_data_frame() %>% 
  ## Drop the features that we dropped before the analysis in Project 1, as 
  ## well as the features we do NOT want to use as predictors!
  select(-Median_RawPredictedReadmissionRate, 
         -Median_RawMedicareSpending, 
         -Median_RawHospitalReturnDays, 
         -Median_RawDeathRate, 
         -ID,
         -localI, 
         -pValue, 
         -quadrant, 
         -geom, 
         -lag_medianReadmissions)
```

### 2.4.1.2 For comparison, let's fit the OLS regression of the Box-Cox transformed pneumonia-related readmissions against the predictors

```{r}
OLS <- lm(bc_PredictedReadmissionRate ~ ., data = sarTrain)
```

#### **Question 17A**: [0.5 points]
Why am I fitting an OLS regression again? 

**HINT 1**: There are actually TWO reasons! But you only need one for full credit...

**HINT 2**: What did aggregation do to our sample size?

> Your answer here. By Collins
Ols will act as our baselie model for comparison and 2nd aggregation reduced our sample size significantly.

### 2.4.1.3 Check residuals for spatial autocorrelation
If we didn't already know that we have spatial relationships from the Moran's $I$ and the LISA, we could also consider calculating a Moran's $I$ on the residuals of the OLS regression. This would tell us, even if we had NOT yet done a Moran's $I$ or LISA, that whether we should pay more attention to our spatial relationships and explicitly include them in our regression model. 

$H_0$: The null hypothesis is that the residuals are not spatially autocorrelated. Thus, the OLS is not misspecified and we do not need to worry about spatial relationships in our target.

```{r}
## Moran's I test on residuals
moran.test(residuals(OLS), states_sw)
```

#### **Question 17B**: [1.5 points]
- What would we conclude from the results of the Moran's $I$ test on the __residuals__? 

> Your answer here. By Collins
p-value = 0.06133 > 0.05 . since the P-value is greater than significance level (0.05), we fail to reject the null hypothesis and conclude that there is no significant evidence of spatial autocorrelation in the residuals of the OLS regression model

- If we had started here, rather than calculating the Moran's $I$ and LISA first, could we have been potentially mistaken about spatial relationships? Why or why not?

> Your answer here. By Collins
Yes we would have been mistaken because both Moran's $I$ and LISA have already shown there is spatial relationships. Yet at this point the moran (I) test on OLS residuals shows there is no spatial relationship which would have been misleading without prior knowledge.

- Why might these results disagree with what we found from the Moran's $I$ and LISA on the scaled target?

> Your answer here.By Collins
Am not very sure of my findings here but after a little research, I have found out that the Moran's (I) test on the residuals are disagreeing with what we found from Moran's (I) and LISA on the scaled target because the Moran's (I) test on the residuals is testing for spatial Autocorrelation in the residuals of the OLS regression model, whereas the Moran's I and LISA on the scaled target are testing for spatial relationships in the target variable itself. The Moran's I test on the residuals is asking whether the residuals are randomly distributed in space, or if there is a pattern to their distribution. If the residuals are not spatially autocorrelated, it means that the OLS regression model has accounted for the spatial relationships in the data.
on the other hand, The Moran's I  and LISA test on the scaled target are asking whether there are spatial relationships in the target variable itself, If there are significant spatially relationships in the target variable, it means that the variable is not randomly distributed in space and there may be underlying spatial processes that are driving the patterns.

### 2.4.1.4 Fit the SAR model
The `lagsarlm()` from the `spatialreg` package will fit the SAR for us, allowing us to specify the spatial weights matrix, `states_sw`, that we calculated all the way back [here](#findSW).

```{r}
## SAR (spatial lag model)
SAR <- lagsarlm(bc_PredictedReadmissionRate ~ ., data = sarTrain, 
                      listw = states_sw)

summary(SAR)
```

### 2.4.1.5 Interpret the SAR model 
The `summary()` output of the SAR model shows a lot going on. Let's go through the three key pieces of evidence we want to assess from these results.

1. Spatial Autoregressive Coefficient ($\rho$, "Rho")

Here, we find that $\rho = 0.33291$ with an estimated p-value of $p = 0.012065$ based on a Z-statistic of $Z = 2.5102$. What the Wald Z-test is doing is testing whether $\rho = 0$, just like in Pearson correlation tests! 

**CONCLUSION**: With a p-value LESS than 0.05, we would conclude that $p \ne 0$ - thus there is *sufficient spatial dependency in the target variable to justify including the lag term in this model*. That's opposite what the Moran's $I$ on the residuals implied but feels very consistent with what we found for both global and local Moran's $I$ earlier.  

2. Likelihood Ratio Test

More generally, Likelihood Ratio Tests (LRTs) are used to compare **well one nested regression model fits compared to an unnested model**. If you've ever done stepwise regression, you've inherently done a Likelihood Ratio Test possibly without knowing it! Note that because SAR is a nested model, it reduces to OLS when the spatial lag coefficient $\rho = 0$. When there is NO spatial lag ($\rho = 0$), the first term of the SAR model cancels out! 

The `lagsarlm()` unfortunately will not automatically compare the Spatial Lag Model (SAR) with the Ordinary Least Squares (OLS) model - **but we can**. We will do this by separately performing a Likelihood Ratio Test (LRT). But the LRT test is very simple. The formula for the likelihood ratio (LR) of the two models can be found by:

$LR = 2 \times (\log L_{SAR}  - \log L_{OLS})$ 

where $\log L_{SAR}$ is the log-likelihood of the SAR model and $\log L_{OLS}$ of the OLS model. The LR test statistic follows a $\chi^2$ distribution with 1 degree of freedom. 

But if that feels too mathy for you, just remember this: **A LR of 0 means there is NO difference between the two models being compared, but the bigger the LR the higher the probability that the two models are different.** Thus, our $H_0$ (null hypothesis) is that there is no difference between the SAR model and the OLS. So, if our $p$-value is less than 0.05, we will reject that null hypothesis. 

In the output, the $\log L_{SAR} = -12.75$ but the $\log L_{OLS}$ isn't shown. We can very quickly perform the LRT in `R` using the `anova()` function. **NOTE** that this function is confusingly named here; it will either do an Analysis of Variance (ANOVA) or an Analysis of Deviance, also known as the Likelihood Ratio Test which is what we are doing here!

```{r}
## Give it the larger model first, then the null model.
## SAR is our larger model; the null OLS is nested within the SAR because
## the SAR = OLS when rho = 0!
anova(SAR, OLS)
```

This tells us that the log-likelihood of SAR ($\log L_{SAR}$) is -12.75, the log-likelihood of OLS is ($\log L_{OLS}$) is -15.146, and the likelihood ratio (LR) statistic is 4.7905 with an estimated $p$-value of 0.028616. Although it does not say this, the degrees of freedom is 1 here: 19 df from SAR - 18 df from OLS. 

Thus, if we wanted to do it by "hand" (i.e., calculate it according to the formula), it would look like this:

```{r}
## Likelihood ratio test by "hand"

## Extract the log-likelihoods for each model
logLik_OLS <- logLik(OLS)
logLik_SAR <- logLik(SAR)

## Calculate the likelihood ratio (LR) - make sure to use as.numeric()!
LR <- 2 * (as.numeric(logLik_SAR) - as.numeric(logLik_OLS))
## Find the estimated p-value for the LR statistic using the chi-squared 
## distribution. Use the df = 19-18 that we observed above. 
p <- pchisq(LR, df = 1, lower.tail = FALSE)

## Print the results!
cat("The likelihood ratio LR =", LR, "with 1 degree of freedom has p =", p)
```

Which is the same as what the `anova()` function gave us!

**CONCLUSION**: With a $p$-value LESS than 0.05, we would conclude that there is a difference between the two models. The more complex model, SAR, adds sufficient information to the model to help its fit. Thus, there is *sufficient evidence that a spatial model improves our predictive ability over what an OLS alone would*. Again, that's opposite what the Moran's $I$ on the residuals implied but feels very consistent with what we found for both global and local Moran's $I$ earlier.  

3. Lagrange Multiplier (LM) Test for Residual Spatial Autocorrelation

The LM test is based on the spatially lagged residuals and compares the residual spatial pattern to what would be expected under independence - meaning that it's similar to the test we did earlier when we tested whether there was a spatial pattern to the OLS residuals! These are very similar tests, albeit calculated in slightly different ways. 

The null hypothesis ($H_0$) again is that there is NO spatial pattern to the residuals; the difference now is that **we are testing if there's still "leftover" spatial autocorrelation in the residuals AFTER fitting our SAR model**. In other words - did the SAR model do a sufficiently good job at dealing with spatial autocorrelation? If yes, yay! We're done! If no, we'd need to do a Spatial Error Model (SEM) or Spatial Durbin Model (SDM), the other two types of regression I alluded to wayyyy back at the beginning of this section.
 
Our LM test statistic, per the `summary()` output, is 0.05318 with a $p$-value of 0.81762. 

**CONCLUSION**: With a $p$-value MORE than 0.05, we could conclude that there is NO residual spatial autocorrelation after fitting the SAR. Thus, the SAR model is sufficient for explaining the spatial autocorrelation in the target!

#### **Question 18**: [3 points]
Briefly summarize everything we know at this point: (1) Global Moran's $I$ results, (2) Local Moran's $I$ (LISA) results, (3) Moran's $I$ on the residuals of the OLS model, (4) the test for spatial autoregression using the coefficient $\rho$, (5) the Likelihood Ratio Test (LRT), and (6) the LM test for residual spatial autocorrelation after fitting the SAR model. DO NOT simply copy my interpretations but instead try to make sure you understand each of these pieces of evidence and what they tell us.

                    TAKE A 2ND LOOK AT THIS PART!!!!!!

> 1. Global Moran's I is a measure used to assess the overall spatial autocorrelation in a dataset. It ranges from -1(indicating perfect dispersion) to +1 (indicating perfect clustering), with values equal to zero indicating randomness. A significant positive Global Moran's I value indicates that similar values tend to cluster together spatially, suggesting the presence of spatial autocorrelation.
Global Moran's $I$ results. Our global spatial autocorrelation test found a Moran's $I$ of 0.268 with a $p$-value of 0.001397, which is less than 0.05. Thus, we reject the $H_0$ of a random distribution and conclude that we have a significant positive spatial autocorrelation for median pneumonia-related readmissions! This suggests that states with similar median readmission rates tend to cluster geographically in the U.S 

> 2. Local Moran's (LISA Local Indicators Of Spatial Association), provides insights into spatial autocorrelation at a local level, identifying clusters, outliers, and spatial regimes.
Local Moran's $I$ (LISA) results.Hotspots (Quadrant I, "High-High") will be states that have:
 - A significant $p$-value ($p < 0.05$), which suggests that local $I_i$ is significantly positive ($I_i > 0$) 
 - A centered median hospital pneumonia-related readmissions rate that is positive
 - A spatially-lagged median hospital pneumonia-related readmissions rate that is positive

>  A significant $p$-value ($p < 0.05$), which suggests that local $I_i$ is significantly negative ($I_i < 0$) 
 - A centered median hospital pneumonia-related readmissions rate that is negative
 - A spatially-lagged median hospital pneumonia-related readmissions rate that is negative

> 3. Moran's $I$ on the residuals of the OLS model. this assesses whether residuals from OLS regression model exhibit spatial autocorrelation.
p-value = 0.06133 > 0.05 . since the P-value is greater than significance level (0.05), we fail to reject the null hypothesis and conclude that there is no significant evidence of spatial autocorrelation in the residuals of the OLS regression model.

> 4. The test for spatial autoregression using the coefficient $\rho$. Coefficient p (rho) measures the strength of patial autocorrelation among the dependent variable in autoregressive (SAR). Asignificant rho (p) value indicates the presence of autoregression, meaning that the value of the dependent variable at one location is influenced by the values at neighboring locations.  
$\rho = 0.33291$ with an estimated p-value of $p = 0.012065$ based on a Z-statistic of $Z = 2.5102$. With a p-value LESS than 0.05, we would conclude that $p \ne 0$ - thus there is *sufficient spatial dependency in the target variable to justify including the lag term in this model

> 5. the Likelihood Ratio Test (LRT) compares the fit of two models, typically a restricted model (eg., OLS) and an unrestricted model (eg., SAR), to determine if the additional parameters in the unrestricted model significantly improve the fit. A significant LRT results suggests that the SAR model provides a better fit to the data than the OLS model, indicating the presence of spatial effects.
likelihood ratio (LR) statistic is 4.7905 with an estimated $p$-value of 0.028616.With a $p$-value LESS than 0.05, we would conclude that there is a difference between the two models. The more complex model, SAR, adds sufficient information to the model to help its fit. Thus, there is *sufficient evidence that a spatial model improves our predictive ability over what an OLS alone would 

> 6. The Lagrange Multiplier (LM) test for residual spatial autocorrelation after fitting the SAR model. The LM test is based on the spatially lagged residuals and compares the residual spatial pattern to what would be expected under independence.With a $p$-value MORE than 0.05, we could conclude that there is NO residual spatial autocorrelation after fitting the SAR. Thus, the SAR model is sufficient for explaining the spatial autocorrelation in the target

What is your **final takeaway**? Do we need to account for spatial autocorrelation in our regression model?
> No, OLS residuals show no existence of spacial autocorrelation

### 2.4.1.6 Inferential statistics - which predictors are important?

```{r, echo = FALSE}
## Print a table using gtsummary
tbl_regression(SAR) %>% 
  modify_caption("Table 7. Spatial Lag (SAR) Regression results.") %>% 
  modify_header(label = "**Coefficient**") %>% 
  add_significance_stars(hide_p = FALSE)
```

#### **Question 19**: [2 points]
We can think of dealing with **spatial spillover** as helping us distinguish the wheat from the chaff (truth from noise). **Interpret** which of the coefficients are significant ($p$-value < 0.05) AND compare that to both Table 10 (parsimonious OLS model results) AND Figure 20 (Feature Importance from Elastic Net) from __Project 1__. Now that we are accounting for spatial autocorrelation, which predictors are still important? Did any become significant (important) now that weren't before?

> Your answer here. By Collins
1. Rho p-value = 0.012 < 0.05
2. Perioperative pulmonary embolism or deep vein thrombosis rate P-value	= 0.001 < 0.05
3. Medicare spending per patient P-value = 0.001 < 0.05
4. Quiteness P-value = 0.019 < 0.05
Table 20:feature importance as determined by Elastic Net
1. Doctor communication
2. Hospital return days for pneumonia patients
3. Medicare spending per patient
In comparison only Medicare spending seems to feature in Elastic Net and still show significance in inferential statistics

### 2.4.1.6 Inferential statistics - which predictors are important?

Recall that our goal for our stakeholder is to be able to __predict(!)__ which hospitals perform better than others. They want to make a deployable tool for patients/customers. So, let's test the predictive ability of our model by using our testing data to calculate the $RMSE$. 

1. _First, join the testing data with the shapefile, then create a test dataset suitable for SAR:_

This is called "spatially embedding" our testing data. Notice that we are using the spatial weights **from the training set** to prevent data leakage!!

```{r}
## Join the shapefile with spatial weights and neighbor lists back to the 
## testing data
states_sf_AggTest <- left_join(states_sf, stateAggTest, 
                                by = c("ID" = "region")) #if we use the same shape files that we made from training set is that not data leakage? we are introducing training set data into test set
            ## Start from the testing data
sarTest <- states_sf_AggTest %>% 
  ## Make it a dataframe/tibble
  as_data_frame() %>% 
  ## Move the ID column to the rownames
  column_to_rownames("ID") %>% 
  ## Drop the features that we dropped before the analysis in Project 1, as 
  ## well as the features we do NOT want to use as predictors!
  select(-Median_RawPredictedReadmissionRate, 
         -Median_RawMedicareSpending, 
         -Median_RawHospitalReturnDays, 
         -Median_RawDeathRate,
         -geom) %>% 
  ## Have to drop any missing values from the testing data
  drop_na()
```

2. _Now, perform the predictions using the testing data and the SAR model we already fit:_

The `spatialreg` package has its own version of the `predict` function, which will take the SAR model, the testing data, and the spatial weights (`states_sw`).

```{r}
predictions <- predict(SAR, newdata = sarTest, listw = states_sw) %>% as_tibble()
## Returns as a list; so make sure to turn into a dataframe for convenience
```

```{r, echo = FALSE, warning = FALSE, message=FALSE}
predictions %>% 
  ## Add a new column for the states, which comes from the rownames
  mutate(State = rownames(sarTest)) %>% 
  ## Reorganize the columns to show State first, then everything else
  select(State, everything()) %>% 
  ## Just show the first 6 rows
  head() %>% 
  kable(digits = 2,
    format = "html",
    caption = "Table 8. First 6 rows of SAR predictions using the testing data.") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
  )
```

We can see that the predictions contain the following components:

- `fit`: the __predicted outcome__ $\hat{y}$; here, this includes the influence of the spatial lag!
- `trend`: the combination of linear predictor(s) for each value of the target $y$; here, this is a combination of all $\beta_1X_1 + ... \beta_nX_n$ in our model - so all of the predictors and their coefficients
- `signal`: the spatial term, $\rho Wy$; this is the influence that the spatial autocorrelation has on each value of the target $y$ 

3. _Calculate the RMSE:_

We will use the `fit` component from the predictions to calculate RMSE. 
```{r}
## Calculate RMSE using the `fit` values
sarRMSE <- sqrt(mean((sarTest$bc_PredictedReadmissionRate - predictions$fit)^2))
```

4. _Compare that with the RMSE from the OLS:_

To be able to determine how much better a job its doing (if any), we need to compare the RMSE of the new OLS (not the one from Project 1) to the RMSE from the SAR model we just calculated.

```{r}
olsPredictions <- predict(OLS, newdata = sarTest)
olsRMSE <- sqrt(mean((sarTest$bc_PredictedReadmissionRate - olsPredictions)^2))
```

```{r, echo = FALSE, warning = FALSE, message=FALSE}
## Make a dataframe for comparison of RMSE values
data.frame(Model = c("OLS", "SAR (Spatial Lag)"),
  RMSE = round(c(olsRMSE, sarRMSE), 2)) %>% 
  ## Display with kable
  kable(digits = 2,
    format = "html",
    caption = "Table 9. Comparison of RMSE for OLS and SAR models.") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
  )
```


#### **Question 20A**: [0.5 points]
Why is it challenging to interpret this RMSE in terms of the original/raw median pneumonia-related hospital readmissions? (I know.. I know... you've gotten it by now. Just checking.)

> Your answer here. By Collins
Because this RMSE is calculated using scaled transformed target (pneumonia-related hospital readmissions)

**So, let's add the Mean Absolute Error (MAE) as well, which may be easier to interpret**.
```{r}
## OLS MAE
olsMAE <- mean(abs(sarTest$bc_PredictedReadmissionRate - olsPredictions))

## SAR MAE
sarMAE <- mean(abs(sarTest$bc_PredictedReadmissionRate - predictions$fit))
```

```{r, echo = FALSE, warning = FALSE, message=FALSE}
## Make a dataframe for comparison of RMSE values
data.frame(Model = c("OLS", "SAR (Spatial Lag)"),
  RMSE = round(c(olsRMSE, sarRMSE), 2),
  MAE = round(c(olsMAE, sarMAE),2)) %>% 
  ## Display with kable
  kable(digits = 2,
    format = "html",
    caption = "Table 10. Comparison of RMSE for OLS and SAR models.") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
  )
```

#### **Question 20B**: [1.5 points]
Interpret the MAE **and** the RMSE. Which model is better? Don't forget to interpret the RMSE in terms of the transformed target!

> Your answer here. By Collins
MAE (Mean Absolute Error) measures the average difference between predicted and the actual values. the smaller the MAE the better a model is in predicting the target. In thi regard SAR (Spatial lag) MAE = 0.79 performed better than OLS at 0.84 
RMSE (Root Mean Squared Error) is the square root of the average of squared differences between predicted and actual values. The smaller the RMSE the better the model. In our case SAR(Spatial Lag) = 1.15 performed better than OLS = 1.33 . But considering that this scaled and log transfomed we might need to go back to original data to avoid a misleading interpretation. (Am not sure I answered this "transformed target" correctly!)

### 2.4.2 Other Types of Spatial Regression Models
The results of our SAR showed that we did not need to worry about any other types of spatial regression because the LM Test for spatial autocorrelation of the residuals was not significant. Still, I want to take a brief moment to explain what those models are and how we do them, using our current data as an example. They are important enough alternatives to the SAR we may need them in the future!

**In both of these cases**, you'd consider these if your LM Test for residual spatial autocorrelation was **significant**.

### 2.4.2.1 Spatial Error Model
We've discussed how the SAR only models spatial "spillover" into the target, but we've also peeked at whether we have spatial spillover in the residuals (errors). Why would we care? **One of the fundamental assumptions of an OLS regression is independence of our error terms**, which is something we typically do not test for because we assume we have it based on our research/experiment/analytical design. But the moment we have spatial dependence, well, we may potentially violate this key assumption!

In our case, after controlling for all known predictors and modeling spatial dependence in the target only, we were able to uphold the assumption of independence (that's what the LM test showed). But sometimes, even after SAR, our data may still show similar residual patterns. At such a point, the next step would be to fit a Spatial Error Model (SEM). 

The general structure of the SEM is:

$y = + \lambda W \epsilon + \beta_1 X_1 + ... +\beta_n X_n + E$

where $y$ is the target variable (median pneumonia-related readmissions), $W$ is the spatial weights matrix, which defines the neighborhoods (stored in `states_sw`!), and $\lambda$ is the _spatial error coefficient__. This means that the first term of the equation, $\lambda W \epsilon$, measures the spatial correlation between our errors. So, if $\lambda = 0$, the spatial correlation between the errors isn't enough to bias the standard errors. As with SAR, the remaining terms, $\beta_1 X_1 + \beta_2 X_2 + ... \beta_n X_n$ are just the standard regression coefficients for each of the $n$ predictors. $E$ is an unmeasured error term, which is now the overall error leftover after fitting the SEM regression. 

Fitting the model is pretty easy using the `errorsarlm()` function:

```{r}
## Fit the SEM (spatial error model) using the training data and weights matrix
SEM <- errorsarlm(bc_PredictedReadmissionRate ~ ., data = sarTrain, 
                        listw = states_sw)

## Uncomment to view results!
 summary(SEM)
```

#### **Question 21A**: [1 point]
Test the null hypothesis that $\lambda = 0$, i.e., that there is spatial autocorrelation in the residuals. What p-value do you get and what conclusions do you make? 
#This null hypothesis is contradicting our earlier conclusion that if $\lambda = 0$, the spatial correlation between the errors isn't enough to bias the standard errors

> Your answer here. By Collins
Lambda: 0.61363, p-value: 0.014877 < 0.05 therefore we reject null hypothesis and conclude that there is spatial autocorrelation in the residuals.

> if i have to go by "null hypothesis that, lambda = 0$, i.e., that there is spatial autocorrelation in the residuals" then lambda = 0.61363 is not equal to zero and  p-value: 0.014877 < 0.05 we reject the null hypothesis and conclude that there is no spatial autocorrelation in the residuals

**WHY** might you find a significant SEM but the LM test from the SAR is NOT significant? What does that suggest about our SAR model? (This is not a trick question, we've already discussed it.)

> Your answer here. By Collins
the lm from SAR .With a $p$-value MORE than 0.05, we could conclude that there is NO residual spatial autocorrelation after fitting the SAR. Thus, the SAR model is sufficient for explaining the spatial autocorrelation in the target. I brough this one here to help me understand this question better.

>I think this would suggest the model is not suffcient enough to explain the spatial autocorrelation in the target


### 2.4.2.2 Spatial Durbin Model

The Spatial Durbin Model (SDM) extends the Spatial Lag Model (SAR) by incorporating spatial lags of both the target variable AND the predictors. The idea here is that not there may be spatial autocorrelation in not only the target but also some (if not all) of the predictors. For example, if there was a policy or some underlying demography that affected the pneumonia-related hospital readmissions of one state that ALSO influenced that of a neighboring state, and we had measured that as a predictor (e.g., poverty rates or the number of elderly in the population), then an SDM would be a good idea provided that you had a significant LM test.

The general structure of the SDM is:

$y = \rho Wy + \{\beta_1 X_1 + ... +\beta_n X_n\} + \{W_1X_1\theta_1 + ... + W_nX_n\theta_n\} + \epsilon$

where $y$ is the target variable (median pneumonia-related readmissions), $W$ is the spatial weights matrix, which defines the neighborhoods (stored in `states_sw`!), and $\rho$ is the spatial lag coefficient, making the first term of the equation, $\rho Wy$, the same as it was in SAR! The next term, $\beta_1 X_1 + \beta_2 X_2 + ... \beta_n X_n$ are just the standard regression coefficients for each of the $n$ predictors. Then comes the third term, $\{W_1X_1\theta_1 + ... + W_nX_n\theta_n\}$, which represents EACH of the spatially-lagged predictors we are including. Note that we would not have to spatially lag all predictors. As before, $\epsilon$ is an unmeasured error term. 

So, just by looking at all that mathy-math, we can see this model is a lot more complicated! The advantage, however, is that it would allow us to account for spatial autocorrelation in the target and predictors simultaneously, which is immensely powerful. And I like power! (Statistical power, that is.)

Fitting the model is pretty easy using the `lagsarlm()` function:

```{r}
SDM <- lagsarlm(formula = bc_PredictedReadmissionRate ~ ., data = sarTrain, 
  listw = states_sw, type = "mixed") ## "mixed" is necessary to specify as SDM

## Uncomment to view results!
 summary(SDM)
```

**Now, you might notice something a bit odd here**. Now, the test of the $H_0: \rho = 0$ is NOT significant (p = 0.17868) but the LM test IS significant (p = 0.012548), which is a **complete reversal**. What gives?!

The reality is that we likely have **yet unaccounted for spatial structure in the data**, in the form of OMITTED VARIABLES. Yikes! How can we deal with that if we have no idea what we omitted?!

#### TAKEAWAYS 

- The SAR did a "good enough" job at inference. We've muddied the waters a bit by doing the SDM. Sometimes "good enough" is totally okay! However, don't forget that for **prediction** purposes, the SAR model is not going to work. 

- There are very likely to be **omitted** spatial variables. We haven't, for example, included anything about underlying demographics or **Social Determinants of Health**. 

- If we were determined to improve this, we would either 
  - (1) Try a Spatial Durbin Error Model (SDEM) or 
  - (2) Try to find the ommitted variables. **We are going to go with option 2!**


#### **Question 21B**: [1 point]
Briefly summarize a possible workflow, based on what we've done here as well as with help from other sources if needed, to use (1) Moran's $I$ and/or LISA, (2) SAR, (3) SEM, and (4) SDM. Table 8 (below) may help you or you're free to refer to it to save writing. Feel free to write this is as numbered or bulleted list, if you'd like!

> Your answer here.
1. Moran's (I) is a way to measure clustering and will tell us if clustering exist overall. It ranges from -1 to +1, positive indicate clustering while negative indicate dispersion. This is the best way to begin testing for overall spatial Association in a general view.
2. LISA (Local Indicators of Spatial Association). This follows global moran I test, though unlike it, LISA helps pinpoint specific states that significantly contributes to global pattern. LISA, identifeis clusters of similar values high-high (hotspots) and Low-low (Coldspots), its also used to test High-low and Low-High (outliers). Global Moran's I and LISA are used for exploratory spatial data analysis to identify patterns of spatial autocorrelation and local clusters.
3. SAR(Spatial Autoregressive Model). Its applied when there is spatial dependence in the target variable, indicating that the outcome in one area is influenced by neighboring outcomes. That is the value of a variable at a location is influenced by the values of the same variable at neighboring locations. this is captured through a spatial weights matrix that defines the relationships between locations
4. SEM (Spatial Error Model) is a type of spatial regression model used to account for spatial autocorrelation in the error terms of regression analysis. Its like a detective, sniffing out unmeasured spatial effects that might be hiding in the residuals or omitted variables that are not captured by the predictors in the model.
5. SDM (Spatial Durbin Model). Its a spatial regression model that accounts for spatial dependence in both the outcome variable and the predictor variables, It includes the spatial lagged values of both the dependent variable and the idependent variables. This allows SDM to capture the spatial spillover effects of the predictors on the outcone variable. In short its like a master strategist, accounting for both local and neighboring effects of predictors and outcomes.

>The workflow should start with exploratoey analysis using Moran's I and/ or Lisa to identify spatial patterns, then based on these results decide whether to use SAR, SEM or SDM. But if simplicity is a consideration then (SAR or SEM) after which you can consider complex more powerful models like SDM.
 

##### Table 11. Comparison of spatial regression models and when to use them.
| **Model**                            | **Spatial Dependence In**   | **Use When**                                               |
| ------------------------------------ | --------------------------- | ---------------------------------------------------------- |
| **SAR** (Spatial Lag/Autoregressive) | Target variable             | Outcome in one area is influenced by neighboring outcomes  |
| **SEM** (Spatial Error)              | Residuals $\epsilon$        | Unmeasured spatial effects or spatial error correlation    |
| **SDM** (Spatial Durbin)             | Both target and predictors  | Capture spatial spillover of both outcomes and predictors  |


#### **Question 22**: [2 points]
**DATA SCIENCE EXTENSIONS OF SPATIAL MODELING** $\rightarrow$ At this point, it should feel clear that we have spatial influence in our data with state-level differences in pneumonia-related hospital readmissions. Yet, building a robust predictive model is still not within our grasp. How could we get there?

Our next steps will involve moving in the direction of __adding omitted spatially correlated features__, but your first task is a thought experiment and a little research. 

Do data scientists ever extract the spatially lagged targets/predictors and put them into machine learning models, like Elastic Net - or others like Random Forest or Neural Networks? Make sure to give an example from a scholarly or journalistic article. 

**HINT 1**: Searching [Google Scholar](https://scholar.google.com) can be a starting point for academic articles. 

**HINT 2**: The [Harvard Data Science Review](https://hdsr.mitpress.mit.edu) can be a solid starting point for articles that are more hybrid academic/journalistic.

**HINT 3**: [Medium](https://medium.com) or LinkedIn can sometimes have decent how-to guides, but these articles are NOT validated in any way. If you choose an article from here, make sure you validate it with what we've done here and discussed in class!

> Your answer here. By Collins
Yes Data scientist extract spatially lagged targets/predictors and incorporate them into machine learning models. For example a study on landslind hazard spatiotemporal prediction used data-driven models, inluding machine learning and deep learning, to estimate where, when and how big landslinds may occur.
reference
Fang, Z., Wang, Y., van Westen, C., & Lombardo, L. (2024). Landslide hazard spatiotemporal prediction based on data-driven models:Estimating where, when and how large landslide may be.International Journal of Applied Earth Observation and Geoinformation, 126,103631.

#### **Question 23**: [2 points]
**DATA SCIENCE EXTENSIONS OF SPATIAL MODELING** $\rightarrow$ Why is the **state-level** spatial dataset inappropriate for any further machine-learning? In other words, why are we currently restricted __with the dataset exactly as it is__ to just the SAR, SEM, or SDM?

**HINT**: Check the dimensions of `sarTrain` if you need to!

> Your answer here. By Collins
state-level spatial dataset is inappropriate for further machine-learning applications like SAR, SEM, or SDM due to the loss of spatial detail (By aggregating data at the state level, much of the spatial detail and variation within states os lost), risk of ecological fallacy (this is where relationships observed at the aggregated state level do not necessarily hold true at the individual level) and challenges in meeting model assumptions.

#### **Question 24**: [1 point]
**DATA SCIENCE EXTENSIONS OF SPATIAL MODELING** $\rightarrow$ Is there another spatial structure to this dataset that we could explore for use with machine learning instead of state-level?

> Your answer here. By Collins
Yes, I think we could use either Zip code though it might feel too granular, CountyParish, or CityTown.


# 3 Finding omitted spatially-autocorrelated variables

At this stage, we know that the SAR model is "good enough" - if we want to do inference only, that is! The SDM "hinted" at some likely omitted variables that are spatially correlated with the predictors in our dataset. The tricky this is - how do we figure out what we've omitted?

Perhaps you noticed, as I did, that some of the higher rates of pneumonia-related readmissions tend to be in denser regions, like D.C. New Jersey, or states with larger elderly populations, like Florida and California (see [Table 4](#table4)). Perhaps poverty rates are also important, like West Virginia or Kentucky. The point is, we could _speculate_ about possible demographic or socially-determined causes of poor health outcomes, all of which are likely to have their _own_ underlying spatial structure as well! 

#### **Question 25**: [1 point]
What is a **Social Determinant of Health (SDOH)**? Where does this phrase come from?

> Your answer here. By Collins
Social Determinant Of Health (SDOH) originates from World Health Organization (WHO). The WHO, defines SDOH as the circumstances in which people are born, grow, live, work, and age, and the systems put in place to deal with illness. These circumstances are shaped by the distribution of money, power, and resources at global, national, and local levels. SDOH includes factors such as socioeconomic status, education, employment, housing, access to healthcare, and environmental conditions all of e=which can significantly impact an individuals health outcomes.

## 3.1 Pre-processing population-level data to test SDOH hypotheses

Let's bring in some data from the 2023 [US Census](https://www.census.gov) (these are projections based on the 2020 Census), as well from the [Administration for Community Living (ACL)](https://acl.gov) which had conveniently already compiled further data we could use from the US Census. The 2023 US Census Bureau projections were downloaded from [here](https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-detail.html), and the ACL data were downloaded from [here](https://acl.gov/aging-and-disability-in-america/data-and-research/profile-older-americans). For our convenience, I already put them into a `csv` we can import. The geographic measurements also come from the US Census Bureau, which I obtained
[here](https://www.census.gov/geographies/reference-files/2010/geo/state-area.html). 

**Note** that, if our administrative data from the Centers for Medicare and Medicaid Services (CMS) that we're using are from fiscal year 2024, then we would want to use the 2023 census data! This is because of [how federal government fiscal years run](https://www.usa.gov/federal-budget-process).

```{r}
## Read in, calculate the percent of the total adult population over 65,
## Make the states lowercase
## Calculate population density as hundreds of people per square mile

pop <- read_csv("2023_US_Census.csv", show_col_types = FALSE) %>% 
  mutate(over65PercentPop = over65Pop / adultPop,
         region = tolower(region),
         popDensity = (totalPop/100)/(landSqMi))      
```

```{r, echo = FALSE, warning = FALSE, message=FALSE}
pop %>% 
  arrange() %>% 
  top_n(6) %>% 
   kable(digits = 2,
    format = "html",
    caption = "Table 12. First 6 rows of US Census Bureau data.") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
  )
```

**Take a look** at the original `csv` file. It included `totalPop` (includes children), `adultPop` (all individuals 16+ years), `over65pop` (all adults 65+ years), `percBelowPoverty` (percent of the total population living at or below the Poverty Index for 2023), and `landSqMi` (the land area in square miles). I then calculated the `over65PercentPop` as the percent of the population that is over the age of 65 relative to the total adult population over the age of 65. I also calculated `popDensity`, which is the population per land in square miles. This gives us a better sense of how dense vs. spread out a geographic area may be. Note too that these data have _already been aggregated to the state-level_. 


**Lastly**, let's join the census data with the training and testing data (`states_sf_AggTrain` and `states_sf_AggTest`) so that we have those data downstream for analysis.

```{r}
## Join with the states aggregated shapefile trainig data from Section 2 
states_sf_AggTrain <- left_join(states_sf_AggTrain, pop, 
                            by = c("ID" = "region"))

## And do testing data as well:
states_sf_AggTest <- left_join(states_sf_AggTest, pop, 
                            by = c("ID" = "region"))
```

## 3.2 Testing Hypotheses: Readmissions by Population Density

I didn't formally lay these out as **hypotheses** (statements that explain WHY something happens) that we could test, but I sure could! Let's start by formulating and testing a hypothesis about population density. 

**Formulation of Hypothesis 1**: States with high population density have higher median pneumonia-related readmissions **because** denser populations support more hospital facilities, are likelier to reflect more urban areas, and may even enable faster spread of pneumonia as compared to states with less dense populations.

We have a hypothesis now, but how can we test it? (Hypotheses will always be a "because" statement!)

### 3.2.1 Heuristic (Visual) Tests

A heuristic is a solution that we arrive at by trial-and-error or, very oftenly, by visual or some other kind of inspection. It's different from statistical testing but often as valid because it allows us to **explore** the data. Every time we do Exploratory Data Analysis we're doing heuristic assessment of our datasets!

So, let's use our mapping skills to help us to assess whether we think there is any support for a correlation between population density and median pneumonia-related hospital readmissions. We will print four plots (one we've previously made, `p1`) for analysis.

#### **Question 26**: [1 point]
Add your comments to the code that makes the three new plots: `p2a`, `p3a`, and `p4a`. 

> Answer in the comments.

```{r, echo = TRUE, warning = FALSE, message = FALSE}
## Make a map `p2a` of population density:
p2a <- states_sf_AggTrain %>% 
## COMMENT HERE this line filters out district of columbia
  filter(ID != "district of columbia") %>%  ## <- Comment this out to see why!By commenting this out, that is by including district of columbia, P2a becomes very pale, that is the red color fades!
## COMMENT HERE creating a ggplot object for mapping pop density
ggplot() +
  ## COMMENT HERE adding a geop_shape files layer
  geom_sf(aes(fill = popDensity), color = "grey20", alpha = 1, size = 0.2) +
  ## COMMENT HERE customizing the fill color scale for pop density
  scale_fill_gradient(
    name = "Hundreds per sq. mi.",
    low = "white", high = skittles[3],
    na.value = "grey90"
  ) +
  ## COMMENT HERE adding a title and caption to the map
  labs(title = "Adult population density by state",
    caption = "Source: U.S. Census Bureau, 2023") +
  ## COMMENT HERE setting the background for the map
  theme(
    panel.grid = element_blank(),
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white"),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom"
  ) +
  ## COMMENT HERE sets the coordinate system for tthe map of pop density by state
  coord_sf()

## Make a scatterplot called `p3a` with best-fit reference line between 
## population density and the pneumonia-related hospital readmissions

p3a <- states_sf_AggTrain %>% 
## COMMENT HERE filter out district of columbia
  filter(ID != "district of columbia") %>%  ## <- Comment this out to see why! The scatter points vanishes from the middle of the plot and aligns themselves on the y-axis
## COMMENT HERE creating a ggplot object for mapping pop density and medium raw predicted readmission rate
ggplot(aes(x = popDensity, y = Median_RawPredictedReadmissionRate)) +
## COMMENT HERE adding a geom layer for creating scatterplot with points
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "lm", se = T, color = skittles[3]) +
  theme_minimal() +
  ## COMMENT HERE  adding labels to give the map a title and axis labels
  labs(title = "Hospital readmissions by population density",
       x = "Hundreds per sq. mi.",
       y = "Predicted Pneumonia-related Hospital Readmissions")

## Make a barplot `p4a` that shows the highest density states with their median
p4a <- states_sf_AggTrain %>% 
## COMMENT HERE filtering out district of columbia
  filter(ID != "district of columbia") %>%  ## <- Comment this out to see why! it occupies position one as the most densily populated state and ohio exist top 10
  ## COMMENT HERE arranging pop density in descending order
  arrange(desc(popDensity)) %>% 
  ## COMMENT HERE show the head with only first 10 rows
  head(10) %>% 
  ## COMMENT HERE creating a ggplot object for mapping pop density
ggplot(aes(x = fct_reorder(ID, popDensity), y = Median_RawPredictedReadmissionRate)) +
  ## COMMENT HERE adding a customised color
  geom_col(fill = skittles[3]) +
  theme_minimal() + 
  ## COMMENT HERE this swaps the x and y coordinates of this plot
  coord_flip() +
  ## COMMENT HERE adding labels that is title and axis
  labs(title = "Top 10 Most Dense States",
     y = "Median % hospital readmissions",
     x = "",
     caption = "Source: CMS, 2024 & U.S. Census, 2023; n.b. Wash D.C. excluded")
```

```{r, echo=FALSE}
## This prints the plots in a nice arrangement when knitted
p1
p2a
grid.arrange(p3a, p4a, ncol = 2)
```

#### **Question 27A**: [0.5 points]
Compare the new map, `p2` of population density to the original map, `p1`. It may be hard to tell at first, but do you see a correlation between population density and median pneumonia-related admissions?

> Your answer here.By Collins
yes there is some resemblance with the exception of Nevada and North Dakota. the once that i have noticed

#### **Question 27B**: [0.5 points]
Now look at the scatterplot, `p3`, and the column graph, `p4`. What conclusions can you make, if any, from these two graphs and why?

> Your answer here. By Collins
The scatter plot shows a positive correlation between predicted pneumonia-related hospital readmissions and population density, however there is some variation in data and the effects of outliers is very strong.
>The bar chart shows new jersey as the most densily populated state with the highest readmission rate, however Rhode Island, Massachusetts, Connecticut, and delaware though densily populated show less readmission rate compared to other less densily populated with higher rates of readmissions (Florida, New YOrk, Pennsylvania, and Ohio)

### 3.2.2 Adding Population Density to the SAR

Now, we can also calculate the spatial statistics using the updated dataset now that we've add population density. Let's make a new `sarTrain` and `sarTest` so we can repeat the spatial regressions we did in Part 2.

#### 3.2.2.1 Make the updated `sarTrain` and `sarTest`
```{r}
## TRAINING
            ## Start from the training data
sarTrain <- states_sf_AggTrain %>% 
  ## Make it a dataframe/tibble
  as_data_frame() %>% 
  ## Drop the features that we dropped before the analysis in Project 1, as 
  ## well as the features we do NOT want to use as predictors!
  select(-Median_RawPredictedReadmissionRate, 
         -Median_RawMedicareSpending, 
         -Median_RawHospitalReturnDays, 
         -Median_RawDeathRate, 
         -ID,
         -localI, 
         -pValue, 
         -quadrant, 
         -geom, 
         -lag_medianReadmissions,
  ## Make sure to remove any correlated variables/variables we don't want to
  ## test from the Census data!!
         -totalPop,
         -adultPop,
         -over65Pop, 
         -percBelowPoverty,
         -landSqMi, 
         -over65PercentPop)

## TESTING
            ## Start from the testing data
sarTest <- states_sf_AggTest %>% 
  ## Make it a dataframe/tibble
  as_data_frame() %>% 
  ## Move the ID column to the rownames
  column_to_rownames("ID") %>% 
  ## Drop the features that we dropped before the analysis in Project 1, as 
  ## well as the features we do NOT want to use as predictors!
  select(-Median_RawPredictedReadmissionRate, 
         -Median_RawMedicareSpending, 
         -Median_RawHospitalReturnDays, 
         -Median_RawDeathRate,
         -geom,
         ## Make sure to remove any correlated variables/variables we don't want to
         ## test from the Census data!!
         -totalPop,
         -adultPop,
         -over65Pop, 
         -percBelowPoverty,
         -landSqMi, 
         -over65PercentPop) %>% 
  ## Have to drop any missing values from the testing data
  drop_na()
```

#### **Question 28**: [3 points]
We are about to walk through the spatial regressions again now that we have added `popDensity` to the dataset. You will need to answer an interpretation question after **each** code chunk!

#### 3.2.2.2 Fit the OLS regression and test Moran's I on the residuals
```{r}
OLS <- lm(bc_PredictedReadmissionRate ~ ., data = sarTrain)
## Moran's I test on residuals
moran.test(residuals(OLS), states_sw)
```

**QUESTION**: What does Moran's $I$ test on the residuals tell us about spatial autocorrelation of the residual (error) terms?

> Your answer here.By Collins
Moran I statistic =  0.10453179 since Moran's I is positive but on the lower end of 1, tells us we there is a very weak possiblity of clustering correlation
p-value = 0.09832 > 0.05 this indicates that the pvalue is greater than our significance level, therefore we fail to reject null hypothesis and conclude there is no spatial outcorrelation in our dataset.

#### 3.2.2.3 Fit the SAR and look at $\rho$ and the LM test
```{r}
## SAR (spatial lag model)
SAR <- lagsarlm(bc_PredictedReadmissionRate ~ ., data = sarTrain, 
                listw = states_sw)
## Uncomment to view results!
 summary(SAR)
```

**QUESTION**: What is the value of $\rho$ and its $p$-value? Does it suggest that we have spatial dependence in our target variable still? Are you surprised? What about the LM test?

> Your answer here. By Collins
Rho: 0.35443 and p-value: 0.016887 < 0.05 . since pvalue is less than 0.05 significance level, the we reject null hypothesis and conclude there is spatial dependence in the target variable bc_PredictedReadmissionRate.
> LM test for residual autocorrelation test value: 0.16512 and the p-value: 0.68448 > 0.05. Since the pvalue is greater than 0.05 siginficance level, we fail to reject null hypothesis and conclude there is no spatial autocorrelation in the residuals

#### 3.2.2.4 Compare the SAR to the OLS model with a Likelihood Ratio Test (LRT)
```{r}
## Give it the larger model first, then the null model.
## SAR is our larger model; the null OLS is nested within the SAR because
## the SAR = OLS when rho = 0!
anova(SAR, OLS)
```

**QUESTION**: Which model is the better fitting model and why?

> Your answer here.By Collins
1. AIC (Akaike Information Criterion). lower AIC values indicate a better model. SAR =  61.513 has a lower AIC value compared to OLS = 65.221. Therefore according to AIC, SAR is better than OLS model.
2. logLik (Log Likelihood) test. Higher log likelihood values indicates a better fit of the model to the data. 
SAR logLik = -10.757 is greter than OLS loglike = -13.611 therefore we conclude that SAR is better.
3.L.Ratio (Likelihood Ratio). a significant L.Ratio indicates that the larger model is a better fit than the null model. for us to test its significance we check the P-Value.
p-value = 0.016887 < 0.05. Since our p-value is greater than 0.05 significance level, the we raject Null hypothesis and conclude that our L.Ratio is significace. Therefore SAR is a better model than OLS
Based on this analysis, SAR is a better fitting model because it has lower AIC, a higher Loglik and a significance L.Ratio with a P-Value less than 0.05


#### 3.2.2.5 Inferential statistics
```{r}
## Print a table using gtsummary
tbl_regression(SAR) %>% 
  modify_caption("Table 7. Spatial Lag (SAR) Regression results.") %>% 
  modify_header(label = "**Coefficient**") %>% 
  add_significance_stars(hide_p = FALSE)
```

**QUESTION**: Did any of the predictors change significance from the first SAR we ran? What about `popDensity`? Is it significant, and how do you interpret its beta coefficient (the slope) AFTER adjusting for spatial dependence in readmission rate?

> Your answer here. By Collins
No the three variables that were significance from the first SAR still are significance here, they have a P-Value of less than 0.05.
> Population density is significant, its P-Value = 0.042 < 0.05 
> population Density beta coeffiecient = 0.01. This means for every one unit increase in population Density, readmission rate increased by 0.01 units. The slope is positive indicating a positive relationship with readmission rate, that is an increase in Popul;ation density by 0.01 units increases readmission rate by 0ne unit. 

#### 3.2.2.6 Fit SEM and SDM
```{r}
## Fit the SEM (spatial error model) using the training data and weights matrix
SEM <- errorsarlm(bc_PredictedReadmissionRate ~ ., data = sarTrain, 
                  listw = states_sw)
## Uncomment to view results!
 summary(SEM)

## Fit the SDM (spatial Durbin model) using the training data and weights matrix
SDM <- lagsarlm(formula = bc_PredictedReadmissionRate ~ ., data = sarTrain, 
    listw = states_sw, type = "mixed") ## "mixed" is necessary to specify as SDM
## Uncomment to view results!
 summary(SDM)
```

#### 3.2.2.6 Predictions using the testing data on SAR, SEM, and SDM

```{r}
## Fit the predictions for the FOUR models using the testing data!
olsPredictions <- predict(OLS, newdata = sarTest)

sarPredictions <- predict(SAR, newdata = sarTest, listw = states_sw) %>% 
  as_tibble()

semPredictions <- predict(SAR, newdata = sarTest, listw = states_sw) %>% 
  as_tibble()

sdmPredictions <- predict(SAR, newdata = sarTest, listw = states_sw) %>% 
  as_tibble()
```

#### 3.2.2.7 Calculate the $RMSE$ and $MAE$ and compare

```{r}
## RMSE
olsRMSE <- sqrt(mean((sarTest$bc_PredictedReadmissionRate - olsPredictions)^2))
sarRMSE <- sqrt(mean((sarTest$bc_PredictedReadmissionRate - sarPredictions$fit)^2))
semRMSE <- sqrt(mean((sarTest$bc_PredictedReadmissionRate - semPredictions$fit)^2))
sdmRMSE <- sqrt(mean((sarTest$bc_PredictedReadmissionRate - sdmPredictions$fit)^2))

## MAE
olsMAE <- mean(abs(sarTest$bc_PredictedReadmissionRate - olsPredictions))
sarMAE <- mean(abs(sarTest$bc_PredictedReadmissionRate - sarPredictions$fit))
semMAE <- mean(abs(sarTest$bc_PredictedReadmissionRate - semPredictions$fit))
sdmMAE <- mean(abs(sarTest$bc_PredictedReadmissionRate - sdmPredictions$fit))

## Make a dataframe for comparison of RMSE and MAE values
data.frame(Model = c("OLS", "SAR (Spatial Lag)", "SEM (Spatial Error)", "SDM (Durbin)"),
           RMSE = round(c(olsRMSE, sarRMSE, semRMSE, sdmRMSE), 2),
           MAE = round(c(olsMAE, sarMAE, semMAE, sdmMAE),2)) %>% 
  ## Display with kable
  kable(digits = 2,
        format = "html",
        caption = "Table 13. Comparison of RMSE and MAE for all models.") %>%
  kable_styling(bootstrap_options = c("hover", full_width = F)
  )
```

**QUESTION**: What does the comparison of the four models' $RMSE$ and $MAE$ imply? Which model is the superior model? Is SAR still "good enough"? Do you think that, despite adding population density, we're likely STILL omitting spatial variables? Explain.

> Your answer here. By Collins
> RMSE (Root Mean Squared Error) the lower the value the better the model, in this regard: SAR,SEM, and SDM have the same value of 1.09 which is lower than OLS at 1.30. therefore they are better.
> MAE (Mean Absolute Error) the lower the value of MAE the better the model, therfore again SAR, SEM, and SDM at 0.74 are better models than OLS at 0.82

Which model is the superior model? 
>Based on RMSE and MAE: SAR, SEM, and SDM are superior to OLS model

Is SAR still "good enough"? 
>Yes SAR is indeed a good model and its performance is comparable to SEM and SDM.

despite adding population density, we're likely STILL omitting spatial variables?
>Despite adding population density, the fact that SAR, SEM and SDM have similar performance suggests that there might still be omitted spatial variables. These models account for spatial dependence, but they may not capture all the underlying spatial relationships. Therefore, it is likely that there are still omitted spatial variables that could improve the model's performance.

## 3.3 Testing Another Hypothesis: Readmissions by ?

The time has finally come for you to choose your own adventure again! There are two more hypotheses we could test using spatially-influenced, social determinants of health (SDOH) that we imported with population density from the U.S. Census. 

#### **Question 29**: [10 points]
Choose ONE of the two hypotheses to explore and, using the code from Section 3.2, walk through both the heuristic and spatial regression tests of your chosen hypothesis. The hypotheses have been provided for you this time. 

- **Hypothesis 2: High Elderly Populations**: States with a higher proportion of their adult population over the age of 65 have higher rates of readmission __because__ of age-related factors that increase the likelihood of severe illness, like pneumonia, and increased risk of complications that could lead to higher hospital readmissions. 

But these are not the only population-related hypotheses we could pose. Perhaps, instead, it has to do with poverty - perhaps states with higher poverty rates also tend to see lower rates of medical coverage, thereby leading hospitals to release patients too early to spare costs. Thus, we might also ask if states with higher poverty rates are more likely to see higher pneumonia readmissions?


## 3.2 Testing Hypotheses: Readmissions by Population Density

I didn't formally lay these out as **hypotheses** (statements that explain WHY something happens) that we could test, but I sure could! Let's start by formulating and testing a hypothesis about population density. 

- **Hypothesis 2: High Elderly Populations**: States with a higher proportion of their adult population over the age of 65 have higher rates of readmission __because__ of age-related factors that increase the likelihood of severe illness, like pneumonia, and increased risk of complications that could lead to higher hospital readmissions.

We have a hypothesis now, but how can we test it? (Hypotheses will always be a "because" statement!)

### 3.2.1 Heuristic (Visual) Tests

A heuristic is a solution that we arrive at by trial-and-error or, very oftenly, by visual or some other kind of inspection. It's different from statistical testing but often as valid because it allows us to **explore** the data. Every time we do Exploratory Data Analysis we're doing heuristic assessment of our datasets!

So, let's use our mapping skills to help us to assess whether we think there is any support for a correlation between High Elderly Populations (over65PercentPop) and median pneumonia-related hospital readmissions. We will print four plots (one we've previously made, `p1`) for analysis.

#### **Question 26**: [1 point]
Add your comments to the code that makes the three new plots: `p2a65`, `p3a65`, and `p4a65`. 

> Answer in the comments.

```{r, echo = TRUE, warning = FALSE, message = FALSE}
## Make a map `p2a65` of elderly (65+) percentage population:
p2a65 <- states_sf_AggTrain %>% 
## COMMENT HERE this line filters out district of columbia
  filter(ID != "district of columbia") %>%  ## <- Comment this out to see why!By commenting this out, that is by including district of columbia, P2a becomes very pale, that is the red color fades!
## COMMENT HERE creating a ggplot object for mapping over65PercentPop
ggplot() +
  ## COMMENT HERE adding a geop_shape files layer
  geom_sf(aes(fill = over65PercentPop), color = "grey20", alpha = 1, size = 0.2) +
  ## COMMENT HERE customizing the fill color scale for population of the elderly
  scale_fill_gradient(
    name = "% of Elderly Population (65+).",
    low = "white", high = skittles[3],
    na.value = "grey90"
  ) +
  ## COMMENT HERE adding a title and caption to the map
  labs(title = " Elderly adult population over 65+ years of age",
    caption = "Source: U.S. Census Bureau, 2023") +
  ## COMMENT HERE setting the background for the map
  theme(
    panel.grid = element_blank(),
    panel.background = element_rect(fill = "white"),
    plot.background = element_rect(fill = "white"),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom"
  ) +
  ## COMMENT HERE sets the coordinate system for the map of over65PercentPop by state
  coord_sf()

## Make a scatterplot called `p3a65` with best-fit reference line between 
## over65PercentPop and the pneumonia-related hospital readmissions

p3a65 <- states_sf_AggTrain %>% 
## COMMENT HERE filter out district of columbia
  filter(ID != "district of columbia") %>%  ## <- Comment this out to see why! The scatter points vanishes from the middle of the plot and aligns themselves on the y-axis
## COMMENT HERE creating a ggplot object for mapping over65PercentPop and medium raw predicted readmission rate
ggplot(aes(x = over65PercentPop, y = Median_RawPredictedReadmissionRate)) +
## COMMENT HERE adding a geom layer for creating scatterplot with points
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "lm", se = T, color = skittles[3]) +
  theme_minimal() +
  ## COMMENT HERE  adding labels to give the map a title and axis labels
  labs(title = "Hospital readmissions by Eldery population 65+",
       x = "% of Elderly Population (65+).",
       y = "Predicted Pneumonia-related Hospital Readmissions")

## Make a barplot `p4a` that shows states with the highest elderly population and their median
p4a65 <- states_sf_AggTrain %>% 
## COMMENT HERE filtering out district of columbia
  filter(ID != "district of columbia") %>%  ## <- Comment this out to see why! it occupies position one as the most densily populated state and ohio exist top 10
  ## COMMENT HERE arranging states with the highest over65PercentPop in descending order
  arrange(desc(over65PercentPop)) %>% 
  ## COMMENT HERE show the head with only first 10 rows
  head(10) %>% 
  ## COMMENT HERE creating a ggplot object for mapping elderly pop 
ggplot(aes(x = fct_reorder(ID, over65PercentPop), y = Median_RawPredictedReadmissionRate)) +
  ## COMMENT HERE adding a customised color
  geom_col(fill = skittles[3]) +
  theme_minimal() + 
  ## COMMENT HERE this swaps the x and y coordinates of this plot
  coord_flip() +
  ## COMMENT HERE adding labels that is title and axis
  labs(title = "Top 10 States with the most Eldery 65+",
     y = "Median % hospital readmissions",
     x = "",
     caption = "Source: CMS, 2024 & U.S. Census, 2023; n.b. Wash D.C. excluded")
```

```{r, echo=FALSE}
## This prints the plots in a nice arrangement when knitted
p1
p2a65
grid.arrange(p3a65, p4a65, ncol = 2)
```

#### **Question 27A**: [0.5 points]
Compare the new map, `p2a65` of elderly (65+) percentage population to the original map, `p1`. It may be hard to tell at first, but do you see a correlation between population density and median pneumonia-related admissions?

> Your answer here.By Collins
No, there is very little resemblance with the exception of florida and West Virginia showing high density in both maps. the once that i have noticed



#### **Question 27B**: [0.5 points]
Now look at the scatterplot, `p3`, and the column graph, `p4`. What conclusions can you make, if any, from these two graphs and why?

> Your answer here. By Collins
The scatter plot shows a weak positive correlation between percentage of the elderly population and predicted pneumonia-related hospital readmissions, however the points are scattered in the middle indicating some degree of uncertainity in the relationship
>The bar chart shows Maine has the highest elderly population with high readmission rate. West Virginia is second in terms of elderly population but has the highest pneumonia related readmission rate. Pennslyvania and New Hampshire even though they rank lowly among top ten elderly population, they still show very high readmission rate

### 3.2.2 Adding Population Density to the SAR

Now, we can also calculate the spatial statistics using the updated dataset now that we've add population density. Let's make a new `sarTrain` and `sarTest` so we can repeat the spatial regressions we did in Part 2.

#### 3.2.2.1 Make the updated `sarTrain` and `sarTest`
```{r}
## TRAINING
            ## Start from the training data
sarTrain <- states_sf_AggTrain %>% 
  ## Make it a dataframe/tibble
  as_data_frame() %>% 
  ## Drop the features that we dropped before the analysis in Project 1, as 
  ## well as the features we do NOT want to use as predictors!
  select(-Median_RawPredictedReadmissionRate, 
         -Median_RawMedicareSpending, 
         -Median_RawHospitalReturnDays, 
         -Median_RawDeathRate, 
         -ID,
         -localI, 
         -pValue, 
         -quadrant, 
         -geom, 
         -lag_medianReadmissions,
  ## Make sure to remove any correlated variables/variables we don't want to
  ## test from the Census data!!
         -totalPop,
         -adultPop,
         -over65Pop, 
         -percBelowPoverty,
         -landSqMi, 
         -popDensity) #replaced over65PercentPop

## TESTING
            ## Start from the testing data
sarTest <- states_sf_AggTest %>% 
  ## Make it a dataframe/tibble
  as_data_frame() %>% 
  ## Move the ID column to the rownames
  column_to_rownames("ID") %>% 
  ## Drop the features that we dropped before the analysis in Project 1, as 
  ## well as the features we do NOT want to use as predictors!
  select(-Median_RawPredictedReadmissionRate, 
         -Median_RawMedicareSpending, 
         -Median_RawHospitalReturnDays, 
         -Median_RawDeathRate,
         -geom,
         ## Make sure to remove any correlated variables/variables we don't want to
         ## test from the Census data!!
         -totalPop,
         -adultPop,
         -over65Pop, 
         -percBelowPoverty,
         -landSqMi, 
         -popDensity) %>% ##replaced over65PercentPop
  ## Have to drop any missing values from the testing data
  drop_na()
```


#### **Question 28**: [3 points]
We are about to walk through the spatial regressions again now that we have added `over65PercentPop` to the dataset. You will need to answer an interpretation question after **each** code chunk!

#### 3.2.2.2 Fit the OLS regression and test Moran's I on the residuals
```{r}
OLS <- lm(bc_PredictedReadmissionRate ~ ., data = sarTrain)
## Moran's I test on residuals
moran.test(residuals(OLS), states_sw)
```


**QUESTION**: What does Moran's $I$ test on the residuals tell us about spatial autocorrelation of the residual (error) terms?

> Your answer here.By Collins
Moran I statistic =  0.132540584 since Moran's I is positive but on the lower end of 1, tells us there is a very weak possiblity of clustering correlation
p-value = 0.05688 > 0.05 this indicates that the P-Value is greater than our significance level, therefore we fail to reject null hypothesis and conclude there is no spatial outcorrelation in our dataset.



#### 3.2.2.3 Fit the SAR and look at $\rho$ and the LM test
```{r}
## SAR (spatial lag model)
SAR <- lagsarlm(bc_PredictedReadmissionRate ~ ., data = sarTrain, 
                listw = states_sw)
## Uncomment to view results!
 summary(SAR)
```

**QUESTION**: What is the value of $\rho$ and its $p$-value? Does it suggest that we have spatial dependence in our target variable still? Are you surprised? What about the LM test?

> Your answer here. By Collins
Rho: 0.36237 and p-value: 0.020514 < 0.05 . since pvalue is less than 0.05 significance level, then we reject null hypothesis and conclude there is spatial dependence in the target variable bc_PredictedReadmissionRate.
> LM test for residual autocorrelation test value:  0.083488 and the p-value: 0.77262 > 0.05. Since the P-Value is greater than 0.05 siginficance level, we fail to reject null hypothesis and conclude there is no spatial autocorrelation in the residuals


#### 3.2.2.4 Compare the SAR to the OLS model with a Likelihood Ratio Test (LRT)
```{r}
## Give it the larger model first, then the null model.
## SAR is our larger model; the null OLS is nested within the SAR because
## the SAR = OLS when rho = 0!
anova(SAR, OLS)
```

**QUESTION**: Which model is the better fitting model and why?

> Your answer here.By Collins
1. AIC (Akaike Information Criterion). lower AIC values indicate a better model. SAR =  64.895 (though larger than population density ) has a lower AIC value compared to OLS = 68.263. Therefore, according to AIC, SAR is better than OLS model.
2. logLik (Log Likelihood) test. Higher log likelihood values indicates a better fit of the model to the data. 
SAR logLik = -12.448 __(-10.757 for Population density)___ is greter than OLS loglike = -15.131 ___(-13.611 for pop density)___ therefore we conclude that SAR is better.
3.L.Ratio (Likelihood Ratio). a significant L.Ratio indicates that the larger model is a better fit than the null model. for us to test its significance we check the P-Value.
p-value = 0.020514 < 0.05 ___(0.016887 for pop density)___ . Since our p-value is greater than 0.05 significance level, the we raject Null hypothesis and conclude that our L.Ratio is significace. Therefore SAR is a better model than OLS
Based on this analysis, SAR is a better fitting model because it has lower AIC, a higher Loglik and a significance L.Ratio with a P-Value less than 0.05




#### 3.2.2.5 Inferential statistics
```{r}
## Print a table using gtsummary
tbl_regression(SAR) %>% 
  modify_caption("Table 7. Spatial Lag (SAR) Regression results.") %>% 
  modify_header(label = "**Coefficient**") %>% 
  add_significance_stars(hide_p = FALSE)
```

**QUESTION**: Did any of the predictors change significance from the first SAR we ran? What about `popDensity`? Is it significant, and how do you interpret its beta coefficient (the slope) AFTER adjusting for spatial dependence in readmission rate?

> Your answer here. By Collins
No the three variables that were significance from the first SAR still are significance here, they have a P-Value of less than 0.05.
> Population o f over 65 years old people is not significant, its P-Value = 0.4 > 0.05 therefore we cannot reject the null hypothesis that the true beta coefficient is zero, suggesting that there may not be a significant relationship between the percentage of the elderly population and predicted pneumonia related readmission rate
> population Density beta coeffiecient = -2.3. This means for every 1% increase in percentage of the population over 65 years old, the predicted pneumonia related readmissions decreases by 2.3 units. The slope is negative indicating a negative relationship with readmission rate, that is an increase in Population of over 65 years old people decrease pneumonia related readmission rate. 

#### 3.2.2.6 Fit SEM and SDM
```{r}
## Fit the SEM (spatial error model) using the training data and weights matrix
SEM <- errorsarlm(bc_PredictedReadmissionRate ~ ., data = sarTrain, 
                  listw = states_sw)
## Uncomment to view results!
 summary(SEM)

## Fit the SDM (spatial Durbin model) using the training data and weights matrix
SDM <- lagsarlm(formula = bc_PredictedReadmissionRate ~ ., data = sarTrain, 
    listw = states_sw, type = "mixed") ## "mixed" is necessary to specify as SDM
## Uncomment to view results!
 summary(SDM)
```
#### 3.2.2.6 Predictions using the testing data on SAR, SEM, and SDM

```{r}
## Fit the predictions for the FOUR models using the testing data!
olsPredictions <- predict(OLS, newdata = sarTest)

sarPredictions <- predict(SAR, newdata = sarTest, listw = states_sw) %>% 
  as_tibble()

semPredictions <- predict(SAR, newdata = sarTest, listw = states_sw) %>% 
  as_tibble()

sdmPredictions <- predict(SAR, newdata = sarTest, listw = states_sw) %>% 
  as_tibble()
```

#### 3.2.2.7 Calculate the $RMSE$ and $MAE$ and compare

```{r}
## RMSE
olsRMSE <- sqrt(mean((sarTest$bc_PredictedReadmissionRate - olsPredictions)^2))
sarRMSE <- sqrt(mean((sarTest$bc_PredictedReadmissionRate - sarPredictions$fit)^2))
semRMSE <- sqrt(mean((sarTest$bc_PredictedReadmissionRate - semPredictions$fit)^2))
sdmRMSE <- sqrt(mean((sarTest$bc_PredictedReadmissionRate - sdmPredictions$fit)^2))

## MAE
olsMAE <- mean(abs(sarTest$bc_PredictedReadmissionRate - olsPredictions))
sarMAE <- mean(abs(sarTest$bc_PredictedReadmissionRate - sarPredictions$fit))
semMAE <- mean(abs(sarTest$bc_PredictedReadmissionRate - semPredictions$fit))
sdmMAE <- mean(abs(sarTest$bc_PredictedReadmissionRate - sdmPredictions$fit))

## Make a dataframe for comparison of RMSE and MAE values
data.frame(Model = c("OLS", "SAR (Spatial Lag)", "SEM (Spatial Error)", "SDM (Durbin)"),
           RMSE = round(c(olsRMSE, sarRMSE, semRMSE, sdmRMSE), 2),
           MAE = round(c(olsMAE, sarMAE, semMAE, sdmMAE),2)) %>% 
  ## Display with kable
  kable(digits = 2,
        format = "html",
        caption = "Table 13. Comparison of RMSE and MAE for all models.") %>%
  kable_styling(bootstrap_options = c("hover", full_width = F)
  )
```

**QUESTION**: What does the comparison of the four models' $RMSE$ and $MAE$ imply? Which model is the superior model? Is SAR still "good enough"? Do you think that, despite adding population density, we're likely STILL omitting spatial variables? Explain.

> Your answer here. By Collins
> RMSE (Root Mean Squared Error) the lower the value the better the model, in this regard: SAR,SEM, and SDM have the same value of 1.13 which is lower than OLS at 1.33. Therefore, they are better.
> MAE (Mean Absolute Error) the lower the value of MAE the better the model, therfore again SAR, SEM, and SDM at 0.79 are better models than OLS at 0.85

Which model is the superior model? 
>Based on RMSE and MAE: SAR, SEM, and SDM are superior to OLS model

Is SAR still "good enough"? 
>Yes SAR is indeed a good model and its performance is comparable to SEM and SDM.

despite adding population density, we're likely STILL omitting spatial variables?
>Despite adding the eldery population (over65PercentPop), the fact that SAR, SEM and SDM have similar performance suggests that there might still be omitted spatial variables. These models account for spatial dependence, but they may not capture all the underlying spatial relationships. Therefore, it is likely that there are still omitted spatial variables that could improve the model's performance.


- **Hypothesis 3: High Poverty Populations**: Higher rates of pneumonia-related readmissions may be associated with higher proportions of the poverty living at or below the poverty threshold __because__ those individuals may have poorer medical coverage, including non- and under-insuredness, or a stronger drive to try to return to work, both of which could in turn pressure some hospitals to discharge sick patients sooner than they otherwise would.

**For full credit**, walk through all of the steps from Section 3.2. Make sure to answer the interpretation questions, too!
