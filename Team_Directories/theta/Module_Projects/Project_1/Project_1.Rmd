---
title: 'Project 1: Biomedical & Clinical Informatics'
author: "Gregory Andriotakis"
date: "20 May 2025"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: flatly
    fig_crop: no
  pdf_document:
    toc: yes
subtitle: 'Merrimack College DSE6630: Healthcare & Life Sciences Analytics'
bibliography: references.bib
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      error = FALSE,
                      cache = TRUE,
                      cache.comments = TRUE,
                      size = 13)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Turn off scientific notation
options(scipen=999)

# Set seed
set.seed(98501)

# Clean, set up, and load
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))

pacman::p_load(tidyverse,
               janitor,
               naniar,
               stringr,
               ggplot2, 
               kableExtra,
               RColorBrewer,
               gridExtra,
               gtsummary,
               prcomp,
               factoextra,
               ggrepel,
               caret,
               missForest,
               mice,
               car,
               stargazer,
               glmnet,
               forcats,
               snakecase
)
```

# 1 Introduction

We will pick up where we left off in Demo 1 to prepare you for analysis. Although this is not going to be as exhaustive as we would like to do if we were doing a complete project, our goal is to see this project go from __messy data__ $\rightarrow$ __prediction & interpretation__. After all, we need to give something to the patient advocacy watchdog that they can deploy!

## 1.1 Our Analysis Plan

Our goal is to see this analysis through to generate a predictive model that the advocacy group can use, and hopefully it will be a decent one. To ensure that it stands the best chance of being a good predictive model, our analysis plan includes all the key steps in the data science life cycle/project flow, including: __cleaning__, __exploration__, __feature selection__, __pre-processing__, __unsupervised machine learning exploration__, and __supervised modeling__. Whew! 

Our specific plan is to perform two big analyses in the hopes of having a nice deliverable to present to our client. The first will be a __segmentation analysis__, or clustering analysis, which will attempt to group our hospitals together on the basis of their attributes and their pneumonia-related readmissions. Thie will be done using a combination of PCA and $k$-means. Our second analysis will be to perform supervised, predictive analysis using an elastic net, which conveniently combines a penalized regression with variable selection so we can point our clients down a path of hospital attributes to focus on, as well as a model they can use to make future predictions.

## 1.2 Your objective.

Your oject is to work through this Project 1. You will have 31 Questions to answer as you work through this, just as with Project 1. Then, you will be able to choose your own adventure again to perform your own __clinical informatics__ analysis:

### Adventure 1. 
__[Minimal coding, focus is on understanding cleaning, pre-processing, and the ML analyses we performed here.]__
Choose ONE other condition (anything other than pneumonia) and run through the analyses again, using our same target variable. You will update your question, hypotheses, and predictions, but will be able to re-use all of the other code with very minor modifications. Credit will be based more on interpretation and not on coding. See more information on Adventure 1 [here](#adventure1).

### Adventure 2. 
__[More coding, but not as much, with a focus on the steps we've undertaken here and extending it to a new question & condition.]__
Choose a slightly more complicated analysis to undertake, for example, focusing on surgical interventions (`HIP-KNEE` & `CABG`) or heart-related conditions (`HF`, `AMI`, & `CABG`). Coding should still be fairly minimal, but you are likely to run into problems with the code working exactly as-is, especially during cleaning. The advantage of this analysis is that it will be much more robust, have larger sample sizes, and would allow us to be able to say something far more informative to a subset of patients (e.g., those considering undergoing surgical interventions). See more information on Adventure 2 [here](#adventure2).

### Adventure 3. 
__[Most coding, but no cleaning or pre-processing, with an emphasis entirely on the machine learning analyses.]__
Continue with the pneumonia-related data that you have here, completely cleaned and pre-processed, and do two more unsupervised OR supervised analyses. For example, you could do another kNN using the $k$ we identify in our Segmentation Analysis here, or you could do a random forest to account for any possible non-linearity. You could even choose to do a Ridge and LASSO regression and compare those analyses to what we get out of an Elastic Net. Any of these are fine, but you must __explain your decision__ in your submission. See more information on Adventure 3 [here](#adventure3).

## 1.3 A Fork in the Road! {#section1.3}

This is the first of your choices in your adventure, and what you choose here could either lead to success or... __sudden death__ (dum, dum, dummmm...). Choose wisely, young data scientists!

## Option 1.3.1 
**Load data from the end of Demo 1, which includes the ordinal and frequency encoding you did (plus a little bonus cleaning to help).** 

Load my version of the fully cleaned, encoded, & ready-to-proceed dataset from the end of Demo 1. These data are not yet ready for analysis (!) but we would be picking up with that here.

```{r}
## Uncomment if this is your choice:
# load("FY2024_data_files/pneumoniaAnalyzeFullyEncoded2024.Rdata")
# dat2Analyze <- pneumoniaAnalyzeFullyEncoded
```

## Option 1.3.2 
**Load data from Demo 1 without ANY encoding, but all other tidying the same.**

```{r}
## Comment out if this is NOT your choice:
load("FY2024_data_files/pneumoniaAnalyzeNoEncoding2024.Rdata")
dat2Analyze <- pneumoniaAnalyzeNoEncoding
```

#### **Question 1**: [1 point]
Which dataset do you choose and why?

> I decided to select the data without encoding.
> This is because Demo 1 performs encoding before a train/test split.
> If we were to then split the data, it is possible that data leakage would occur.
> Therefore, it is best to load the unencoded data and then re-encoding after making the split.

## 1.4 Helper code: Using `R` like an object-oriented language {#helperCode}

Many of you still may be early enough in your programming journeys that you haven't yet learned how to fully leverage the __objected-oriented__ nature of programming languages like `Python`. `R` is not _technically_ an object-oriented language in the same way, but we can hack it to function like one. That's exactly what we are going to do here by loading a __helper function__ directly from the `source` code. 

We went through all the trouble in the Demo to perform encoding and now we get to use it! You will use the `source()` function to load a helper function that I have written, using my answer to the ordinal encoding question of the Demo, so that you can do all your **ordinal** encoding in a single line of code. Neat, huh?

**BUT WHY** are we doing the encoding now if we just chose to bring in non-encoded data? Because we can do __ordinal encoding__ before or after data partitioning with impunity. It does not introduce data leakage to do so here, so we can do it in either order. This means that we can do it now simply for convenience.

**NOTE**: If you chose Option 1, the fully-encoded dataset, you can still run this chunk because there is a conditional in here that will not run your dataset since it is already encoded! But still make sure you understand the idea of `source` code because we will keep using it throughout the course!

```{r, results='hide'}
## This calls the code in the associated .R file
source(file = "doOrdinalEncoding.R")

## This checks to see if you loaded the already encoded dataset
if(!exists("pneumoniaAnalyzeFullyEncoded")) {
    ## This sets a list of PATTERNS to match for the columns to ordinal encode
    encodeList <- c("ComparedToNational_", 
                    "PaymentCategory", 
                    "ValueOfCareCategory",
                    "Score_Emergency department volume")

    ## This runs the function, which takes 3 arguments
    ## Open the source code to see what each argument does!
    dat2Analyze <- doOrdinalEncoding(df = dat2Analyze,
                                     encodeList = encodeList,
                                     quiet = FALSE)
}
```

# 2 Exploratory Data Analysis: Which of the possible target (outcome) variables should we use? (Continued)
**NOTE**: What follows is my (long-winded) answer to **Question 19** from the Demo.

### 2.1 Overview of our options

I am going to quickly break down the differences between the possible targets so we can make sure we really understand **what** they are measuring to allow us to make our most-informed selection of a target. Note: More information was found [in this PDF](https://qualitynet.cms.gov/files/5d0d3ae5764be766b0104c37?filename=Pneumo_ReadmMeasMethod.pdf). **This is really important for helping us choose the right one!**

Notice that I am also including a variable we didn't look at previously called `ComparedToNational_Hospital return days for pneumonia patients`. This is because this is our best assessment of how the hospital is performing with regard to pneumonia readmissions relative to a national average. However, even though I'm including it in the table, I think it's better to think of it as a **solid predictor**. It's telling us how __length of stay (LOS)__ in the hospital after pneumonia readmission stacks up compared to the national average. After a hospital readmission, it's generally better to have _fewer_ return days rather than more because it suggests better quality of care, more timely follow-up, and a greater chance of the patient recovering well at home. Since these are readmissions, having _more_ days than average could also suggest that the hospital missed the severity or didn't properly plan for the patient's recovery at home and released them too early. 

You may be wondering - could we use it as a target? Sure - but it's not for the question we were posed by our stakeholder! Note that, if we did, we would need to perform a multinomial classification analysis. So, instead, let's use this category to help us contextualize our hospitals as we choose between `Predicted`, `Observed`, and `Expected` readmission rates, as well as `Excess Readmission Ratio`. 

```{r, echo = FALSE, collapse=TRUE}
dict <- tribble(
~`Possible Target`, ~`Description`,
"ComparedToNational_Hospital return days for pneumonia patients", "Hospital return days measures the number of days patients spent back in the hospital (in the emergency department, under
observation, or in an inpatient unit) within 30 days after they were first treated and released for pneumonia. Reported as compared to the national average, such that 'below average' is better and 'above average' is worse.", 
"ExpectedReadmissionRate", "The expected number of readmissions in each hospital is estimated using its patient mix and an average hospital-specific intercept. It is thus indirectly standardized to other hospitals with similar case and patient mixes.",
"PredictedReadmissionRate", "The number of readmissions within 30 days predicted based on the hospital’s performance with its observed case mix. The predicted number of readmissions is estimated using a hospital-specific intercept, and is intended to reflect the annual expected performance of the hospital given its historical case and patient mix and performance.",
"ExcessReadmissionRatio", "The ratio of the predicted readmission rate to the expected readmission rate, based on an average hospital with similar patients. Performance is compared against a ratio of one, such that below one is better and above one is worse in terms of readmission rates.",
"observed_readmission_rate", "Our calculation of the observed number of pneumonia-related readmissions within 30 days found by dividing the number of readmissions observed for the hospital during the given period by the number of discharges for that same period, and multiplied by 100 to put it on the same scale as the Predicted and Expected Readmission Rates. It reflects a true observation as reported by the hospital during that period, but is not adjusted for case mixes or prior information for that hospital. Thus, this is a crude, unadjusted value."
)

dict %>% 
  kable(
    format = "html",
    caption = "Table 1. List of possible target variables from the pneumonia-related hospital readmission data. Which one to choose?") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
  )
```

### 2.2 Summary statistics of the possible targets

Now, I know I didn't ask you to do it, but I will actually begin my assessment of which target is most appropriate by constructing a quick summary table. In my assessment of what is an appropriate target, 


```{r, echo = FALSE}
sliceData <- dat2Analyze %>% 
  select(`ComparedToNational_Hospital return days for pneumonia patients`, 
         PredictedReadmissionRate,
         observed_readmission_rate, 
         ExcessReadmissionRatio,
         ExpectedReadmissionRate) %>% 
  rename(`National Comparison Return Hospital Days` = `ComparedToNational_Hospital return days for pneumonia patients`,
         `Predicted Readmission Rate` = PredictedReadmissionRate, 
         `Observed Readmission Rate` = observed_readmission_rate, 
         `ExcessReadmissionRatio` = ExcessReadmissionRatio,
         `Expected Readmission Rate` = ExpectedReadmissionRate) %>% 
  mutate(`National Comparison Return Hospital Days` = ifelse(is.na(`National Comparison Return Hospital Days`), "Unknown", 
                                                      ifelse(`National Comparison Return Hospital Days` == 1, "Better than average",
                                                      ifelse(`National Comparison Return Hospital Days` == 0, "Same as average", "Worse than average")))) %>% 
  mutate(`National Comparison Return Hospital Days` = factor(`National Comparison Return Hospital Days`, levels = c( "Better than average", "Same as average", "Worse than average", "Unknown")))

# Can't get this to work on my computer, so I am commenting it out.
#sliceData %>% 
#  tbl_summary(statistic = list(
#      all_continuous() ~ "{mean} ({sd}), {median}",
#      all_categorical() ~ "{n} / {N} ({p}%)"
#    ), missing_text = "(Missing)") %>% 
#  modify_caption("**Table 2. Possible target measure summaries**") %>%
#  bold_labels()
```

We can see right away that we lose nearly **600 more data points** if we opt to use our calculated `observed_readmission_ratio` (NOTE: nearly 500 if using the 2025 FY data), which is going to be a by-product of the fact that the `Predicted` and `Expected` readmission ratios are using prior information and information about patient mixes to generate these values. Our `observed_readmission_ratio` is crude and unadjusted any way - which could present problems when it comes to true comparability. **So, off the bat it feels as if the constructed target is possibly not a good choice despite its simplicity for stakeholders to understand.** 

### 2.3 Comparing the four possible metrics to`ComparedToNational_Hospital return days for pneumonia patients`

Although `ComparedToNational_Hospital return days for pneumonia patients` is very compelling, it's better as a **predictor** rather than a target because it gets at an aspect of hospital performance: once readmitted due to pneumonia, how long (as compared to the national average) are patients spending in the hospital? This means it makes a really nice predictor to use to _compare our four possible choices for pneumonia-related hospital readmissions_. We are looking for a clear separation in our ideal target between hospitals that have lower readmission rates AND also tend to do better than the national average at healing their readmitted patients!

So, this makes a really nice variable I could choose to **facet** plots, which means creating sub-plots that show up as panels. The thing is, if I know I want to **facet** plots, the first thing I have to do is reshape the data I want from *wide* into *long* format using, for example, the `pivot_longer()` function (the complement of `pivot_wider()` that we used in the Demo).

I don't have to store it as a separate data frame (i.e., I can just pipe it directly into `ggplot()`!) but here I am choosing to so we can more easily make sure it's reshaped correctly along the way. Thus, I am making a long-format version of the possible target variables called `longTargets` so that I can better decide which target to choose.

```{r, echo = FALSE}
## Make the long-format version of just the possible target variables, with some cleanup of the names, etc. for nicer graphs
longTargets <- dat2Analyze %>%
  ## Make a shorter name
  rename(NatlComparisonHospitalDays = `ComparedToNational_Hospital return days for pneumonia patients`,
         `Predicted Readmission Rate` = PredictedReadmissionRate,
         `Excess Readmission Ratio` = ExcessReadmissionRatio,
         `Observed Readmission Rate` = observed_readmission_rate, 
         `Expected Readmission Rate` = ExpectedReadmissionRate) %>% 
  ## Flexible handling depending on which dataset students select; recodes into better, same, and worse
  mutate(NatlComparisonHospitalDays = case_when(
    (NatlComparisonHospitalDays == 1  | NatlComparisonHospitalDays == "Fewer Days Than Average per 100 Discharges") ~ "Better", 
    (NatlComparisonHospitalDays == -1 | NatlComparisonHospitalDays == "More Days Than Average per 100 Discharges") ~ "Worse", 
    (NatlComparisonHospitalDays == 0  | NatlComparisonHospitalDays == "Average Days per 100 Discharges") ~ "Same", 
    .default = "Unknown"),
  NatlComparisonHospitalDays = factor(NatlComparisonHospitalDays, levels = c("Better", "Same", "Worse", "Unknown"))) %>% 
  ## Grab just the five features we want so we can compare them
  select(contains(c("readmission", "NatlComparisonHospitalDays"))) %>% 
  ## Pivot longer
  pivot_longer(-c(5), names_to = "Variable", values_to = "Value")
```

I mentioned `violin` and `density` plots specifically in Question 19 of Demo 1, as well as faceting. I am going to show you two ways to explore these data with these types of plots and you can decide which one(s) you find most informative here. You also may have thought of a better way to display the data too!

#### 2.3.1 Example with `geom_violin()`:
Here I am leaving the `NA` data (which I relabelled as `Unknown`) to get a sense for the degree of missingness.

```{r, fig.cap = "Figure 1. Readmission Rates vs. readmitted hospital LOS", fig.width=8, fig.height=6, echo = FALSE}
longTargets %>% 
  ggplot(aes(x = Value, y = NatlComparisonHospitalDays, fill = NatlComparisonHospitalDays)) +
  geom_violin(alpha = 0.7) +
  labs(title = "Which target to use?",
        subtitle = "By readmitted LOS in hospital compared to national average",
        x = "",
        y = "Rate or Score",
        fill = "Pneumonia-related hospital days (compared to national average):") +
  scale_fill_manual(values = c("gold", "cadetblue", "maroon", "darkgreen"),
                    labels = c("Better", "Same", "Worse", "Unknown")) +
  theme_minimal() +
  theme(legend.position = "bottom",
        strip.text = element_text(size=9),
        legend.text = element_text(size = 7),
        legend.title = element_text(size = 10),
        title = element_text(size = 12, color = "maroon")) + 
  facet_wrap(~Variable, 
             scales = "free",
             ncol = 2)
```

#### **Question 2A**: [0.5 points]
If you have never encountered [violin plots](https://en.wikipedia.org/wiki/Violin_plot) before, they are honestly a wonderful way to assess the **symmetry** and **skew** of a distribution while more easily enabling comparisons between groups. They're effectively a hybrid between a histogram and a boxplot, and we can even opt to overlay the boxplot quartiles in the plots as well, which is particularly handy!

**Your task** is to do just that by adding `geom_boxplot(width = 0.1)` (yes, just that one line!) in the appropriate place. Also change `include = FALSE` to `include = TRUE` in the R chunk header so that it shows up when you knit!

> Make sure to comment your line to call attention to it:

```{r, fig.cap = "Figure 2. Readmission Rates vs. readmitted hospital LOS", fig.width=8, fig.height=6, echo = FALSE, include=FALSE}
editedplot <- longTargets %>% 
  ggplot(aes(x = Value, y = NatlComparisonHospitalDays, fill = NatlComparisonHospitalDays)) +
  geom_violin(alpha = 0.7) +
  geom_boxplot(width = 0.1) + # THIS IS WHERE I AM PLACING THE LINE OF CODE!
  labs(title = "Which target to use?",
       subtitle = "By readmitted LOS in hospital compared to national average",
        x = "",
        y = "Rate or Score",
        fill = "Pneumonia-related hospital days (compared to national average):") +
  scale_fill_manual(values = c("gold", "cadetblue", "maroon", "darkgreen"),
                    labels = c("Better", "Same", "Worse", "Unknown")) +
  theme_minimal() +
  theme(legend.position = "bottom",
        strip.text = element_text(size=9),
        legend.text = element_text(size = 7),
        legend.title = element_text(size = 10),
        title = element_text(size = 12, color = "maroon")) + 
  facet_wrap(~Variable, 
             scales = "free",
             ncol = 2)
```

```{r}
editedplot # Display the plot
```


#### **Question 2B**: [0.5 points]
Now that you can compare these groups more easily, what we're looking for is a balance between **clear difference** between hospital ordinality relative to the national average (i.e., "Are better hospitals really better in pneumonia readmission rates?").

Which target(s) appear the most distinct to you?
> Observed Readmission Rate appears to have the least amount of overlap between the categories.

#### 2.3.2 Example with `geom_density()`:
We could also choose to overlay density plots instead of violin or boxplots, as this can sometimes help us see subtler separation of groups than we can see otherwise. This time, I am also going to ignore the "Unknown" (i.e., missing) class of hospitals to prevent things from getting too messy!

```{r, "Figure 2. Readmission Rates vs. readmitted hospital LOS", warning=FALSE, message=FALSE, echo=FALSE}
longTargets %>% 
  filter(NatlComparisonHospitalDays != "Unknown") %>% 
  ggplot(aes(x = Value, fill = NatlComparisonHospitalDays)) +
  geom_density(alpha = 0.45) +
  labs(title = "Which target to use?",
       subtitle = "By readmitted LOS in hospital compared to national average",
        x = "",
        y = "Rate or Score",
        fill = "Pneumonia-related hospital days (compared to national average):") +
  scale_fill_manual(values = c("gold", "cadetblue", "maroon", "darkgreen"),
                    labels = c("Better", "Same", "Worse", "Unknown")) +
  theme_minimal() +
  theme(legend.position = "bottom",
        strip.text = element_text(size=9),
        legend.text = element_text(size = 7),
        legend.title = element_text(size = 10),
        title = element_text(size = 12, color = "maroon")) + 
  facet_wrap(~Variable, 
             scales = "free",
             ncol = 2)
```

#### **Question 3**: [0.5 points]
Does this view confirm or change your decision from Question 2?

> Once again looking at Observed Readmission Rate,
> there appears to be three clear "peaks" for each category.
> However, there is considerable overlap outside of the peaks,
> which are rather close together.
> Looking at Predicted Readmission Rate,
> the "Worse" category is much more distinct compared to the other options.
> This might be significant for answering our research question.
> Therefore, I am now leaning more towards selecting this as the target. 

### 2.4 What is the relationship of pneumonia-related hospital readmissions to the hospital performance metrics?

Something else we could do to help us make a decision is compare across predictors to look for the most striking relationships among the proposed target (readmission rate), how well compared to national average hospitals are performing AFTER readmitting pneumonia patients (how long after readmission they are in the hospital), and relationships among other hospital-performance predictors, like Medicaid spending or pneumonia-related deaths. 

So, to do this, I'm going to make another long-format dataset and use this to make a bunch of faceted scatterplots where I put one of the readmission rates (let's started with `Predicted Readmissions`) and look for strength in relationships. Note that doing it this way, through plotting, is a **heuristic way** of choosing a target variable and it isn't the only way to do it!

```{r, echo = FALSE}
longScore <- pneumoniaAnalyzeNoEncoding %>%
  rename(NatlComparisonHospitalDays = `ComparedToNational_Hospital return days for pneumonia patients`) %>% 
  select(contains(c("Score", "NatlComparisonHospitalDays", "readmission"))) %>% 
  ## Flexible handling depending on which dataset students select; changes to better, same, worse
  mutate(NatlComparisonHospitalDays = case_when(
    (NatlComparisonHospitalDays == 1  | NatlComparisonHospitalDays == "Fewer Days Than Average per 100 Discharges") ~ "Better", 
    (NatlComparisonHospitalDays == -1 | NatlComparisonHospitalDays == "More Days Than Average per 100 Discharges") ~ "Worse", 
    (NatlComparisonHospitalDays == 0  | NatlComparisonHospitalDays == "Average Days per 100 Discharges") ~ "Same", 
    .default = NA),
  NatlComparisonHospitalDays = factor(NatlComparisonHospitalDays, levels = c("Better", "Same", "Worse", "Unknown")),
  `Score_Emergency department volume` = as.numeric(case_when(`Score_Emergency department volume` == "very high" ~ "5",
                                                         `Score_Emergency department volume` == "high" ~ "4",
                                                         `Score_Emergency department volume` == "medium" ~ "3", 
                                                         `Score_Emergency department volume` == "low"~ "2", 
                                                         `Score_Emergency department volume` == "very low" ~ "1",
                                                        .default = `Score_Emergency department volume`))) %>% 
  pivot_longer(1:12, names_to = "Variable", values_to = "Value") %>% 
  drop_na() %>% 
  mutate(Variable = gsub("Score_", "", Variable)) %>% 
  mutate(Variable = case_when(
    Variable == "Average (median) time patients spent in the emergency department before leaving from the visit A lower number of minutes is better"
                ~"Median time (minutes) patients spent in ED",
    Variable == "CMS Medicare PSI 90: Patient safety and adverse events composite" ~ "Composite patient safety", 
    Variable == "Abdomen CT Use of Contrast Material" ~ "Abdomen CT w/ contrast", 
    Variable == "Perioperative pulmonary embolism or deep vein thrombosis rate" ~ "Perioperative pulmonary embolism/DVT",                     
    Variable == "Left before being seen" ~ "% left ED before being seen", 
    .default = Variable))
```

#### **Question 4**: [1 point]

Change the __y-axis__ out for _each of the four readmission rates. `Predicted Readmission Rates` has been started for you. Re-run the chunk and compare the results. **FOR FULL CREDIT** you must leave the graph on the choice you feel most strongly about AND update the title of the graph with which of the four possible readmission rates you chose. Explain below why you chose this one. 

> I decided to stick with PredictedReadmissionRate.
> Overall, this version demonstrated the clearest separation between the three
> classes.
> This separation makes interpreting the differences between the classes easier.
> For example, we see that the "worse" class has a clear higher rise in
> medicare spending. 
> This would make sense, as a patient who is readmitted more often would be
> spending more due to repeat visits. 

> Observed Readmission Rate and ExcessReadmissionRatio were similar to Predicted,
> however had the three classes ever-so closer together.
> This can be seen in comparisons like death rate, % left ED,
> and medicare spending.
> This means that while many of the same relations are still demonstrated,
> they are seen to a slightly lesser extent, making distinction slightly
> more difficult. 

> Across all comparisons, ExpectedReadmissionRate 
> demonstrated a great amount of overlap for the three categories.

**HINT 1**: What does higher medicare spending per patient tell you about the underlying population of a hospital?

**HINT 2**: Venous thrombolisms and deep vein thromboses (DVTs) are often "sneakier" conditions that have "silent" symptoms until the patient is in crisis. What would having higher scores at providing prophylaxis for these conditions potentially suggest about a hospital's performance?



```{r, fig.width=7, fig.height=8, echo = FALSE, message=FALSE, fig.cap="Figure 4. Readmission rate by hospital performance and readmitted patient LOS"}
longScore %>%  
  ggplot(aes(y = PredictedReadmissionRate,  ## <-- CHANGE THIS!
             x = Value)) +
    geom_point(alpha = 0.25) + 
    geom_smooth(aes(fill = as.factor(NatlComparisonHospitalDays), color = as.factor(NatlComparisonHospitalDays)), 
                method = "lm", 
                se = T, 
                show.legend = T) + 
    ## CHANGE THE TITLE FOR FULL CREDIT!
    labs(title = "Relationship of predicted pneumonia-related hospital \nreadmissions to each performance metric",
        x = "Rate or Score",
        y = "Predicted Pneumonia-related Readmissions",
        color = "Pneumonia-related Readmission Compared to National Average") +
  scale_fill_manual(values = c("gold", "cadetblue", "maroon")) + 
  scale_color_manual(values = c("gold", "cadetblue", "maroon"),
                    labels = c("Better", "Same", "Worse")) + 
  guides(fill = "none", color = guide_legend(override.aes = list(fill=NA))) +    ## To override the filled SE boxes from geom_smooth() in the legend
  theme_minimal() +
  theme(legend.position = "bottom",
        strip.text = element_text(size=8),
        legend.text = element_text(size = 8),
        legend.title = element_text(size = 10),
        title = element_text(size = 12, color = "maroon")) + 
  facet_wrap(~Variable, 
             scales = "free_x",
             ncol = 2)
```

**HINT 3**: Here's how I would interpret the __relationship between PREDICTED pneumonia readmissions and pneumonia death rates, looking at whether hospitals performed better, same, or worse in terms of length-of-stay after readmission__. What I notice immediately is a moderate slope for hospitals with worse readmitted lengths of stay (LOS) [maroon line] compared to same [blue line] and especially better [yellow line]. This relationship is negative, implying that as the pneumonia-related death rate increases, readmissions __goes down__. Oh! So what this implies is that hospitals with higher deaths from pneumonia tend to have lower readmissions - and this is especially true for the ones who are have longer (worse than average) readmitted patient stays. In other words - these hospitals are __ probably releasing their pneumonia patients too soon from their first hospitalization__, leading to higher readmissions, longer readmitted stays, and even higher pneumonia-related deaths! 

### 2.5 What is the relationship of pneumonia-related hospital readmissions to the patient satisfaction metrics?

Now let's do a similar analysis but for the `HcahpsLinear...` variables. The __HCAHPS (Hospital Consumer Assessment of Healthcare Providers and Systems)__ is a standardized survey that measures adult inpatient perceptions of hospital care. It's a national, publicly reported survey mandated by the Centers for Medicare & Medicaid Services (CMS). Because it focuses on aspects of hospital care that are important to patients, it is often used to improve patient satisfaction and hospital quality. 

```{r, echo = FALSE}
longHCAHPS <- pneumoniaAnalyzeNoEncoding %>%
  rename(NatlComparisonHospitalDays = `ComparedToNational_Hospital return days for pneumonia patients`) %>% 
  select(contains(c("HcahpsLinear", "NatlComparisonHospitalDays", "readmission"))) %>% 
  ## Flexible handling depending on which dataset students select; changes to better, same, worse
  mutate(NatlComparisonHospitalDays = case_when(
    (NatlComparisonHospitalDays == 1  | NatlComparisonHospitalDays == "Fewer Days Than Average per 100 Discharges") ~ "Better", 
    (NatlComparisonHospitalDays == -1 | NatlComparisonHospitalDays == "More Days Than Average per 100 Discharges") ~ "Worse", 
    (NatlComparisonHospitalDays == 0  | NatlComparisonHospitalDays == "Average Days per 100 Discharges") ~ "Same", 
    .default = NA),
  NatlComparisonHospitalDays = factor(NatlComparisonHospitalDays, levels = c("Better", "Same", "Worse", "Unknown"))) %>% 
  pivot_longer(1:10, names_to = "Variable", values_to = "Value") %>% 
  drop_na() %>% 
  mutate(Variable = gsub("HcahpsLinearMeanValue_", "", Variable))
```


#### **Question 5**: [1 point]

Again, you task is to change the __y-axis__ out for _each of the four readmission rates. `Predicted Readmission Rates` has been started for you. Re-run the chunk and compare the results. **FOR FULL CREDIT** you must leave the graph on the choice you feel most strongly about AND update the title of the graph with which of the four possible readmission rates you chose. Explain below why you chose this one. 

> Overall, I feel the same about PredictedReadmissionRate as I did in
> question 4. 
> The distinction is most visible when using this variable.
> One noticeable take-away is that increased communication, information,
> and responsiveness all trend towards lower readmission rates.
> This would suggest that open and consistent communication, as well as 
> providing patients with information about their care, leads to less
> readmissions. 

> Once again, Observed Readmission Rate and ExcessReadmissionRatio were similar,
> yet slightly less defined than what was seen with PredictedReadmissionRate. 

> As before, there is a great amount of overlap between the classes when using
> ExpectedReadmissionRate. 

**HINT 1**: What does higher nurse and doctor communication mean for improved patient outcomes, including readmission?

**HINT 2**: Why might cleanliness not have much effect here even though pneumonia is a communicable disease?

```{r, fig.width=7, fig.height=8, echo = FALSE, message=FALSE, fig.cap="Figure 5. Readmission rate by patient satisfaction and readmitted patient LOS"}
longHCAHPS %>%  
  ggplot(aes(y = PredictedReadmissionRate,  ## <-- CHANGE THIS!
             x = Value)) +
    geom_point(alpha = 0.25) + 
    geom_smooth(aes(fill = as.factor(NatlComparisonHospitalDays), color = as.factor(NatlComparisonHospitalDays)), 
                method = "lm", 
                se = T, 
                show.legend = T) + 
    ## CHANGE THE TITLE FOR FULL CREDIT!
    labs(title = "Relationship of predicted pneumonia-related hospital \nreadmissions to each patient satisfaction metric",
        x = "Rate or Score",
        y = "Predicted Pneumonia-related Readmissions",
        color = "Pneumonia-related Readmission Compared to National Average") +
  scale_fill_manual(values = c("gold", "cadetblue", "maroon")) + 
  scale_color_manual(values = c("gold", "cadetblue", "maroon"),
                    labels = c("Better", "Same", "Worse")) + 
  guides(fill = "none", color = guide_legend(override.aes = list(fill=NA))) +    ## To override the filled SE boxes from geom_smooth() in the legend
  theme_minimal() +
  theme(legend.position = "bottom",
        strip.text = element_text(size=8),
        legend.text = element_text(size = 8),
        legend.title = element_text(size = 10),
        title = element_text(size = 12, color = "maroon")) + 
  facet_wrap(~Variable, 
             scales = "free_x",
             ncol = 2)
```

**HINT 3**: Here's how I would interpret the __relationship between PREDICTED pneumonia readmissions and overall hospital rating, looking at whether hospitals performed better, same, or worse in terms of length-of-stay after readmission__. What I notice immediately is a pretty substantial negative slope for hospitals with worse readmitted lengths of stay (LOS) [maroon line], moderate negative slope for hospitals with the same readmitted LOS [blue line], and slightly decreasing slope for hospitals with better readmitted LOS better [yellow line]. So what this implies is that hospitals with high ratings tend to have lower pneumonia-related readmissions, even among the hospitals with worse readmitted length-of-stays. This relationships is strongest when hospitals have longer LOS; maybe having a long readmission LOS stay **actually implies a form of quality**, like "Well, we got it wrong the first time, let's make darn sure we get it right the second?" But hospitals that were already getting it pretty right don't have as steep of slope because they were already doing well at readmissions to begin with, hence the almost flat line.


#### **Question 6**: [2 points]
What relationships do you find most compelling from these scatterplots and why? Also, name one outstanding question you have at this point, whether it is about the data, the data collection, or something more speculative.

> With the first group of scatterplots, the relation with medicare spending was
> the most compelling and intuitive. 
> Each class was (mostly) clearly separated, and the positive slope makes sense.
> One would naturally presume that higher readmissions would lead to patients
> having to spend more with medicare, as they are visiting the hospital more.

> With the second group of plots, the relations with doctor/nurse communication
> and discharge information were compelling. 
> With doctor/nurse communication, we see that increased attention and willingness
> to educate patients trend towards lower readmissions.
> Likewise, with discharge information, we see that when patients are provided
> with post-hospital care instructions, they are less likely to be readmitted. 
> These relations could be easily explained to stakeholders and provides clear
> suggestions for hospital improvements. 

> One question I have is why the better category sometimes shows a rise
> in the variables about communication. 
> This goes against the other two trends, and paints a different picture than
> the one we might expect. 

Although there is certainly more EDA we could do, we are going to wrap that up here for the sake of moving on - but, as always, I encourage you to think about or even explore what else you would like to know about your data that we did not do!

### 2.5 Final remarks on target selection.

Going forward, I have selected `PredictedReadmissionRate` as our target. In a more thorough analysis, we would likely want to consider:

1. Circling back to our stakeholder to try to refine our question or target using domain knowledge

2. Perform experiments with our models to find the best predictive model since that is our ultimate goal. 

However, given that we do not have the option or time for that, let me explain my choice. It's the **best balance** of distinguishing hospitals with better and worse readmitted hospital stays, while still providing an __adjusted, hospital-specific__ measure. Adjusted values will generally always be a better choice than the crude, even if our crude measure captures something interesting about the data, as it did here, or if it's easier for our stakholder to comprehend. Our crude measure, `observed_readmission_rate` did the __best job__ of recapitulating the readmitted hospital LOS (`ComparedToNational_Hospital return days for pneumonia patients`), but if that's really our focus, wouldn't it then be better to switch from a readmission rate to that as our target? Probably. 

If you still feel unsure about what is the best choice, you're not alone - so do I! This is where, because we are working with an __imagined stakeholder__, we must make the best decision we can given the question we defined in the Demo. I would defend my choice of `Predicted Readmission Rate` for three key reasons:

* It has been adjusted to be hospital-specific, using a hospital-specific intercept and historic data about the hospitals' performances

* We don't lose nearly 600 data points to missingness.

* Our imagined stakeholder, and the question we set, was about __readmission rate__ so even though __readmitted return hospital days__ might also be a similarly viable candidate, it doesn't reflect our imagined stakeholder's request. Note that the `ExpectedReadmissionRate` is standardized for similar hospitals, and may not truly reflect a given hospital; the `ExcessReadmissionRatio`, while informative, was the least likely to reflect the distinctiveness in hospital return days, which is still of interest to us. 

But remember that what matters most is that we are __matching our question__. 

#### **Question 7**: [1 point]

Do you agree with my choice of target? Why or why not? (You are free to disagree!)

> Overall, I agree with the choice of target.
> At first, I had certain concerns that made me favor the observed rates over
> the predicted.
> Primarily, the predicted rates invovled calculated measures of which I don't
> have the original data. 
> Using data generated like this can make explaining what our model is predicting
> potentially more difficult.

> However, if we put this aside, or are willing to do more research into this,
> the scatterplots demonstrate that the separation in readmission rates is most
> visible when looking at the predicted readmission rates. 

> Beyond predicted vs observed, neither of the other two options really made
> sense for what our research question was. 

### 2.6 Bias and Limitation

#### **Question 8**: [2 points]
All studies have bias (introduced or systemic error) and limitations (failure to fully explain something). We could go more deeply into these concepts, but for now I want you to take a stab and just brainstorming either ways we might have bias OR the limitations of using this target variable. You do not need to answer both, but you're welcome to do so. You can read more about bias [here](https://www.ncbi.nlm.nih.gov/books/NBK574513/) or see some deeper explanations of limitations [here](https://www.aje.com/arc/how-to-write-limitations-of-the-study/). You do not need to write a lot; just 2-3 sentences. I am simply asking you to pause and reflect on our choices here before we proceed. 

> Using PredictedReadmissionRate as our target comes with a limitation on
> interpretability.
> As I previously mentioned, our target variable is a balanced, calculated 
> prediction. 
> Specifically, these values were created using statistial modeling,
> which we do not have immediate access to. 
> By using this as a target, we are unable to fully explain what our model is
> truly predicting, as we lack the complete understanding of how these
> values were obtained. 


# 3 Pre-processing and Feature Selection

The time has finally come to officially move __back__ into pre-processing and feature selection so that we can finally build some models. I want you to take note of the order in which I'm doing things here; the flow can change from project to project, but generally it's a good idea to get your dataset to a state of __manually curated features__ before making your data partition.

## 3.1 Further feature selection and refinement.

### 3.1.1 Dropping the targets we aren't planning to use. [Manual Method]

It's time to say goodbye to the features we know would be far too redundant or non-informative given our target of `PredictedReadmissionRate`. We don't need any of the `Sample_` columns, for example; these are merely sample sizes. Useful if we were doing standardization, but not necessary for us right now. We also are dropping the other possible target features that are too redundant with `PredictedReadmissionRate`.

#### **Question 9A**: [0.5 points]
Add your comments to the code as indicated to explain what I was doing at each of the steps.

> Your answer in the comments.

```{r}
dat2Analyze <- dat2Analyze %>% 
  # Remove the following columns from the data
  select(-ExpectedReadmissionRate, 
         -ExcessReadmissionRatio,
         -observed_readmission_rate, 
         -contains(c("Sample_", "NumberOfPatients")),
         -NumberSurveysCompleted) %>% 
  # Rename columns to have shorter names
  rename(`Median time (minutes) patients spent in ED` = 
         `Score_Average (median) time patients spent in the emergency department before leaving from the visit A lower number of minutes is better`,
         `Composite patient safety` = `Score_CMS Medicare PSI 90: Patient safety and adverse events composite`,
         `ComparedToNational_Composite patient safety` = `ComparedToNational_CMS Medicare PSI 90: Patient safety and adverse events composite`) %>% 
  # Move target variable to front of dataset
  select(PredictedReadmissionRate, everything())

# Rename columns to remove unneeded parts that lengthen names
colnames(dat2Analyze) <- gsub('Score_|HcahpsLinearMeanValue_|_Payment', '', colnames(dat2Analyze))
```


### 3.1.2 Dropping any features with near-zero variance. [Automated Method]

Next, we are going to take advantage of the `nearZeroVar()` function that is part of the `caret` package. We will drop any of the columns with near-zero variance. This is a way of automating (and thus standardizing) the process for choosing to drop non-informative columns due to zero or nearly zero variance.

#### **Question 9B**: [0.5 points]
If you are not familiar yet with the concept of __zero or near-zero variance__, do a little digging. Based on what you read, try to explain for your naive advocacy group stakeholders, why it's important to remove the features with little variance.

> When a feature has variance, it means that values differ between cases.
> Having zero variance would mean that every value for that feature is the same.
> Likewise, near-zero variance would be a feature with very little difference between cases.
> If we want to build a predictive model to inform about readmission,
> then we want to train the model on features that help distinguish between levels of readmission.
> Features with zero or near-zero variance will contribute little to this,
> and may actually make things more confusing for our model.
> Therefore, we want to remove these features.  

```{r}
#library(caret)
#library(dplyr)
#library(kableExtra)
```


```{r, echo = F}
## Identify the columns with near-zero variance
zero_var_df <- dat2Analyze[, nearZeroVar(dat2Analyze)]

## Print the columns
data.frame(names(zero_var_df)) %>% 
  rename(`Variables Dropped` = names.zero_var_df.) %>% 
  kable(
    format = "html",
    caption = "Table 3. Columns dropped due to near-zero variance.") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
)

## Drop them from the dataset:
dat2Analyze <- dat2Analyze[, -nearZeroVar(dat2Analyze)]
```

**Take note** of the number of features here.

> 2 columns were dropped:

> 1. ComparedToNational_Postoperative respiratory failure rate

> 2. ComparedToNational_Perioperative pulmonary embolism or deep vein thrombosis rate

#### **Question 9C**: [0.5 points]
You may or may not have noticed that I moved `FacilityID` to our rownames in the datasets you loaded today. Why did I choose to do that? Why do I want to make sure unique identifiers, like `FacilityID`, are out of the dataframe before I either (a) run `nearZeroVar()` or (b) proceed with the rest of the analysis?

> Unique identifiers give a unique value to every row in the data.
> This means that the feature would be calculated as having the highest possible variance.
> However, these features do not contribute to the predictive abilities of our models,
> as they do not give meaningful information to be generalized to our target variable.
> In addition, they can cause data leakage if the model learns to associate target outcomes
> with particular IDs. 
> Therefore, they need to be removed before proceeding. 

#### **Question 9D**: [1 point]
Do some research on, or lean into your previous knowledge, about the **curse of dimensionality** or the **big-P, little-N (n >> P) problem**. Using whatever you find or remember, can you justify why we likely do NOT have a **big-P, little-N (n >> P) problem** here? Make sure to explain. 

```{r}
dim(dat2Analyze)
```


> The "curse of dimensionality" refers to problems that arise when your data has too many features.
> Increasing dimensionality can cause data to become more sparce,
> making it harder for models to find patterns within the data.
> In turn, models are more likely to become overfit, resulting is worse generalization.
> This can also be described by the "big-P, little-N" problem.
> Here, P refers to the number of features and N refers to the number of observations.
> When P is big, it takes many more observations to make a reliable model.
> If N is small when P is big, you will run into the problems described above.
> While we have a great number of features (31 features),
> we have a much greater number of observations (4816 rows).
> Therefore, it is unlikely that we will face this problem. 

## 3.2 Assess missingness & devise an imputation plan

We are going to deal with imputation in a rather quick and elegant way that requires minimal effort from us using a model-based approach with `missForest`. You may recall from lecture that I said we are choosing a model-based approached _without uncertainty_ because our ultimate goal is to create a deployable model for our stakeholder.

#### **Question 10**: [1 point]
Write some code that will quickly tally the number of columns in the dataset that contain missing values. You will find `colSums()` and `is.na()` functions likely useful here. **What percent of the features have at least some level of missingness?**

```{r}
sum(colSums(is.na(dat2Analyze)) > 0)
```

> All 31 columns are missing some data.

**Which column(s) are not missing any data?**

```{r}
sum(colSums(is.na(dat2Analyze)) == 0) # Just to confirm
```

> There are no columns that are not missing any data. 

**How many complete observations do we have in the dataset?** You may find `nrow()` with `complete.cases()` or `drop_na()` useful!
```{r}
complete_ob <- nrow(dat2Analyze[complete.cases(dat2Analyze), ])
print(complete_ob)
```

> We have 644 complete observations in the dataset.

### 3.2.1 Use the `md.pairs()` function from the `mice` package to assess the extent of missing values before imputing. 

When we are missing a lot of data, as we are here, it can sometimes be useful to predetermine whether the _patterns of missing correlate with our target_. Why? We care if missing values correlate with the target because this may indicate that the missingness itself carries information. Thus, ignoring it could lead to biased models or low predictive power!

In fact, missingness can itself __become a predictor__. If a missing value is associated with the outcome, it may carry predictive signal. For example, in our data, hospitals with high rates of missingness in `Left before being seen` highly associated with pneumonia readmission might indicate a worrying pattern. Perhaps hospitals are negligently failing to report how many of their patients leave without being seen because of larger, systemic issues that ALSO happen to impact overall quality and patient care. Thus, missingness could itself become a predictor! 

To do, I will leverage the `md.pairs()` function from the `mice` package which calculates the pairwise correlations of missingness between variables and allows us to visualize those patterns using a clustered heatmap. When I do this, I will drop `State` from the matrix because it will create errors if I retain it. (**Why?**)

```{r}
## Drop State and analyze the pairwise missingness using the mice package
miceMatrix <- dat2Analyze %>% select(-State) %>% md.pairs()
```

What `md.mice()` does is create two pairwise correlation matrices, `mr` and `mm`, where:

* `mr` = __Missing vs. Responded__
  - Pairwise matrix showing how often the row has a missing value in one variable and an observed (i.e., non-missing or "responded") value in the other
* `mm` = __Missing vs. Missing__
  - Pairwise matrix showing how often the row has a missing value in one variable and missing value in the other variable

Let's take a look at the first 6 rows and 4 columns of the `mr` matrix:

```{r, echo=FALSE}
miceMatrix$mr[1:6, 1:4] %>% 
  kable(format = "html",
    caption = "Table 4. Missing vs. Reponded Matrix Slice") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F))
```

Any given cell in a matrix is always of the form $[i, j]$, where $i$ is the row and $j$ is the column - just as you are used to with dataframes! So, in these correlation matrices of our pairwise variables, cell $[1,1]$ is `PredictedReadmissionRate` vs. itself, so the count is 0 - as is every value on the diagonal, like with all correlation matrices. For cell $[1,2]$ `PredictedReadmissionRate` vs. , there are 42 observations where variable `PredictedReadmissionRate` ($i$) is missing but `MRSA Bacteremia` ($j$) was observed. Thus, we can **extract the number of cases where variable $i$ is missing and variable $j$ is observed using** `miceMatrixmr[i, j]`.

Now, let's take a look at the first 6 rows and 4 columns of the `mm` matrix now:

```{r, echo=FALSE}
miceMatrix$mm[1:6, 1:4] %>% 
  kable(format = "html",
    caption = "Table 5. Missing vs. Missing Matrix Slice") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F))
```

Similarly, `miceMatrix$mm[i, j]` gives us the number of cases where both variables $i$ and $j$ were co-missing.

#### **Question 11A**: [0.5 points]
Explain what you think the number in cell $[6,1]$ means. Out of all the observations in the dataset, does this feel like a high degree of co-missingness?

> The number in cell $[6,1]$ means that there are 1788 observations where
> values for both Postoperative respiratory failure rate and PredictedReadmissionRate are missing.
> Considering that there are 4816 records in total, that means about 37% of the data
> for this relation is missing. 
> This seems to be a high level of co-missingness, but is less than several other relations
> demonstrated within the calculated table. 


Lastly, we can calculate a **proportion of missingness vs. response** between any two variables $i$ and $j$ such that:

$Proportion_{i,j} = \frac{mr_{i,j}}{mr_{i,j} + mm_{i,j}}$

In non-mathy speak, it's the fraction of the times that variable $j$ is observed among rows where $i$ is missing. So, if you want to impute variable $i$ - let's say it's `MRSA Bacteremia` - then you will be interested in the variables typically **observed** (present in the dataset) when $i$ is missing. Those are the variables that will be able to help us predict, or fill in, $i$. 

 - If this ratio is close to 1, $j$ is nearly always available when $i$ is missing, so $j$ might be useful for imputing $i$.
 - If this ratio is close to 0, $j$ is usually missing when $i$ is missing, so it’s not helpful for imputing $i$. You can also use this to tell you about **patterns of missingness with regard to your target variable when you target is** $i$. 

```{r, fig.cap="Figure 5. Correlation between Missing and Response", echo = FALSE, fig.width = 9, fig.height = 9}
## Calculate proportion from pairwise pattern of missingness
p <- as.matrix(miceMatrix$mr/(miceMatrix$mr+miceMatrix$mm))

## Set a color palette
prettyPurples <- colorRampPalette(brewer.pal(8, "PuRd"))(8)

## Make a heatmap
heatmap(p, 
        col = prettyPurples, 
        cexRow = 0.5, 
        cexCol = 0.5, 
        margins = c(13, 13), 
        Colv = NA)
```

It may feel a little counterintuitive, but here __darker colors__ indicate a higher correlation between missingness and response (proportions nearer to 1). In other words, these are the variables that are going to be more informative during imputation for a given variable. 

But what if we wanted to know about patterns of missingness with regard to our target, `PredictedReadmissionRate`? Well, we could get that too!

```{r, fig.cap="Figure 6. Correlation between Missing and Missing for Target", echo = FALSE, fig.width = 7, fig.height = 7}
## Calculate proportion from pairwise pattern of missingness
p <- as.matrix(miceMatrix$mm/(miceMatrix$mr+miceMatrix$mm))

## Set a color palette
prettyPurples <- colorRampPalette(brewer.pal(10, "PuRd"))(30)

## Make a heatmap
p[1, ] %>% 
  data.frame(Proportion = .) %>% 
  mutate(Variable = rownames(p),
         Variable = fct_reorder(Variable, Proportion)) %>% 
  filter(Variable != "PredictedReadmissionRate") %>% 
  ggplot(aes(x = Variable , y = Proportion)) +
  geom_bar(aes(fill = Variable), stat = "identity", color = "navy") +
  theme_minimal() +
  labs(title = "Highest Correlations of Missingness \nwith Predicted Readmissions",
       x = "Variables",
       y = "Proportion of Co-missingness") +
  scale_fill_manual(values = (prettyPurples)) +
  coord_flip() +   
  theme_classic() +
  theme(legend.position = "none")
```


#### **Question 11B**: [0.5 points]

What columns seem to be most correlated in terms of missingness to our target, `PredictedReadmissionRate`? Does this give you any cause for concern? Feel free to speculate as to why you might see this.

> Some of the highest correlated missingness include MRSA Bacteremia, 
> Postoperative respiratory failure rate, and Medicare spending per patient.
> MRSA Bacteremia tracks the rate in which patients develop specific
> bloodstream infections. 
> Postoperative respiratory failure rate tracks the rate in which
> patients experience breathing problems after surgery.
> The missingness in this data is concerning as both would be key signs for
> hospital performance and the need to readmit. 
> Medicare spending might also indicate how often a patient is in the hospital,
> as higher spendings could indicate more time in the hospital.
> This information could be missing for a number of possible reasons.
> Poor data tracking might be one possibility.
> Another possibility, moreso for the first two identified,
> is that both of these values are very small in the hospital overall,
> and therefore were not properly measured.

## 3.3 Drop rows that are missing from the target variable. 

We will not be able to use those rows at all moving forward, sadly. That is going to reduce our sample size pretty sizably, from $N = 4,816$ to only $N = 2,726$ (or from $N = 4,775$ to only $N = 2,731$ .

```{r, echo = FALSE}
dat2Analyze <- dat2Analyze %>% 
  filter(!is.na(PredictedReadmissionRate)) 

paste0(c("Rows: ", "Columns: "), dim(dat2Analyze))
```


#### **Question 12**: [0.5 points]
Thinking back to your answer to Question 12, do you feel any differently given these dimensions of the dataset?

> While the number of rows has nearly been cut in half,
> our observations still greatly outnumber our features.
> Therefore, I still do not believe there will be a problem with dimensionality.

## 3.4 Do the splits! 

At this stage, we are almost ready to __impute__ - but not until we have done our data partitioning first! In fact, splitting our data into training and testing partitions first is really important to prevent __data leakage__.

#### **Question 13**: [1 point]
Explain to your stakeholder, in simpler language, (1) why we partition data into testing and training sets and (2) what data leakage is. You do not need to go into great detail, but it should explain enough that they understand the basic ideas.

> The purpose of having a predictive model is so that it will be able to accurately classify
> new, previously unseen cases.
> If we train the model on the entire data, we will not be sure if it can generalize
> its predictions to unseen cases.
> Therefore, we partition the data into train and test sets so that we can test
> the model on data it was not trained on to validate its accuracy. 

> Data leakage is when information from our training data contaminates the test data.
> This can happen when feature engineering is performed before splitting the data.
> If new features are calculated from existing features, then information can carry
> over into the test set if calculated before splitting. 
> Data leakage harms model integrity, as it will have an unfair advantage at
> predicting the test cases. 

You may recall that I mentioned that the optimal ratio is has been argued to be $\sqrt{p} : 1$, where $p$ is the number of parameters (which may or may not equal your predictors depending on your planned analysis). Note that you do not have to split only a single time; you could make different splits for different analyses, if needed. However, we are going to use a single split here for convenience. 

Now, wouldn't it be nice to have a handy function that could calculate the optimal split ratio for us? So, let's write one! This is something you can use with ANY analysis going forward!

#### **Question 14A**: [1.5 points]
There are three parts to this question to get it to all come together correctly. 

1. Comment the function to explain what it does (I've defined the arguments for you)

2. Run the function (fill in the blank)

3. Fill in the missing piece in the data partitioning chunk below

```{r}
calcSplitRatio <- function(p = NA, df) {
  ## @p  = the number of parameters. by default, if none are provided, the number of columns (predictors) in the dataset are used
  ## @df = the dataframe that will be used for the analysis
  
  # If p is not defined, set p to the number of predictors
  if(is.na(p)) {
    p <- ncol(df) -1   # Remove the target variable from the pool of predictors
  }
  
  # Calculate the optimal split ratio (test set)
  test_N <- (1/sqrt(p))*nrow(dat2Analyze)
  # 
  test_prop <- round((1/sqrt(p))*nrow(dat2Analyze)/nrow(dat2Analyze), 2)
  # Subtract optimal test proportion to find optimal train 
  train_prop <- 1-test_prop
  
  # Print the optimal split ratio
  print(paste0("The ideal split ratio is ", train_prop, ":", test_prop, " training:testing"))
  
  # Return the optimal train proportion
  return(train_prop)
}

## Fill in the blanks to run:
train_prop <- calcSplitRatio(df = dat2Analyze)
```

**Hint**: One important note here is that if the number of parameters are not provided (which is true for the starter code I gave you to run it - it does NOT pass any parameters to that argument!), then by default our function is designed to take the number of columns of the dataframe and subtract one for the target variable. Neat, huh?

Now, uncomment this code and fill it out to run it. 
```{r}
ind <- createDataPartition(dat2Analyze$PredictedReadmissionRate,   ## Put the target here!
                            p = train_prop,           ## Use the object you just made!
                            list = FALSE)
 
train <- dat2Analyze[ind, ]
test <- dat2Analyze[-c(ind), ]
```

#### **Question 14B**: [0.5 points]
What split ratio did you get? Is it close to the canonical 80-20 split? Why do you think that happened?

> Our exact split is 82-18.
> The split is relatively close to the canonical 80-20 split.
> Considering the fact that we are working with a large set of data,
> we do not need to dedicate a large percentage of the total data to the test set
> in order to have a sufficiently large enough pool. 


__Did you get stuck somewhere?__ If so, load the `test` and `train` data in case you struggled with Question 18 so you can proceed:

```{r}
load(file = "FY2024_data_files/pneumoniaTrain.Rdata")
load(file = "FY2024_data_files/pneumoniaTest.Rdata")
```


## 3.5 Impute missing variables using `missForest`.

We are going to take advantage of the `missForest()` function from the `missForest` package. In a nutshell, what `missForest` does is it will fit either a regression- or classification-based random forest model and use the OOB ("out-of-bag") results to predict and fill in `NA`s. The advantage is that this process is non-linear, done in just a few lines of code, and can handle mixed data-types (although you need to make sure that all your numerical types are of the same numerical type, and that all characters or factors are of the same type).

#### 3.5.1 Running `missForest`

The way I have chosen to do this is to separate the data so that I can turn the encoded categories back into categories temporarily, then stitch them back together into a temporary dataframe so that I can run `missForest`. Notice that I temporarily drop the `State` and target features; I drop `State` to spare ourselves a little computational time and I drop `PredictedReadmissionRate` to prevent __data leakage__. **Be patient!!**. Since this is randomForest, it could take a minute or two to run. **NOTE:** _If your machine fails to run this, there are data you can load. Some computers may struggle to run this._

```{r, include = FALSE}
# Remove encoded categorical variables for imputation, plus the target and state (which have no missing)
data_sans_encoded = train %>% 
  select(-PredictedReadmissionRate,
         -State,
         -`Emergency department volume`, 
         -contains("ComparedToNational"))

data_cat = train %>% 
  select(`Emergency department volume`, 
         contains("ComparedToNational")) %>% 
  mutate_all(as.factor)

# Stitch them back together:
temp <- cbind(data_sans_encoded, data_cat)

# Impute missing values using missForest
imputedTrain <- missForest(temp, 
                          variablewise = TRUE,
                          verbose = FALSE)
```

**NOTE**: Did you catch that I not only _removed_ the target before imputation, but I dropped `State` too? This is because we know `State` is/will be frequency encoded (depending on our path), and it could create imputation issues as a result. This leaves me with **29 columns to perform imputation on**. 

#### 3.5.2 Load this if it will not run for you
```{r}
load(file = "FY2024_data_files/imputedTrain.Rdata")
```

#### 3.5.3 Quick summary of the results

**NOTE:** If yours didn't run, make sure to look at the table output in the knitted HTML I provided you. Also, change the `include=FALSE` in the chunk header if you were not able to get `missForest` to run.

```{r, echo = FALSE, include = TRUE}
# Show a quick summary of the results
data.frame(imputedTrain$OOBerror, names(temp)) %>% 
  rename(Variable = `names.temp.`,
         `OOB Error` = imputedTrain.OOBerror) %>% 
## Print the columns
  kable(digits = 2,
    format = "html",
    caption = "Table 6. missForest OOB Error Rates for the imputed variables, training dataset") %>%
    pack_rows("MSE", 1, 22) %>%
    pack_rows("PCF", 23, 28) %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
)
```

#### **Question 15**: [1 point]
What does `MSE` (Mean Squared Error) and `PCF` (Proportion of Falsely Classified) indicate here? You may find looking at the `missForest` [Vignette](https://stat.ethz.ch/education/semesters/ss2012/ams/paper/missForest_1.2.pdf) helpful. Which variable(s) had the highest OOB error?

> Mean Squared Error (MSE) and Proportion of Falsely Classified (PCF) are measures of 
> quality for the imputed values.

> MSE is used to measure the continuous variables.
> A lower MSE suggests a lower amount of error, meaning the values are closer to
> actual values. A higher MSE suggests a greater degree of inaccuracy in the
> imputed variables.

> PCF is used to measure the categorical variables.
> PCF is measured on a scale of 0 to 1, representing how many cases were
> incorrectly imputed. 
> A PCF of 0 would suggest that there is no error in the imputation.
> A greater PCF suggests more imputed values are incorrect.

> The variable with the highest OOB error for MSE was Payment for pneumonia patients,
> with a score of 948452.73.

> The variable with the highest OOB error for PCF was Emergency department volume,
> with a score of 0.46.

**NOTE 1**: If the OOB error (either MSE or PCF) from `missForest` is really high, it’s a red flag that the imputation for those variables is untrustworthy. This could be do to too much missingness, weak relationships among the variables making it hard to predict, or low quality data (e.g., noisy, sparse). In such cases, we want to **DROP** any variables with extremely high OOB error because they add noise. A general rule of thumb is that if the PCF > ~0.3 (30% of the imputed values are wrong) or if there is especially high MSE (inflation) relative to the variance of the variable, we should drop it. Let's set those aside so we can drop them from `test` and `train` both downstream! 

**NOTE 2**: Even though the PCF is a bit too high, I am choosing to retain `ComparedToNational_Hospital return days for pneumonia patients` because it is a primary predictor. This could be a poor choice, however! Every decision we make as data scientists can _come with pitfalls_.

```{r}
## These look okay. Uncomment to see their variances.
#var(train$`Hospital return days for pneumonia patients`, na.rm = T)
#var(train$`Median time (minutes) patients spent in ED`, na.rm = T)
#var(train$`Healthcare workers given influenza vaccination`, na.rm = T)

cols2drop <- c("Payment for pneumonia patients", "Emergency department volume")
```

#### **Question 16**: [1 point]
Alternatively, we could use MICE (Multivariate Imputation with Chained Equations), which is a method that will model uncertainty along with the variables. Do a little research. How does MICE work? Include a citation to a source.

> MICE is a method that iteratively models missing values with a regression model,
> using the other variables for the regression.

> MICE starts by filling in the missing values with guesses.
> The guess could be anything like the mean value or the most common value. 
> Then, one variable at a time, MICE removes the temporary guess values and then
> builds a regression model for that variable using the other completed variables.
> MICE proceeds through this process for each variable in the dataset.

> MICE accounts for uncertainty by generating multiple versions of the dataset.
> Each version uses slightly different guess values in the beginning, so that
> the outcome of the regressions will change to account for the uncertainty.

> Source: https://pmc.ncbi.nlm.nih.gov/articles/PMC3074241/

#### 3.5.4 Putting it alllllll back together...

We are almost done with imputation. The last thing we need to do is extract the imputed values from `imputed_data$ximp`, and add the `State` and target variable back. Then we get to wash, rinse, and repeat for the test set! Oh my!

##### 3.5.4.1 Put the original `State` and `target` variables back

```{r}
imputedTrain <- imputedTrain$ximp %>% 
  ## Put the original State and target variables back on
  mutate(State = train$State,
         PredictedReadmissionRate = train$PredictedReadmissionRate) 
```

##### 3.5.4.2 Now do the imputation for `test`

```{r, include = FALSE}
# Remove encoded categorical variables for imputation, plus the target and state (which have no missing)
data_sans_encoded = test %>% 
  select(-PredictedReadmissionRate,
         -State,
         -contains("ComparedToNational"))

data_cat = test %>% 
  select(contains("ComparedToNational")) %>% 
  mutate_all(as.factor)

# Stitch them back together:
temp <- cbind(data_sans_encoded, data_cat)

# Impute missing values using missForest
imputedTest <- missForest(temp, 
                          variablewise = TRUE,
                          verbose = FALSE)
```

##### 3.5.4.3 Load this if it  will not run for you

```{r}
load(file = "FY2024_data_files/imputedTest.Rdata")
```

##### 3.5.4.4 Put the original `State` and `target` variables back on `test`

```{r}
imputedTest <- imputedTest$ximp %>% 
  ## Put the original State and target variables back on
  mutate(State = test$State,
         PredictedReadmissionRate = test$PredictedReadmissionRate) 
```

##### 3.5.4.5 Drop the features we determined we should not retain due to unstable imputation from BOTH `train` and `test`

```{r}
imputedTrain <- imputedTrain[, !(names(imputedTrain) %in% cols2drop), drop = FALSE]
imputedTest  <- imputedTest[, !(names(imputedTest) %in% cols2drop), drop = FALSE]
```


#### **Question 17**: [1 point]
Quickly explore how well the imputation did by choosing at least one numeric variable and one of the categorical variables. Did the distributions or frequencies change drastically? You may find the `summary()` and `table()` functions to be fastest here. 

```{r}
# Check "MRSA Bacteremia" distributions

summary(imputedTrain[["MRSA Bacteremia"]])
summary(imputedTest[["MRSA Bacteremia"]])

print("-------------")

summary(train[["MRSA Bacteremia"]])
summary(test[["MRSA Bacteremia"]])
```

```{r}
# Check "ComparedToNational_Death rate for pneumonia patients" distributions

table(imputedTrain[["ComparedToNational_Death rate for pneumonia patients"]])
table(imputedTest[["ComparedToNational_Death rate for pneumonia patients"]])

print("-------------")

table(train[["ComparedToNational_Death rate for pneumonia patients"]])
table(test[["ComparedToNational_Death rate for pneumonia patients"]])
```


> First, we will look at "MRSA Bacteremia".

> Looking at the Train data, the first quartile and median are off by about 1000
> from the original train set to the imputed set. 
> The difference in the mean and third quartile are much less, but still 
> several hundred off.
> The test sets are similarly off when comparing the first quartile,
> but otherwise are relatively the same (especially when compare to the train sets).

> Next, we will look at "ComparedToNational_Death rate for pneumonia patients".

> Overall, the distributions are similar when comparing the train/test sets
> before imputation and after imputation.
> The only differences are within the 0 class, where the difference is always
> less than 10. 

## 3.6 Another Fork in the Road
It's time, yet again, to make a choice. If you opted to load the __fully encoded__ dataset and everything has been done on that, you will run the first chunk. But if you decided that you wanted to __wait to encode__ the TIME HAS FINALLY COME!

### 3.6.1 The Left Fork - No Encoding, Just Converting Data Back After `missForest`

Ultimately, we need to turn the factor features back into numbers for our next analyses but we had to turn them into factors for `missForest`. So, now we need to turn them back. Notice a bit of annoying trickery here. Because we had converted them to a factor type, in order to back-convert them correctly to our original ordinal encoding, we had to first convert to character and then to numeric. So, our conversion goes __factor__ $\rightarrow$ __character__ $\rightarrow$ __numeric__ in order to get exactly what we need! Again, we must remember to perform on BOTH `imputedTrain` and `imputedTest`. 

```{r}
## Uncomment if this is your choice!
# readyTrain <- imputedTrain %>%
#   mutate_if(is.factor, as.character) %>% 
#   mutate_if(is.character, as.numeric)
# 
# readyTest <- imputedTest %>%
#   mutate_if(is.factor, as.character) %>% 
#   mutate_if(is.character, as.numeric)
```

### 3.6.2 The Right Fork - Frequency Encoding

The time has FINALLY come to do frequency encoding, if that is the fork in the road you took all the way back in [Section 1.3](#section1.3). Once again, we will be using a helper function that we load with the `source()` function like we did [here](#helperCode). It will do the frequency encoding for us on the `State` variable, if it needs to be done. If you started from the fully encoded dataset, there is a conditional statement that will skip this for you. 

```{r, results='hide'}
## This calls the helper function in the associated .R file
source(file = "doFrequencyEncoding.R")

## If State is not already a number
if(!is.numeric(imputedTrain$State)) {
    ## This sets a list of COLUMN NAMES I want to frequency encode
    cols2encode <- c("State")
  
    ## This runs the function, which takes up to 4 arguments
    ## Open the source code to see what each argument does!
    ## It returns a LIST of frequency encoded dataframes.
    freqEncoded <- doFrequencyEncoding(train = imputedTrain,
                                       test = imputedTest,
                                       cols2encode = cols2encode, 
                                       quiet = FALSE)
    
    ## Lastly, extract the newly encoded, imputed, dataframes!
    readyTrain <- freqEncoded$train
    readyTest <- freqEncoded$test
}
```

#### **Question 18**: [1 point]
Because we are now frequency encoding AFTER data partitioning (we did ordinal encoding BEFORE the split for convenience), we absolutely __must__ apply the same encoding map to the `test` set that we did to the `train` set. Explain why, and what should we do if some classes of the variable end up in the `train` that is not in the `test` or vice versa? (**HINT**: Look at the source code!)

> When doing frequency encoding, our encodings are calculated from the frequencies
> of the provided original values.
> This means that if we do not apply the same mapping from the train set
> to the test set, each set will likely end up with different frequency encodings.

> If some classes end up in the train set that are not in the test set, there
> is no significant problem. The model simply will not encounter those values
> when looking at the data from the test set.

> If some classes end up in the test set that are not in the train set,
> then the model will struggle to handle the value unless there is a default
> value to fall back on.

> Looking at the source code for the encoding function,
> it can be seen that the function tries to test each value to the map.
> If the value is not found in the map, the value is marked as NA.
> Then, the code converts all NA values into 0s. 

## 3.7  Transformation & Scaling

Ultimately, we know we will need to scale & center our data, as that is required for the analyses I have set out in our Analysis Plan. But before we get into that, it would be nice to know what __realm of analyses__ in which we fall and, more importantly, do we need to consider *transformation* of our target in addition to the scaling & centering we plan to do on the entire dataset. 

**In other words: is our target variable approximately normally distributed?**

```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.cap="Figure 7. Histogram of Raw Predicted Readmission Rate"}
ggplot(readyTrain, aes(x = PredictedReadmissionRate)) +
  geom_histogram(alpha = 0.6, 
                 fill = "orchid",
                 color = "orchid") +
  theme_minimal() +
  labs(title = "Predicted Readmission Ratio, N = 2,726 hospitals",
       y = "Frequency",
       x = "Predicted Readmission Ratio")
```

#### **Question 19A**: [1 point]
Why is a histogram, or even a Q-Q plot, an insufficient way to assess whether a distribution is approximately normal?

> Both methods suffer the problem of subjectivity.
> It is up to the viewer to decide if the plot(s) show a shape that is close
> enough to normal or not.
> For example, the plot above ALMOST looks normal, but has a slight skew to the right.

Now Apply a [Shapiro-Wilk test](https://biostats.w.uib.no/test-for-normality-shapiro-wilks-test/) for normality using the `shapiro.test()` function. What does it indicate about your distribution? (**Note**: Shapiro tests are only reliable for $N < 5000$, but it is fine to perform here.)

```{r}
# Normality Test

shapiro.test(readyTrain$PredictedReadmissionRate)
```

> The W value is 0.98544, which is very close to 1.
> W values close to 1 suggest a fit closer to normal.
> However, the p-value is the more significant factor to read.

> The p-value is 3.085e-14, which is far less than 0.05.
> A value less than 0.05 tells us to reject the null hypothesis.
> This this case, that means that our data is not normally distributed.

### 3.7.1 Box-Cox Transformation

The Box-Cox transformation is a _family_ of power transformations used to stabilize variance and make data more normally distributed (Box & Cox, 1964). Recall from our brief discussion in lecture that the Box-Cox transformation involves the parameter $\lambda$, which when applied to $y$ yields the transformation. It is defined as 

$y^\lambda = \frac{y^\lambda - 1}{\lambda}$ for $\lambda \ne 0$. 

When $\lambda = 0$, it is the same as the $\ln(y)$. Our lecture slides have more reference values for how to interpret $\lambda$. But if $\lambda \ne 0$, then this means that the transformation is only applicable to positive values. The end result is that it helps improve the performance of statistical models that assume normality.

#### 3.7.1.1 There are multiple ways to apply the Box-Cox transform in R, but we are going to take advantage of the one in `caret` to make our lives easier, `BoxCoxTrans()`.

```{r}
bc_data <- BoxCoxTrans(readyTrain$PredictedReadmissionRate)
print(paste0("Box-Cox lambda =  ", bc_data$lambda))
```

We can see that the estimated $\lambda = -0.3$, so __approximately a square-root transform__ (see lecture slides for reference). I found that $\lambda = -0.5$ in FY 2025, for those using that year's data.

#### 3.7.1.2 Now, apply the Box-Cox transformation to the `train` data:

```{r, echo = TRUE}
readyTrain$bc_PredictedReadmissionRate <- predict(bc_data, readyTrain$PredictedReadmissionRate)
```

#### 3.7.1.3  Assess how it did:

```{r, echo = FALSE, message=FALSE}
ggplot(readyTrain, aes(x = bc_PredictedReadmissionRate)) +
  geom_histogram(alpha = 0.6, 
                 fill = "hotpink",
                 color = "hotpink") +
  theme_minimal() +
  labs(title = "Box-Cox Transformed Predicted Readmission Ratio, N = 2,726 hospitals",
       y = "Frequency",
       x = "Predicted Readmission Ratio")


qqnorm(readyTrain$bc_PredictedReadmissionRate)
qqline(readyTrain$bc_PredictedReadmissionRate, 
       col = "hotpink", lty = 2, lwd = 2)
```

##### Figures 8a-b. Predicted Readmission Rate After Box-Cox Transformation

Can you see what it did to the distribution? __look closely__, if not. It's subtle but discernible!!

#### **Question 19B**: [1.5 points]
Now repeat all of the steps to perform a Box-Cox transform on your own to the `imputedTest` data. Why must we (1) also transform the test data and (2) __use the same $\lambda$ as the training data__?

> Since we will be training the model on a certain scale 
> (being the newly transformed distribution), the model will expect data on
> the same scale when given new data to predict.
> Therefore, we need to apply the same transformation to the test data.

> Using the same lambda as the train data is important, as it ensures that
> the test data will be on the same scale as the train data.
> In addition, calculating a new lambda will give the model information about
> the distribution of the test set, causing a source of data leakage. 


```{r}
# Test transformation

readyTest$bc_PredictedReadmissionRate <- predict(bc_data, readyTest$PredictedReadmissionRate)


# Check that transform looks successful

ggplot(readyTest, aes(x = bc_PredictedReadmissionRate)) +
  geom_histogram(alpha = 0.6, 
                 fill = "hotpink",
                 color = "hotpink") +
  theme_minimal() +
  labs(title = "Box-Cox Transformed Predicted Readmission Ratio, N = 2,726 hospitals",
       y = "Frequency",
       x = "Predicted Readmission Ratio")


qqnorm(readyTest$bc_PredictedReadmissionRate)
qqline(readyTest$bc_PredictedReadmissionRate, 
       col = "hotpink", lty = 2, lwd = 2)
```

**NOTE** At this point, you have both an non-transformed (`PredictedReadmissionRate`) and Box-Cox transformed (`bc_PredictedReadmissionRate`) version of the target in your data.

### 3.7.2 Centering & Scaling

Now that we have applied a Box-Cox transform to the target variable, we can center and scale the remainder of the variables. We had to perform the Box-Cox _prior_ to centering because the data has to be positive for a Box-Cox transformation (unless we switch to a [Yeo-Johnson transform (Yeo & Johnson, 2000)](https://academic.oup.com/biomet/article-abstract/87/4/954/232908)). Centering will likely make a lot of the data negative, plus we know we have negatives in our dataset purposefully.

It's time to drop the _original_ target, `PredictedReadmissionRate`, and center and scale everything INCLUDING the target! Here it is for the training set:

```{r}
readyTrain <- readyTrain %>% 
  ## Drop the original target variable
  select(-PredictedReadmissionRate) %>% 
  ## Make anything that might still be a factor from encoding is back to number
  mutate_if(is.factor, as.numeric) %>%
  ## Center and scale everything!
  scale(center = TRUE, scale = TRUE) %>% 
  data.frame()
```

#### **Question 19C**: [0.5 points]
Do the same for your test data. Make sure to use `readyTest` and overwrite it. 

```{r}
# Test set centering

readyTest <- readyTest %>%
  select(-PredictedReadmissionRate) %>%
  mutate_if(is.factor, as.numeric) %>%
  scale(center = TRUE, scale = TRUE) %>%
  data.frame()
```

**Hint**: Make sure to turn your centered, scaled __matrix__ back into a dataframe! By default, it creates a matrix.

### 3.7.3 To save time, you are welcome to start from here after you've worked through the above steps.
```{r}
load(file = "FY2024_data_files/readyTrain.Rdata")

load(file = "FY2024_data_files/readyTest.Rdata")
```

# 4 Assessing Multicollinearity & Feature Redundancy {#multicollinearity}

As we finally, finally wrap-up pre-processing there is one final assessment we need to perform: **multicollinearity**. As I am certain you recall, multicollinearity is when we have high correlation between predictors (so much so, in fact, that it comes at the cost of any correlation with the target!). Recall from lecture that multicollinearity is when one or more of our predictors are moderately to highly correlated with each other. This can __inflate our coefficients__ and thereby cause spurious associations through __false positives__ (i.e., small p-values when we don't actually have them!).

Here we will implement two common ways we assess multicollinearity: pairwise correlations and Variance Inflation Factors (VIFs). 

## 4.1 Pairwise Correlation 

Pairwise correlation is usually done as a Pearson or Spearman correlation. We will calculate a correlation matrix and look at the correlation of the variables within the training dataset to assess possible areas of multicollinearity. We can then visualize this matrix with a heatmap, similar to how we did with missingness, except now blue indicates a __positive__ correlation, red indicates a __negative correlation__, and yellow indicates __no (or zero)__ correlation. Darker colors indicate stronger correlations. 

```{r, echo = FALSE, fig.width = 9, fig.height = 9, fig.cap="Figure 9. Correlation Heatmap of All Variables"}
## Calculate a correlation matrix
corrMat <- cor(readyTrain)

## Make a heatmap
heatmap(corrMat, 
        col = rev(colorRampPalette(brewer.pal(8, "RdYlBu"))(8)), 
        cexRow = 0.6, 
        cexCol = 0.6, 
        margins = c(10, 10))
```

#### **Question 20A**: [1 point]
Where do you see the highest potential for multicollinearity? Why do you say that?

> The relationships demonstrated in the top left corner show some potential for
> multicollinearity. As described above, blue represents positive correlations.
> The darker the square, the more correlated the variables.
> This corner has a great amount of moderately dark squares.
> We see this as well in the bottom right corner. 

> There are a few even darker blue square scattered around the heatmap.
> Many of which somewhat are branching off of the red diagonal.

> The variables that fall upon the red diagonal demonstrate high correlation, however
> that diagonal represents the correlation of variables against themselves.
> Therefore, it is not meaningful for this analysis.

Which variable(s) seem to be most correlated with our target, `bc_PredictedReadmissionRate`?

> The variable the seems to be most correlated with our target is 
> ComparedToNational_Hospital.return.days.

> Other variables that also seem to be decently correlated include (but are not limited to):

> SurveyResponseRate

> Cleanliness

> Quietness

> Recommend.hospital

> Staff.responsiveness

## 4.2 Fitting a model to estimate VIF

Another way we can assess multicollinearity is by fitting a multiple linear regression model and estimating the __variance inflation factors__, or VIF, which estimates how much the variance of an estimated regression coefficient increases if your predictors are correlated. Specifically, variance inflation can identify __multicollinearity__ when the VIF is greater than ~4 and especially when it is above 10.

## 4.2.1 Fit a linear regression with the training data, using the Box-Cox transformed `Predicted Readmission Rate` as the target. 
```{r}
mod <- lm(bc_PredictedReadmissionRate ~ ., data = readyTrain)
```

## 4.2.1 Next, let's calculate the VIFs using the `vif()` function that is part of the `car` package. 

Note that I am not printing all of the VIFs, just the top 15 after sorting in descending order.

```{r} 
## Calculate the VIFs and put into a dataframe to print the table
vifResults <- vif(mod) %>% data.frame
## Change the column names of the dataframe
colnames(vifResults) <- c("VIF")
```

```{r, echo = FALSE, warning=FALSE,message=FALSE}
## Sort by VIF, print the top 15 worst offenders
vifResults %>% 
  arrange(desc(VIF)) %>% 
  top_n(15) %>% 
  ## Pass through kable() to make it pretty
  kable(digits = 2,
    format = "html",
    caption = "Table 7. Top 15 Variance Inflation Factors after multiple linear regression") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
)
```

#### **Question 20B**: [1 point]
Does multicollinearity seem to be an issue in this dataset and, if so, among which variables? Why do you come to this conclusion?

**HINT 1**: What was used to calculate `Overall Hospital Rating`? Does this help us better understand why the VIF is so large?

**HINT 2**: Is, for example,`Payment Category for pneumonia patients` created from `Payment for pneumonia patients`? [These are Medicare payments]? Which one do you think we should keep amd why?

> Multicollinearity appears to be an issue, as many variables are 
> calculated from other variables.
> For example, "Overall Hospital Rating" is calculated from five other measures,
> including readmissions!

> Another example, "Payment Category for pneumonia patients" is created from
> "Payment for pneumonia patients". 
> "Payment for pneumonia patients" records the actual amount paid,
> while "Payment Category for pneumonia patients" bins these values into
> similar groups to create a categorical measure. 
> Keeping the continuous original data would be more valuable,
> as it allows for a greater degree of flexibility and subtleness when 
> making a prediction.

Which variable(s) seem to be most highly correlated with our target? Is it a positive or negative correlation? Does it make sense?

> Overall.hospital.rating, Recommend.hospital, and Nurse.communication are among
> the most highly correlated variables with our target. 
> These variables are positively correlated with the target based on the heatmap.
> This is not intuitive, as you would expect higher hospital ratings and greater
> communication to result in less readmissions. 


### 4.3 Dropping the most collinear variables

Even though we plan to proceed with an elastic net (which is much more robust to multicollinearity), some of these VIFs are too high for comfort. We also want to pause and ask - **were any of the variables used to make other variables?** (The answer is "yes"!). Those are excellent candidates to scrub too; we want to use our domain knowledge and what our stakeholders are asking for to help us decide which ones to keep.

Because the goal is a deployable predictive model, we likely want something more granular than most of the `ComparedToNational` features can give us, with the __exception of our key predictor__, `ComparedToNational_Hospital return days for pneumonia patients`. I will remind you of what we explored in Section 2 with this variable; not only that, we can see how **how highly correlated it is with our target!**.

### 4.3.1 Drop variables with VIF over 5 from both the training and testing data

```{r}
## Drop those with VIF over 5
vifOver5 <- vifResults %>% filter(VIF >=5) %>% rownames()
print(vifOver5)

## Now drop them
readyTrain <- readyTrain %>% select(-any_of(vifOver5))
readyTest <- readyTest %>% select(-any_of(vifOver5))
```

### 4.3.2 Now drop all of the `ComparedToNational` and `PaymentCategory` features **except** `ComparedToNational_Hospital return days for pneumonia patients`
```{r}
readyTrain <- readyTrain %>% 
  ## Rename the column we want to keep
  rename(Hospital.return.days.for.pneumonia.patients = ComparedToNational_Hospital.return.days.for.pneumonia.patients) %>% 
  ## Drop all the other columns 
  select(-contains("Compared"), -contains("PaymentCategory"), -SurveyResponseRate)

readyTest <- readyTest %>% 
  ## Rename the column we want to keep
  rename(Hospital.return.days.for.pneumonia.patients = ComparedToNational_Hospital.return.days.for.pneumonia.patients) %>% 
  ## Drop all the other columns 
  select(-contains("Compared"))
```


# 5 Unsupervised Learning Methods: Segmentation Analysis

Predictive modeling is our ultimate goal, but let's pretend our stakeholders have expressed a desire to better understand how the hospitals relate to one another and whether there are any general patterns we should be interested in for enhanced interpretation. After all, a prediction is one thing, but understanding the _why_ is still usually important!

Thus, we have opted to undertake a __Segmentation Analysis__ to see if there is any underlying segments, or clusters, of hospitals based on the data we have. We will use the unsupervised methods of Principal Component Analysis (PCA) and $k$-means clustering do this; it will help us to determine if there are broader classifications of our hospitals that we need to report back to the advocacy group. 

## 5.1 Principal Component Analysis

First, we apply a PCA on the data and display the principal components. Note that we have the same number of **principal components** as we do variables in the data set! The _cumulative proportion_ of variance adds to 100% by our last principal component; but we're really more interested in the portion of variance explained by EACH of the principal components. This can help us figure out just how many PCs are important for explaining the most variance. For example, if we were interested in feature reduction, we could use the top PCs that explain, say, some portion of the variance that we wish to retain.

#### **Question 21**: [2 points]

1. At what principal component do we surpass 80% of explained variance? (**HINT**: look at the cumulative variance row rather than tha proportion of variance row).

> We surpass 80% of explained variance at PC11.
> At this point, the cumulative proportion is 0.83983.

```{r}
pca <- prcomp(readyTrain)
summary(pca)
```

2. You may also notice that this is always ordered from most proportion of variance on the left (thus is PC1 defined) to our final principal component on the right. What percent of variance is explained by just PC1? What percent by the final PC? Is the attributable portion of the variance in PC1 high, medium, or low based on some quick research or prior knowledge?

> The percent of variance explained by PC1 is 22.41%.
> The percent of variance explained by PC18 is 1.056%.
> PC1 alone containing aprox. 22% of the variance is high. (REVIEW THIS?)



3. We use scree plots to graphically determine how many principal components to retain and how many principal components best explain the variance of the data. The $x$-axis displays the principal components (PC1, PC2, etc.), while the $y$-axis shows the eigenvalues (or proportion of variance explained) for each component. The **elbow method** we discussed in class is the point where the slope starts to flatten or the point at which the percentage of variance explained seems to level off. If we were doing this for feature reduction, we might think of this as approximately how many principal components would be needed to explain the total variation in the training data.  However, it is a **heuristic** method, so it can sometimes be a little uncertain from the plot alone what the optimal number of PCs is. Let's take a look at the top-15 PCs on a scree plot now:

```{r, echo= FALSE, fig.cap="Figure 10. Scree Plot of First 15 Principal Components"}
## Show the scree plot
fviz_screeplot(pca, 
         addlabels = TRUE, 
         ylim = c(0, 30),
         ncp = 15,
         barfill = prettyPurples[13],
         barcolor = prettyPurples[15],
         main = "Scree Plot: First 15 Principal Components")
```

How many PCs appear optimal to you based on the scree plot?

> To me, it appears as if the elbow is at PC6. 

4. People often mis-write scree (I've even accidentally done it) as SCREE, as if it's an acronym like AUROC or ROC or even PCA. But it's not. Why is it called a scree plot? (**Hint**: It may not be what you expect!)

> In nature, a scree slope refers to a loose pile of rocks at the base of a mountain.
> A scree plot resembles this (from a certain angle) and therefore gets its name
> from this natural formation.

### 5.1.2 Variable loadings and contributions

Next, let's explore how much the variables are contributing (this is based on the weight of their rotations). 
In PCA, we are interested in the __loadings__, i.e., the linear combination weights (coefficients) whereby unit-scaled components define or "load" a variable. Loadings help us interpret principal components.

You may have noticed we used the `prcomp()` function above; `prcomp()` will return the loadings in the variable `$rotation`, which contains a matrix of variable  loadings. These loadings have been determined from the eigenvectors, which without getting into what that is (because we'd have to take a departure into linear algebra), suffice it to say that they are how we are measuring the direction and magnitude during each subsequent rotation of the principal components as we measure the variance explained.

Additionally, we are going to color this by our grouping variable and key predictor in the dataset, `Hospital.return.days.for.pneumonia.patients`. Recall that when we explored this earlier this variable is the performance of each hospital in terms of its pneumonia-related return-to-hospital days relative to the national average. In other words, hospitals that were "Worse than average" had readmitted pneumonia patients with longer-than-average stays. 

```{r, echo = FALSE, fig.cap="Figure 11. PCA Biplot Showing the Top Feature that Contributes to Explained Variance"}
## Create a factor variable that contains the information about the compared to national average - return to hospital days variable
returnHospitalComparison <- factor(readyTrain$Hospital.return.days.for.pneumonia.patients,
                                   labels = c("Worse", "Same", "Better"))

## Now show the biplot
fviz_pca_biplot(pca, 
             palette = c("maroon", "cadetblue", "gold"),
             label = "var",
             geom = "point",
             geom.var = "text",
             addEllipses = TRUE,
             ellipse.alpha = 0.2,              ## Change alpha 0-1
             col = "black",
             col.var = "black",
             habillage = returnHospitalComparison,
             select.var = list(contrib = 8),   ## Try changing the number
             repel = F)                        ## Try switching to FALSE
```

This biplot (Figure above) shows that, on principal components 1 and 2 (the PCs that account for the __most__ variance in the training data) there is subtle but noticeable grouping of hospitals in terms of how long their readmitted pneumonia patients stayed in the hospital. I have chosen to display the __#1__ feature that contributes to that variance, `Communication.about.medicines`. 

Each quadrant on a PCA biplot corresponds to a different directional influence of the original variables. E.g., because it is in the top-right quadrant I where both PC1 and PC2 are positive, this suggests that communication about medicines are positively associated with variables that point into the same quadrant. Since it seems to correspond with where the "better" hospitals are, this suggests that these hospitals are made somewhat distinctly "better" at readmitted hospital stay lengths because of correlations with communication about medicines. (**NOTE**: If you look at the 2025 data, the bipolot is effectively inverted but the conclusions seem to be the same in 2025 as well).

**A general rule of thumb is:**

1. Top-right Quadrant I
  - PC1 and PC2 are both positive
  - Thus points in this quadrant are positively associated with variables pointing into this quadrant
  - Indicates high values for variables with arrows in this quadrant

2. Top-left Quadrant II
  - PC1 is negative, PC2 is positive
  - Thus points here are associated with high values of variables pointing to the left and top
  - These observations contrast with those in the right-side quadrants on PC1
  
3. Bottom-left Quadrant III
  - PC1 and PC2 are both negative
  - Thus points here are negatively associated with variables pointing into the other quadrants
  - These points could represent a distinct subgroup or pattern with behavior opposite to other points.

4. Bottom-right Quadrant IV
  - PC1 is positive, PC2 is negative
  - Thus points here are associated with variables pointing _into_ this quadrant
  - May indicate a contrast with Quadrant II on PC2

#### **Question 22**: [1 point]

Play around with the graphical parameters in the biplot, especially those I've marked with comments. Try adding, for example, more variables to display rather than just the top contributor. **NOTE** that if you find variables in the same quadrant together, this means that they are correlated. What variable(s) can you find that **contrast** with `Communication.about.medicines`?

> Some variables that contrast with Communication.about.medicines include:

> Perioperative.pulmonary.embolism.or.deep.vein.thrombosis.rate

> Postoperative.respiratory.failure.rate

> Composite.patient.safety

Now, look at the contributions to PCs 1 and 2 (Figure Below) from the variables in the training dataset. Anything above the dotted red-line contributes **significantly** to the overall explanation of variance, and anything below the dotted line does not. Does anything about this plot stand out to you?

**HINT 1**: What do the top contributing variables all have in common? In other words, did they all come from the same data source?

**HINT 2**: Where does the target, `PredictedReasmissionRate`, fall among the significant variables?

> A majority of the top contributing variables come from the HCAHPS data.

> The target falls at the very end of the significant variables.

```{r, echo=FALSE, fig.width=8, fig.height=8, fig.cap="Figure 12. Variables that Significantly Explain Variance."}
fviz_contrib(pca, 
             choice = "var", 
             axes = 1:2,
             fill = prettyPurples[13],
             color = prettyPurples[15],
             xtickslab.rt = 70)
```

## 5.2 *k*-means Clustering

$k$-means is a centroid-based clustering algorithm, where a "centroid" is the geometric center of an object often measured by Euclidean distance. In the algorithm, the distance between each data point and a centroid is calculated by randomly grabbing data; these distances are then used to assign the data point to a cluster. The goal is to identify the optimal $k$ number of groups in a dataset by minimizing the distance of each datapoint to a respective centroid.

### 5.2.1 Visualizing Clusters

Although multiple methods exist to determine the optimal number of clusters, one commonly employed is __heuristic__, i.e., it isn't really computational. You will either choose the optimal clusters based on a set of options of $k$ graphically or you can once again employ the "elbow" method again, as you did with PCA. Let's start by visually inspecting the effect of different $k$ clusters on PC1 and PC2 of the dataset. I have chosen to test $k$ groups of 2 through 9. 

```{r, echo = F}
skittles <- c("#de378d","#32a840", "#463ab5","#0e87e3", "gold", "#ae5bc9", "#e35c0e", "cadetblue", "#0ee383")
for(k in 2:9) {
  ## Initialize a plot
  plotName <- paste0("p", k)
  ## Calculate the kmeans for each given k
  kmeansResult <- kmeans(readyTrain, 	
                     centers = k,   ## number of clusters
                     nstart = 25,   ## num of times to repeat the process 
                                    ## with random initialization; increase 
                                    ## if you're failing to converge
                     iter.max = 1000,    ## num of iterations to run k-means
                     algorithm = "MacQueen")    ## since the default algorithm 
                                                ## can struggle with close 
                                                ## points, adjusting the method
  ## Make a set of graphs to compare!
  kmeansGraph <- fviz_cluster(kmeansResult, 
                              geom = "point", 
                              data = readyTrain,
                              show.clust.cent = TRUE,
                              ggtheme = theme_minimal(),
                              ellipse.type = "norm",
                              palette = skittles,
                              pointsize = 0.5) + 
                  ggtitle(paste0("k = ", k))
  assign(plotName, kmeansGraph)
}
grid.arrange(p2, p3, p4, p5, nrow = 2)
grid.arrange(p6, p7, p8, p9, nrow = 2)
```

##### Figure 13. Biplots showing possible k clusters

#### **Question 23A**: [0.5 points]
Which number of clusters, $k$, do you think has the best explanatory power? Why? Is it is hard to tell?

> Telling which value of k has the best explanatory power is difficult
> because a heuristic method like this really requires an understanding of the data.
> Just looking at the produced cluster plots alone, many of the clusters feel like
> valid potential options.
> The difficultly in deciding also comes from the fact that our data is mostly
> clumped together, and does not form visible distinct clusters.

> To rule some out, the higher values of k (those >4) seem to be a stretch.
> Many of the "clusters" overlap one another and seem to exist due to the
> algorithm having to identify a certain number of clusters.

> While k = 4 suffers less from this issue, it still looks less natural than
> k = 2 and k = 3.
> k = 2 demonstrates a clear split in half of the clump of data,
> which possibly makes the most sense. 
> From the plot alone, it is hard to justify any other kind of split,
> as there are no naturally occuring groups to see. 

### 5.2.3 Within-sum-of-squares (WSS) elbow method

Next, let's employ the 'elbow' method again, this time to look at when the __within sum of squares__ drops off rather than the percent variance, as was done in PCA.

```{r, echo = FALSE, fig.cap="Figure 14. WSS Scree plot for k-means"}
# Determine number of clusters
fviz_nbclust(readyTrain,
             FUNcluster = kmeans, 
             method = "wss",  
             linecolor = prettyPurples[13]) +
  labs(title = "WSS Elbow Method for Optimal k")
```

#### **Question 23B**: [1 point]

What is the __within sum of squares__ and what exactly is it measuring here? By the elbow method, which $k$ is the optimal number of clusters? Did it agree with your choice from the other heuristic method?

> Within sum of squares is a measure that looks at how closely packed together
> data points are within their clusters.
> A lower WSS tells us that points are more close together within the cluster.

> The most visible elbow appears to be a k = 2.
> This agrees with my answer from the other method. 

### 5.2.3 Silhouette method

Lastly, we will apply the __silhouette method__, which measures the average silhouette width, a combination of (1) how similar any given point is to its own cluster (also called __cohesion__) and (2) how different that same point is from other clusters (also called __separation__). Silhouette scores can range from $-1 \rightarrow +1$, with larger values indicating better clustering (i.e., more cohesive clusters that are separated from each other). Thus, a silhouette score of 1 indicates __perfect separation and cohesion__ whereas a silhouette of 0 indicates overlapping clusters. 

What we can do is iteratively test our possible $k$ values of 2 to 9 and a silhouette score will be calculated for each $k$ tested. The highest silhouette score indicates the optimal $k$.

```{r, echo=FALSE, fig.cap="Figure 14. Silhouette Plot for k-means"}
fviz_nbclust(readyTrain, kmeans, method = "silhouette",
             k.max = 9, 
             nstart = 25,
             iter.max = 1000,
             linecolor = prettyPurples[13]) +
  labs(title = "Silhouette Method for Optimal k")
```

#### **Question 23C**: [0.5 points]

Are you surprised at the optimal $k$ from the silhouette method? Does it seem to agree with what you've seen with the WSS elbow method? (**Hint**: agreement generally suggests evidence that you have found the optimal $k$).

> The silhouette method agrees with my answers from the other two methods.

Lastly, let's extract the optimal $k$ from the silhouette method.

```{r}
## Extract optimal k from silhouette method; first get the cluster data
clusterData <- fviz_nbclust(readyTrain, kmeans, method = "silhouette")$data
## Then extract the maximum y
k <- as.numeric(clusterData$clusters[which.max(clusterData$y)])
```


## 5.3 Segmentation Analysis

As we said before, the goal of segmentation analysis is to broadly cluster our hospitals by their features to help us characterize them. 

#### **Question 24**: [1 point]
Let's explore the clusters - segments - of the hospitals based on their average values from the __original__ dataset (we start from `imputedTrain`, which hadn't yet been transformed, centered, or scaled, BUT the multicollinear variables also haven't been dropped either). Your task is to add comments to this code chunk. 

```{r, echo = TRUE, fig.height = 8, fig.width=6, fig.cap="Figure 16. Boxplot Comparisons of Important Variables by Cluster"}
# Run k-means on the train data
kmeansResult <- kmeans(readyTrain, 	
                         centers = k,      # set k to our optimal number (2)
                         nstart = 50,      
                         iter.max = 1000,  
                         algorithm = "MacQueen")
# Clean data, add clusters
kmeansTrain <- imputedTrain %>% 
  ## Rename the column we want to keep
  rename(Pneumonia.Hospital.Return.Days = `ComparedToNational_Hospital return days for pneumonia patients`) %>% 
  # Remove columns with VIF over 5, that contains "Compared", that contains "PaymentCategory", and SurveyResponseRate
  select(-any_of(vifOver5), -contains("Compared"), -contains("PaymentCategory"), -SurveyResponseRate) %>% 
  # Add in clusters and recode Pneumonia.Hospital.Return.Days as numerical rankings
  mutate(Cluster = kmeansResult$cluster,
         Pneumonia.Hospital.Return.Days = as.numeric(if_else(Pneumonia.Hospital.Return.Days == "1", 1,
                                                     if_else(Pneumonia.Hospital.Return.Days == "0", 0, -1)))) %>% 
  # Standardize column names to snake case
  clean_names()

## Using % contribution graph, the order of the top 10:
pcaImportantVars <- factor(c("Communication.about.medicines",
                       "Doctor.communication", 
                       "Composite.patient.safety",
                       "Discharge.information",
                       "Quietness",
                       "Postoperative.respiratory.failure.rate",
                       "Cleanliness",
                       "Periop.Rate.Of.Embolism.Or.Thrombosis",
                       "Predicted.Readmission.Rate",
                       "Pneumonia.Hospital.Return.Days"))

# Covert variable names to lower case
pcaImportantVars <- tolower(pcaImportantVars)
# Replace periods with underscores
pcaImportantVars <- gsub("\\.", "_", pcaImportantVars) %>% 
# Convert variable names to title case
  to_any_case("title")

# Create box-plots for variable distributions
kmeansTrain %>% 
  # Manually rename variables for consistency
  rename(Predicted.Readmission.Rate = predicted_readmission_rate,
         Periop.Rate.Of.Embolism.Or.Thrombosis = perioperative_pulmonary_embolism_or_deep_vein_thrombosis_rate) %>% 
  # Clean and convert to title case
  clean_names(case = "title") %>% 
  # Select the cluster values and our identified important variables
  select(Cluster, any_of(pcaImportantVars)) %>% 
  # Pivot data to long format
  pivot_longer(-1, names_to = "Measure", values_to = "Value") %>% 
  # Set variable ordering
  # filter(Measure %in% pcaImportantVars) %>% 
   mutate(Measure = factor(Measure, levels = to_any_case(c("Communication.about.medicines",
                        "Doctor.communication", 
                        "Composite.patient.safety",
                        "Discharge.information",
                        "Quietness",
                        "Postoperative.respiratory.failure.rate",
                        "Cleanliness",
                        "Periop.Rate.Of.Embolism.Or.Thrombosis",
                        "Predicted.Readmission.Rate",
                        "Pneumonia.Hospital.Return.Days"), case="title"))) %>% 
  # Create box-plots by cluster
    ggplot(aes(x = Cluster, y = Value, fill = as.factor(Cluster))) +
    geom_boxplot(alpha=0.75) +
    scale_fill_manual(values = c(skittles[1], skittles[6])) +
    labs(title = "Cluster Means",
         x = "",
         fill = "Cluster") +
    theme_classic() +
    theme(legend.position = "bottom",
        strip.text = element_text(size=8),
        legend.text = element_text(size = 8),
        legend.title = element_text(size = 10),
        title = element_text(size = 12, color = "maroon")) + 
    # Facet the plots 
    facet_wrap(~ Measure, 
             scales = "free_y",
             ncol = 2)

# Create a summary table for our important variables
kmeansTrain %>% 
  # Manually rename variables for consistency
  rename(Predicted.Readmission.Rate = predicted_readmission_rate,
         Periop.Rate.Of.Embolism.Or.Thrombosis = perioperative_pulmonary_embolism_or_deep_vein_thrombosis_rate) %>% 
  # Clean and convert to title case
  clean_names(case = "title") %>% 
  # Select the cluster values and our identified important variables
  select(Cluster, any_of(pcaImportantVars)) %>% 
  # Group the table data by cluster, list the mean and median values
  tbl_summary(by = "Cluster",
              statistic = list(all_continuous() ~ c("{mean} ({median})"))) %>% 
  modify_header(label ~ "Variable",
                all_stat_cols() ~ "**Cluster {level}**") %>% 
  modify_caption("Table 8. Summary Statistics of Important Cluster Variables")

```

**HINT**: If these are back to the _original_ values, this is more interpretable for our stakeholder. We will now read this just like a typical boxplot; for example, median (the middle line) `Doctor Communication` seems higher in **Cluster 1** than it does in **Cluster 2** with only some overlap in the distributions (the boxes and whiskers don't really overlap each other). The table confirms this; the median `Doctor Communication` score is 91% in Cluster 1 but only 88% in Cluster 2.   

#### **Question 25**: [2 points]
Notice, for example, that Cluster 1 hospitals seem to include the hospitals that had the __lowest__ pneumonia-related readmission rates, with the average rate ~2% higher in Cluster 2 hospitals. Thus, for our stakeholder, we might decide to rename Cluster 1 as "Highest Performing Hospitals" or something along those lines. Notice also that this cluster had more __below__ national average return hospital days (a "1"). This means that they had more readmitted pneumonia patients who spent less time in the hospital upon readmission than the national average. 

Look at how else you might broadly characterize - or segment - these hospitals. Your job is to make a table for your client, summarizing each of the two clusters with FOUR primary criteria. Make sure to give each cluster an informative name as I did; you're welcome to rename Cluster 1 if you think of a catchier name than I chose!

After you come up with names of the two clusters, you will then come up with at least FOUR characteristics per cluster summarizing what you see in either the boxplots or table. You may choose to focus on different attributes for different clusters. Fill in the code for the table below. I start you out with two examples for Cluster 1, but feel free to add more! 

```{r, echo = FALSE, collapse=TRUE}
dict <- tribble(
~`Top Defining Attributes`, ~`Description`,
"Predicted Pneumonia-related hospital readmissions", "These hospitals have a lower readmission rate, suggesting that hospitals in this cluster may be better at diagnosing and treating pneumonia",
"Return hospital days due to pneumonia", "Readmitted patients spend below the national average number of days in hospital, suggesting these hospitals are able to more quickly treat and stabilize pnuemonia patients", 
"Cleanliness", "These hospitals had an overall higher percentage of reported cleanliness, suggesting that hospitals in this cluster may provide a more clean environment for patients ",
"Doctor Communication", "These hospitals had an overall higher percentage of reports that doctors listened and communicated well, suggesting that hospitals in this cluster may have doctors who are more actively engaged with the patients",
"Predicted Pneumonia-related hospital readmissions", "These hospitals have a higher readmission rate, suggesting that hospitals in this cluster may be worse at diagnosing and treating pneumonia",
"Discharge Information", "These hospitals had an overall lower percentage of reports from patients that they felt they were provided with instructions on what to do during recovery at home, suggesting that hospitals in this cluster may only be focusing on patient care during their stay",
"Communication About Medicines", "These hospitals had an overall lower percentage of reports from patients that they felt they were properly communicated with about the medicines they were taking, suggesting that hospitals in this cluster may not focus on educating their patients about their treatments",
"Doctor Communication", "These hospitals had an overall lower percentage of reports that doctors listened and communicated well, suggesting that hospitals in this cluster may have doctors who are less actively engaged with the patients"
)

dict %>% 
  kable(
    format = "html",
    caption = "Table 9. Hospital segmentation analysis: three types of broad groupings identified.") %>%
    pack_rows("1: Highest Performing Hospitals", 1, 4) %>%
    pack_rows("2: Minimum Communication Hospitals", 5, 8) %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
  )
```

## 5.4 Final Remarks on Segmentation

We don't technically have to stop our segmentation analysis here, but we will so that we can focus on other analyses. One thing we might choose to do in the future, for example, is to choose a classification machine learning algorithm to test the predictions based on these  clusters. 

#### **Question 26**: [1 point]
Make a recommendation for our client for 1-2 machine learning analyses __your team__ would choose to use to test whether these clusters can be used for robust predictions of hospital performance for pneumonia patients. How would you know whether or not your predictions were robust? What would you look for or compare?

> One suggested option would be building separate supervised models for predicting
> our target, with one model for each cluster. 
> Seeing if each model tends to predict in a certain direction would give insight
> into if these clusters can be used to differentiate between hospitals with low
> or high admissions.

> Another suggestion is to build a supervised model that uses cluster assignment
> as a predictor. 
> If using cluster membership as a predictor greatly increases accuracy, then
> we will know that these clusters hold meaningful insights to readmission rates.

> To evaluate these predictions, metrics like R-squared and RMSE can be compared
> between the cluster-based models and non-cluster based models.
> If the cluster-based models have better performance metrics, then it is likely
> that the clusters provide meaningful predictive information. 

# 6 Supervised Learning Methods

We are going to perform two supervised learning methods: an Ordinary Least Squares (OLS) regression followed by an Elastic Net. 

## 6.1 Ordinary Least Squares (OLS) regression 

Earlier in [Section 4]{#multicollinearity} when we assessed multicollinearity, we fit an OLS regression model so that we could calculate variance inflation factors (VIFs). However, because OLS regression is so incredibly sensitive to multicollinearity, we should re-run it now that we've removed some of the most redundant variables. 

Let's model that now, using the `readyTrain` dataset and plot some diagnostic plots. to assess the key assumptions of an OLS linear regression. The four plots are diagnostic plots for the primary assumptions of a linear regression: 

  1. __Linearity__ between the outcome and the predictors

  2. __Normally distributed__ residuals

  3. __Homosecadasticity__ (equality of variances) in the residuals

  4. __No high leverage/influence points__ (no significant outliers)


```{r, echo=FALSE}
## Fit the model
mod <- lm(bc_PredictedReadmissionRate ~ ., 
          data = readyTrain)

## Plot diagnostic plots
plot(mod, 1)
plot(mod, 2)
plot(mod, 3)
plot(mod, 4)
```

##### Figure 17. OLS Regression Assumption Diagnostic Plots

#### **Question 27A**: [1 point]

What is your general assessment of our OLS assumptions? Met or unmet? Why?

> The OLS assumptions appear to be met. 

> The Residuals vs Fitted plot shows the points mostly randomly scatted along
> the center line.
> The line is nearly horizontal at 0,
> although features a slight curve upwards.
> However, this might be interpreted as close enough to normal
> as the points do not seem to form any real pattern. 

> The distribution appears to be almost normal.
> The Q-Q plot shows some points on both ends diverging from the diagonal.
> However, most of the points do in fact fall in line with the normal pattern.
> In addition, the p-value of the Shapiro-Wilk test is 0.06735,
> which is above 0.05.
> This tells us that we cannot reject the null hypothesis,
> suggesting that the data is at least somewhat close to normal.

> The Residuals vs Fitted plot shows the points mostly randomly scatted along
> the center line.
> In addition, the Scale-Location plot shows an aproximately horizontal fit. 
> This suggests that there is omosecadasticity in the residuals.

> All Cook's distances appear to be below 1, and never much higher than 0.30.
> This suggests that there are no significant outliers.

**Note:** If you are not entirely sure about the Q-Q plot, you can run a Shapiro-Wilk test on the residuals using the following code:

```{r}
shapiro.test(residuals(mod))
```

The **most important** OLS assumption for Elastic Net is __linearity of the relationship between predictors and the response__. The other, stricter OLS assumptions don't matter as much because of regularization.

**Do you find support for linearity here**? Why or why not?

> As stated above, the support for linearity is most likely there.
> There is a slight curve in the Residuals vs Fitted plot,
> however it is not to a great extent.
> The points also do not form a clear pattern
> and are mostly balanced around the red line.
> Therefore, it is arguable that there is support for linearity.

2. Let's now assess how well our model fits with the adjusted $R^2$.

```{r, echo = FALSE}
## Extract just the R-squared
summary(mod)$adj.r.squared
```

#### **Question 27B**: [1 point] {#rmse}
It is a surprisingly high (although it isn't stellar). You may need to do some outside research, but explain in a sentence or two why, generally speaking, we use **adjusted** $R^2$ and not the unadjusted one (confusingly labeled as `multiple r-squared` in the `summary()` output below). 

> The unadjusted R-squared value is prone to inflation whenever more predictors are added.
> This value will rise even if the predictors do not add any extra significance
> to the model.

> Adjusted R-squared penalizes predictors which fail to add extra significance.
> This provides us with a more accurate, meaningful value to interpret.
> Therefore, we use it over the unadjusted value. 

3. Find the best model. 

Use the `step()` function to do a backward regression to find our __most parsimonious__ model. Stepwise regression drops terms that are not significant or important, thereby only keeping the features that contribute to the overall explanation of variance in predicted readmission rate.

```{r, results='hide'}
## Perform a backward, stepwise regression to find the most parsimonious model
bestMod <- step(mod, 
            direction = "backward", 
            trace = FALSE)
```

Lastly, let's take a peek at the results. We will use a package called `stargazer` to help us visualize the results in a nicely organized way.

```{r, results='asis'}
## Print a table using stargazer
stargazer(bestMod, 
          type = "html",
          title = "Table 10. Parsimonious OLS Regression Results.")
```

Lastly, let's assess how good a predictive model our OLS regression model is. We will use the `predict()` function to first fit our most parsimonious model `bestMod` to the test data:

```{r}
## Make the predictions
predictions <- predict(bestMod, newdata = readyTest)
```

The Root Square Mean Error ($RMSE$) is the square root of the average of the squared differences between predicted and actual value, such that $RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^n\cdot(y_i-\hat{y}_i)^2}$ where $y_i$ is each observed value, $\hat{y}_i$ is each corresponding predicted value, and $n$ is the number of observations. **NOTE**: Recall that a residual is $y_i-\hat{y}_i$ - the distance between every point and its predicted value! It's also important to realize that the $RMSE$ gives more weight to large errors because of squaring. Thus, it's sensitive to outliers.

#### **Question 27C**: [1 point]
Can you interpret what the RMSE tells your client here? Why or why not? What about the $R^2$?

**Hint 1**: Remember that the outcome here is Box-Cox transformed. Would you need to undo the transformation to be able to assess what the RMSE (or $R^2$) is actually telling us? Recall that the Box-Cox $\hat{y} = \frac{1}{y^\lambda}$ where our $\lambda = -0.3$ AND we also centered and scaled these data. 

**Hint 2**: Is it practical to undo both transformations? How does that hinder our ability to make an interpretation for our client?

```{r}
# Calculate RMSE
rmse <- sqrt(mean((readyTest$bc_PredictedReadmissionRate - predictions)^2)) 
rmse # Print results
```



> We obtained a RMSE of 0.8102472.
> However, the RMSE is suppose to be interpreted within the original units
> of our target variable.
> Since the data is transformed, this value is not truly comparable to our target.

> Undoing both transformations, while technically possible, is not practical for
> our purposes. 
> Considering that the model was made to predict within the transformed space,
> interpretation would become extremely complex if we were to try and judge its
> performance in the original space. 

> The R-squared value still tells us how well the model fits the data.
> However, it is telling us how well the model fits the transformed data.
> This must be kept in mind when interpreting the R-squared value.


Lastly, let's graph the effect predictive ability of our OLS model by comparing the relationship between the **true** values of `bc_PredictedReadmissionRate` from the `readyTest` dataset to the predictions we made using the model on the testing data.

```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.cap="Figure 18. Predictions Plot for OLS Regression"}
## Add the predictions to the dataset
readyTest$predictions <- predictions

## Graph what the predictions look like relative to the actual data
ggplot(readyTest, 
       aes(x = predictions, y = bc_PredictedReadmissionRate)) +
       geom_point(alpha = 0.5, color = "hotpink") +
       geom_smooth(color = "hotpink", fill = "hotpink", se = T, method = "lm") +
  theme_minimal() +
  labs(title = "OLS Multiple Linear Regression Predictive Power",
       x = "Predicted Readmission Rate from Model",
       y = "Actual Readmission Rate")
```

**NOTE** that, while there is an obvious linear trend, it is weak. There is a lot of error around the predictions from the OLS regression!

## 6.2 The Elastic Net

Perhaps the seemingly low $R^2$ is because we still have too much multicollinearity or overfitting. So, as we discussed in lecture, we can address this using regularized regressions, such as the __Elastic Net__, which is more robust to multicollinearity by assessing penalties for non-zero variance. Typically, we would likely conduct a Ridge and/or LASSO before running an Elastic Net, but since we already know that (1) we have a fairly high amount of multicollinearity in our original `readyTrain` dataset and (2) we know we want to be able to perform feature reduction and figure out which features are important predictors of hospital readmissions, we're going to jump straight into Elastic Net, which performs both.

### 6.2.1 The Elastic Net Unpacked

In general, regularization is a technique that applies a penalty term to a cost function of a machine learning model, like the OLS regression. The reason is to discourage overfitting. This penalty term constrains the model's coefficients, which limits how flexible they are during training. Why? 

The more flexible the coefficients, the less likely they will "learn" new data, i.e., the more generalizable they are! Thus, by applying this penalty (constraint), it improves the model's performance on "unseen" or new data.

As mentioned in lecture, Elastic Net regression is a general regularization technique that combines the regularization techniques of Ridge and LASSO to help us perform both feature selection (i.e., significant coefficients) and feature reduction (importance). The elastic net can even help us with the $P >> n$ problem! 
Elastic net has two regularization terms, called __L1__ (or the Lasso regularization term) and __L2__ (the Ridge term).

* **L1**: makes some of the coefficients zero, thereby selecting only the most important features (shrinks them to zero). It is often said that this __encourages sparsity in the model__.

* **L2**: reduces the coefficients to small but non-zero values, thereby reducing the coefficients of the unimportant features (which we can use to determine significance). It does this by adding a penalty term to the cost function of the model, which is proportional to the coefficient squared. This allows us to retain all the features in the model but reduces the overall impact of the non-significant or less-important ones.

Thus, by combining these techniques together, the Elastic Net is balancing feature selection with feature reduction! If you're interested in reading the original paper on Elastic Net by Zou and Hastie (2005), you can find a free PDF of the paper [here](https://www.stat.purdue.edu/~tlzhang/mathstat/ElasticNet.pdf).

### 6.2.2 Running, tuning, & cross-validating the Elastic Net

The elastic net can be run using the `glmnet` package (which we are accessing via `caret`), and can be cross-validated as random forest and SVM from our last project could be. This is a big improvement over OLS regression, which cannot be tuned. Additionally, the elastic net has __two hyperparameters__ that we can tune, $\alpha$ and $\lambda$ (not to be confused with the $\lambda$ from Box-Cox transformation - Greek letters are abundant in statistics! Why do you think I named our course teams as I did?!). 

* $\alpha$ is the strength of the regularization, representing the balance between __L1__ and __L2__ regularization. Larger $\alpha$ results in L1 regularization (feature selection / Ridge) whereas smaller $\alpha$ results in more L2 regularization (higher shrinkage / feature reduction / Lasso). When $\alpha=0$ the regression is equivalent to OLS regression! $\alpha$ is also referred to as the __mixing percent__ of L1 and L2 regularization.


* $\lambda$ is the shrinkage parameter, such that when $\lambda = 0$ no shrinkage is performed and is equivalent to an OLS regression. As $\lambda$ increases, the coefficients are increasingly shrunk and thus the more features will have coefficients shrunk to zero, regardless of the value of $\alpha$. 


#### 6.2.2.1  We will start by setting our CV parameters just as we did on the last project, using the `trainControl()` function in `caret`. 

**NOTE** that I am performing a 5-fold repeated cross-validation (CV) with 5 iterations.

```{r}
ctrl <- trainControl(method = "repeatedcv",  ## Do repeated CV
                     number = 5,             ## Number of k-folds
                     repeats = 5,            ## Number of repeats
                     search = "grid",        ## Grid search (vs. random)
                     verboseIter = FALSE)    ## Don't show me all the results
```

#### 6.2.2.2 Then, we fit the model using the `train()` function in `caret`, specifying that we want to perform an elastic net, which will tune the model and perform CV per the specifications in the `trainControl()`. **NOTE** that I am allowing the values of $\alpha$ to range from $0 \rightarrow 1$ and allowing $\lambda$ to range from $0 \rightarrow 5$. 

```{r}
## Create a search grid that allows alpha to range from 0 --> and 
## allows lambda to range from 0 --> 5
searchGrid <- expand.grid(.alpha = seq(0, 1, length.out = 10), 
                          .lambda = seq(0, 5, length.out = 15))

## Fit the elastic net using the tuning grid and CV scheme laid out
elasticMod <- train(bc_PredictedReadmissionRate ~ ., 
                    data = readyTrain, 
                    method = "glmnet", 
                    tuneGrid = searchGrid,
                    trControl = ctrl) 
```

#### **Question 28**: [1 point]
Explain what we are doing here. You may want to look up the parameters of the `trainControl()` and `train()` functions. Make sure to address what type of CV and hyperparameter search is being performed, as well as if both the $\alpha$ and $\lambda$ hyperparameters of elastic net are being tuned. 

**Hint 1**: Why are we doing cross-validation, and specifically why did I choose the one I did?

**Hint 2**: What does $\lambda = 0$ mean? How about $\alpha = 0$? 

> First we are setting the cross-validation parameters using trainControl().
> We are doing a repeated k-fold cross-validation, where k = 5.
> That means that the data is partitioned into 5 folds.
> Four folds are used for training, and one fold is used for testing.
> This process is repeated for every fold.
> That entire process represents one repetition,
> so we repeat this process five times (as defined by repeats = 5).

> Cross-validation helps prevent overfitting by iteratively training and testing
> the model on different data. 
> A repeated k-fold helps this further by repeating the CV process with varying
> partitions of the data. 
> Doing so helps prevent a single split from having too much influence on
> the model.

> After setting our cross-validation parameters, we set up and run our
> elastic net model.
> Here, we first define our target and what predictors to use.
> The "~ ." tells the model to use all of the predictors in the dataset.
> We then define our dataset as our train set and our model type as "glmnet",
> which is our elastic net implementation.
> The tuneGrid = searchGrid tells the code to implement a grid search to test and
> identify the best combinations of hyperparameters. 

> Our grid search is specifically testing every combination of values for
> lambda and alpha. 
> We have defined our searchGrid for lambda values from 0 to 1
> and alpha values from 0 to 5.
> We have also set the grid to allow up to 10 values of lambda and 
> 15 values of alpha, giving us in total 150 combinations to test. 

> A lambda of 0 would mean that the model does not apply regularization.
> This will be a standard linear regression model.

> An alpha of 0 would mean that there would only be L2 penalty.
> This will be a ridge regression. 



### 6.2.3 Elastic Net Results

#### 6.2.3.1 We are going to first take a look at the top 15 results of the Elastic Net in a table form, sorted from lowest $RMSE$:

```{r, echo = FALSE, message=FALSE}
elasticMod$results %>% 
  data.frame() %>% 
  arrange(RMSE) %>% 
  top_n(15) %>% 
kable(digits = 2,
    format = "html",
    caption = "Table 11. Top Performing Results of the Elastic Net Tuning & 5-fold Repeated Cross-Validation") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
)
```

**NOTE**: Folds that fail to converge will have an $R^2$ = `NA`.

#### 6.2.3.2 We can also extract the hyperparameters of our best tuned model:

$\alpha$:
```{r, echo=FALSE}
elasticMod$bestTune$alpha
```

$\lambda$:
```{r,echo=FALSE}
elasticMod$bestTune$lambda
```

#### **Question 29**: [1 point]
Check the values of $\alpha$ and $\lambda$ here. What do they suggest about the optimal regularization that is being fit here? (Recall that when both $\alpha$ and $\lambda$ are zero, it's an OLS regression!)

> Both alpha and lambda are zero, which tells us our optimal regularization is
> an OLS regression.
> This means that penalizing coefficients did not improve performance.

Let's also plot the results of the tuning search. Can you figure out the type of tuning search you did? (**Hint**: the visual pattern should confirm your answer from earlier!) The diamond denotes the best combination of hyperparameters!

```{r}
results <- elasticMod$results %>% 
  data.frame()
best <- results[as.numeric(rownames(elasticMod$bestTune)), ]
```

```{r, echo = FALSE,  fig.cap="Figure 19. Hyperparamater Grid Search for Elastic Net"}
results %>% 
  drop_na() %>% 
  mutate(lambda = round(lambda, 1)) %>%  
  ggplot(aes(x=alpha, y = RMSE, color = as.factor(lambda))) +
  geom_point() +
  theme_minimal() +
  geom_point(aes(x = best$alpha, best$RMSE), 
             shape = 5, 
             size = 3,
             color = "black") +
  labs(title = "Elastic Net Hyperparameter Tuning",
       subtitle = "Best mixing percentage + λ shown as a diamond",
       x = expression(alpha), 
       color = expression(lambda)) + 
  scale_color_manual(values = c(skittles, "#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#00AFBB", "#E7B800", "#FC4E07"))
```

#### 6.2.3.3  Important Features

Important features can also be extracted, using the `varImp()` function from `caret`.

```{r}
important <- varImp(elasticMod)$importance
```

```{r, echo = FALSE, fig.cap="Figure 20. Variable Importance from Best Elastic Net Model"}
important %>% 
  mutate(Feature = rownames(important)) %>% 
  mutate(Feature = gsub("\\.", " ", Feature)) %>% 
  arrange(desc(Overall)) %>% 
  ggplot(aes(y = Overall, fill = Overall, x = fct_reorder(Feature, Overall))) +
  geom_col(color = "navy") + 
  scale_fill_continuous(low = "#DCCAE3", high = "#C00E50") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Feature importance as determined by \nElastic Net",
       x = "",
       y = "Importance", 
       fill = "")
```

#### **Question 30**: [1 point]

Which of the features were most important? How does this compare to the most important features out of OLS regression? Make sure you're looking at the __absolute value of the beta coefficients to assess importance in OLS regression__. 

> The most important feature is Hospital return days for pneumonia patients.
> Following, less important features include:

> Medicare spending per patient

> Death rate for pneumonia patients

> State

> This compares very well to the most important features of the OLS regression.
> The top feature was also Hospital return days for pneumonia patients, with a
> beta (absolute value) of 0.396.
> Following important features included:

> Medicare spending per patient (0.167)

> Death rate for pneumonia patients (0.154)

> State (0.110)

> As seen, these are the same features in the same order.

How about about of PCA? You may have already noticed that importance almost seems inverted! Why?!

**Hint 1**: What is the fundamental difference between a PCA and any regression?

**Hint 2**: What does "importance" mean in a PCA vs. any regression?

**Hint 3**: What does the sign of the loadings mean in a PCA? Are they actual or arbitrary?

> The difference in importance seen between PCA and regression comes down to
> the fundamental purpose of each.
> Regression is a supervised modeling method that aims to find predictors that
> help describe the target variable.
> The variables that have the highest effect on the target will be deemed
> most important.

> On the other hand, PCA is unsupervised.
> Its goal is to identify PCs that will capture the greatest amount of
> variance of predictors.
> Unlike regression, it has no specific concern for our target variable.
> Importance here is determined by which predictors contribute the highest
> amount of variance.

> The inverse in importance can be attributed to these differences.
> It appears that the predictors which contributed to the most variance overall
> was rather opposite to the predictors that have an effect on the target.

> PCA loading signs are not like regression signs.
> In regression, the signs indicate how the predictors impact the target.
> PCA loading signs, on the other hand, do not have a significant impact on
> interpretation. 

#### 6.2.3.2 Check model performance

Assessing __model performance__ means assessing how well our training model does when applied to "unseen" data, here our `readyTest` dataset. Because we know the right answer - the true value of `PredictedReadmissionRate`, we can assess the error that the training model produces when fitted against the testing data. We measure that error as $RMSE$ and $MAE$. We briefly reviewed what the $RMSE$ measures back in [Question 36]{#rmse}, so I will only review what the $MAE$ is here.

Mean Absolute Error ($MAE$) is the average of the absolute differences between predicted and actual values, such that $MAE = \frac{1}{n}\sum_{i=1}^n\cdot|y_i-\hat{y}_i|$ where $y_i$ is each observed value, $\hat{y}_i$ is each corresponding predicted value, and $n$ is the number of observations. So, unlike $RMSE$, $MAE$ treats all errors equally and therefore is more robust to outliers. So, $MAE$ will tell you the average magnitude of the error, regardless of direction!

Let's also make predictions on the testing sets for both the OLS and Elastic Net models using the `postResample()` function in `caret`; this will not only do predictions, but it will enable us to more easily generate metrics that we can use to compare model performance, such as $R^2$ and MAE.

```{r}
## Post-resample on OLS
prOLS <- postResample(pred = predict(mod, newdata = readyTest), 
                      obs = readyTest$bc_PredictedReadmissionRate)

## Post-resample on Elastic Net
prElastic <- postResample(pred = predict(elasticMod, newdata = readyTest), 
                      obs = readyTest$bc_PredictedReadmissionRate)
```

```{r, echo=FALSE}
## Display the output as a table
rbind(prOLS, prElastic) %>% 
  kable(digits = 2,
    format = "html",
    caption = "Table 12. Comparison of Model Performance of OLS and Elastic Net Regressions") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
)
```

#### 6.2.3.3 Check for overfitting/underfitting

#### **Question 31**: [1 point]

Using the same method, check for overfitting / underfitting. Make predictions using `readyTrain` rather than `readyTest`, and then check how the RMSE on the training data compares to the RMSE of the training data for the Elastic Net. How do they compare? (Recall what we discussed when we did the gene expression demo: the relationship of the test vs. training accuracy can be used to assess overfitting/underfitting. The same logic applies to RMSE too!) __How do both compare to what we got out of the OLS regression?__ Which would you say is the better predictive model, the OLS regression or the Elastic Net? Why?

> Overall, both models performed identically.

> This is not surprising, as we saw that the elastic net model had both a 
> lambda and alpha of 0.
> We had interpreted this as meaning the elastic net was identical to an 
> OLS regression.
> Therefore, both models are comparatively the same.

```{r}
## Post-resample on OLS
prOLS <- postResample(pred = predict(mod, newdata = readyTrain), 
                      obs = readyTest$bc_PredictedReadmissionRate)

## Post-resample on Elastic Net
prElastic <- postResample(pred = predict(elasticMod, newdata = readyTrain), 
                      obs = readyTest$bc_PredictedReadmissionRate)
```

```{r, echo=FALSE}
## Display the output as a table
rbind(prOLS, prElastic) %>% 
  kable(digits = 2,
    format = "html",
    caption = "Table 13. Comparison of Training Performance of OLS and Elastic Net Regressions") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
)
```

#### 6.2.3.4 Graphical comparison of the two models' predictions

```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.cap="Figure 19. Predictions Plot for Elastic Net vs. OLS Regression"}
elasticPredictions <- predict(elasticMod, newdata = readyTest)
testingResults <- c(readyTest$predictions,
                    elasticPredictions)
label <- c(rep("OLS Prediction", nrow(readyTest)),
           rep("Elastic Net Prediction", length(elasticPredictions)))
temp <- cbind.data.frame(testingResults, label)
temp$actual <- rep(readyTest$bc_PredictedReadmissionRate, 2)

## Graph what the predictions look like relative to the actual data
ggplot(temp, 
       aes(x = testingResults, y = actual, fill = label, color = label)) +
       geom_point(alpha = 0.5) +
       geom_smooth(se = T, method = "lm", alpha = 0.45) +
  scale_color_manual(values = c("navy", "hotpink")) +
  scale_fill_manual(values = c("navy", "hotpink")) +
  theme_minimal() +
  labs(title = "Elastic Net vs. OLS Predictive Power",
       x = "Predicted Readmission Rate from Model",
       y = "Actual Readmission Rate",
       fill = "", 
       color = "") +
  theme(legend.position = "bottom")
```

**It should not come as a surprise, but these models are performing VERY similarly!**


#### **Question 32**: [4 points]
What conclusions do you make at this point about whether to make predictions with an OLS regression, Elastic Net, or neither? What recommendations would you make to our client at this point? What other analyses might your suggest, or other data we might want to include? Please try to flesh out 2-3 recommendations in at least a paragraph.

**HINT**: Make sure to ask yourself whether these data appear linear or not. Could non-linearity be a cause for a low $R^2$? If not, what other causes are there and how will that impact your recommendations?

> Overall, performance between both models is very similar. However, neither model does a great job at predicting readmissions. Both models, when applied to the test data, obtained an R-squared value of 0.35. This means that the model accounts for 35% of the variation in predicted readmission rates. Inversely, our model does not account for 65% of the variation in our target. 

> Looking back at the distribution of our data points, it is clear that while there is some semblance of a positive linear relation, it is rather weak. Therefore, our low R-squared values could be a result of utilizing incorrect model types. Both models tested were variants of linear regressions. Testing a non-linear model is recommended to examine if it can better capture the variance.

> Another suggestion is to return to our cluster analysis. We previously identified two potential clusters in the data. Building off of this, it would be recommended to create a kNN model to predict data based on these clusters. This kind of analysis will further demonstrate if our data can meaningfully be described by these clusters, and will provide us with a tool to help identify where new data points fall. 

# 7 Choose Your Own Adventure

It's time to choose your adventure! After working through this demo and answering the 31 questions, you will choose one of three possible trajectories. The following are some brief hints or tips for each of the choices available to you.


## Adventure 3. 
**[Continue analyzing the pneuomnia data.]**

- Make sure to justify your choice.

- Choose at least two new algorithms to perform. They can be unsupervised or supervised, but must be 2+.

- A natural extension would be to do a Ridge and/or a LASSO; the former would tell you if you should be controlling for multicollinearity more than you currently are, the latter would tell you if you need to further shrink coefficients than you currently are. Additionally, you could consider a regression-based method that incorporates **non-linearity**, such as a random forest. However, you are free to choose anything you think is appropriate here! 
- Other choices I think you could potentially justify to stakeholders, if you were interested: 

1. Switch to classification-based random forest using `ComparedToNational_Hospital return days for pneumonia patients`

2. kNN using the 2 clusters we've identified and named in our segmentation analysis

3. Our data are also ready for a neural network analysis (e.g., a feed-forward NN)!

> For this section, we will be exploring three models.

> 1. A Random Forest

> 2. A kNN

> 3. A Random Forest including Cluster Assignments

## Random Forest

> As explained earlier, our data does not appear to be strongly linear.
> Random forest is a non-linear model type, and therefore is a good option to
> see if a non-linear model is better at predicting our target.

> A random forest works by creating an ensemble of decision trees.
> A decision tree model classifies data through a series of splits,
> where each split represents some threshold within a variable. 
> This allows for complex, non-linear relations to be mapped out.

```{r}
# Cross Validation
ctrl <- trainControl(method = "repeatedcv",
                    number = 5,
                    repeats = 5,
                    search = "grid",
                    verboseIter = FALSE)

```

```{r}
# Random Forest Training

rfMod <- train(bc_PredictedReadmissionRate ~ .,
               data = readyTrain,
               method = "rf",           # Select Random Forest model
               #tuneGrid = rfGrid,       # Use rfGrid for hyperparameter tuning
               trControl = ctrl)
```

```{r}
# Testing the Model

rfPredictions <- predict(rfMod, newdata = readyTest)
```


```{r}
# Visualizing Results

prRF <- postResample(pred = rfPredictions, obs = readyTest$bc_PredictedReadmissionRate)

rfTable <- as.data.frame(t(prRF))
rownames(rfTable) <- "Random Forest"

rfTable %>%
  kable(digits = 2,
        format = "html",
        caption = "Table: Random Forest Test Set Performance") %>%
  kable_styling(bootstrap_options = c("striped", "full_width" = F))
```

```{r}
temp_rf <- data.frame(
  predicted = rfPredictions,
  actual = readyTest$bc_PredictedReadmissionRate
)

ggplot(temp_rf, aes(x = predicted, y = actual)) +
  geom_point(alpha = 0.5, color = "forestgreen") +
  geom_smooth(se = TRUE, method = "lm", color = "black", linetype = "dashed", alpha = 0.45) +
  theme_minimal() +
  labs(
    title = "Random Forest: Predicted vs. Actual Readmission Rate (Test Set)",
    x = "Predicted Readmission Rate",
    y = "Actual Readmission Rate"
  )
```

> Compared to our previous models, the random forest did slightly better.
> The R-squared value is 0.4, which is 5% better than the linear models. 

> The RMSE value is 0.77, which is lower than the RMSE of 0.81 from the two
> linear models. 
> This means that there is slightly less distance between actual and predicted
> values in the random forest model than in the linear models. 

> We see a similar, but lesser, comparison in MAE.
> The random forest obtained a MAE of 0.61,
> while the linear models obtained a MAE of 0.64.

> Overall, the increase in performance is moderate, but not huge.
> However, this does support the idea that our data contains some
> non-linearity to it which is better described using non-linear models like
> random forest. 


## kNN

> As explained earlier, creating a kNN model will allow us to take a closer look
> at the cluster relations we had previously identified. 

> The two clusters were:

> Cluster 1: Highest Performing Hospitals

> Cluster 2: Minimum Communication Hospitals

> Using a kNN model will help validate the existence of these clusters.
> If the model has a high accuracy at predicting clusters, then it can be assumed
> that the identified clusters represent real divisions in our data.
> If the model has poort accuracy, then it is less likely that the data can be
> accurately described by these clusters. 

```{r}
# Creating Clusters

kmeans2 <- kmeans(readyTrain, centers = 2, nstart = 25, iter.max = 1000, algorithm = "MacQueen")
cluster_labels <- as.factor(kmeans2$cluster)
```

```{r}
# Prepare data with labels

readyTrain_knn <- readyTrain
readyTrain_knn$cluster <- cluster_labels
```

```{r}
# Cross Validation
ctrl <- trainControl(method = "repeatedcv",
                    number = 5,
                    repeats = 5)

# GridSearch for k neighbors
knnGrid <- expand.grid(k = seq(3, 15, by = 2))

# Train kNN model
knnMod <- train(cluster ~ .,
                data = readyTrain_knn,
                method = "knn",
                tuneGrid = knnGrid,
                trControl = ctrl)
```

```{r}
# Prepare test data with labels

assign_cluster <- function(test_data, centers) {
  apply(test_data, 1, function(x) {
    dists <- apply(centers, 1, function(center) sum((x - center)^2))
    which.min(dists)
  })
}

test_clusters <- assign_cluster(readyTest, kmeans2$centers)
test_clusters <- as.factor(test_clusters)

readyTest_knn <- readyTest
readyTest_knn$cluster <- test_clusters
```

```{r}
# kNN on test set

knn_predictions <- predict(knnMod, newdata = readyTest_knn)

confusionMatrix(knn_predictions, readyTest_knn$cluster)
```

> Our kNN model was aproximately 86% accurate in predicting which cluster
> a data point belonged to.
> This suggests there is a reality in the two clusters which we have identified.

> Reviewing what we have discussed, one of the key distinctions between the two
> clusters was the readmission rates.
> Cluster 1 had signigicantly lower readmission rates than cluster 2.

> We also identified that cluster 2 showed many signs of what could be classified
> as a "low communication" approach to healthcare.
> Compared to cluster 1, patients reported that they felt communication was minimal
> for things like post-hospital recovery, information on medicine, and general
> communication from doctors. 

> This suggests that communication with patients is a key aspect to 
> controling readmissions. 

## Random Forest with Clusters

> Now that we have demonstrated both the effectiveness of random forests and
> the apparent legitimacy of the clusters, we will create a new random forest
> model that considers cluster assignment when predicting readmissions.

```{r}
# Preparing Train/Test Data

readyTrain_rf <- readyTrain
readyTrain_rf$cluster <- cluster_labels

test_clusters <- assign_cluster(readyTest, kmeans2$centers)
test_clusters <- as.factor(test_clusters)

readyTest_rf <- readyTest
readyTest_rf$cluster <- test_clusters
```

```{r}
# Cross Validation

ctrl <- trainControl(method = "repeatedcv",
                    number = 5,
                    repeats = 5,
                    search = "grid",
                    verboseIter = FALSE)
```

```{r}
# Training Random Forest

rfMod_cluster <- train(
  bc_PredictedReadmissionRate ~ .,
  data = readyTrain_rf,
  method = "rf",
  trControl = ctrl
)
```

```{r}
# Testing Random Forest

rfPredictions_cluster <- predict(rfMod_cluster, newdata = readyTest_rf)
```

```{r}
# Visualize Results

prRF_cluster <- postResample(pred = rfPredictions_cluster, obs = readyTest_rf$bc_PredictedReadmissionRate)

rfTable_cluster <- as.data.frame(t(prRF_cluster))
rownames(rfTable_cluster) <- "Random Forest (with Cluster)"

rfTable_cluster %>%
  kable(digits = 2,
        format = "html",
        caption = "Table: Random Forest (with Cluster) Test Set Performance") %>%
  kable_styling(bootstrap_options = c("striped", "full_width" = F))

temp_rf_cluster <- data.frame(
  predicted = rfPredictions_cluster,
  actual = readyTest_rf$bc_PredictedReadmissionRate
)

ggplot(temp_rf_cluster, aes(x = predicted, y = actual)) +
  geom_point(alpha = 0.5, color = "forestgreen") +
  geom_smooth(se = TRUE, method = "lm", color = "black", linetype = "dashed", alpha = 0.45) +
  theme_minimal() +
  labs(
    title = "Random Forest (with Cluster): Predicted vs. Actual Readmission Rate (Test Set)",
    x = "Predicted Readmission Rate",
    y = "Actual Readmission Rate"
  )
```

> Surprisingly, we do not see an increase in performance by incorporating
> the cluster data into the random forest. 
> This could suggest that while the clusters have some real identifiable 
> existence, their identification alone does not provide significant 
> predicting power for our model.

> Future suggestions could include:

> 1. Trying to indentify and test different clusters

> 2. Trying to apply different machine learning techniques to our clusters


## Deliverables

I am looking for 1-2 markdown documents with their knitted HTML. For example, if you're choosing Adventures 1 or 2, you may want to work through this document once, make a copy of this markdown document, move the source code to the __TOP__, edit the source code as needed, and then go through this again from the beginning using the new condition(s) you chose. 

Alternatively, if you're choosing Adventure 3, maybe you just choose to add on to the bottom of this document, replacing the 'Choose Your Own Adventure' section with your actual adventure.

Just make sure to answer the questions well,and make sure to justify the decisions you make. Tell me WHY you're choosing the condition(s) you are in Adventures 1 or 2, or Tell me WHY you're doing the analyses you are in Adventure 3. Other than that, make this your own learning adventure!

# References

