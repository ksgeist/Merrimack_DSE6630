@Manual{R-base,
  title = {R: A Language and Environment for Statistical
           Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2019},
  url = {https://www.R-project.org},
}

@article{PMID25598730,
 title={Pneumonia readmissions: risk factors and implications},
 volume={14},
 abstract={Pneumonia continues to be a severe health problem in the United States, responsible for close to 1 million hospital admissions and nearly 140,000 hospital readmissions per year. The literature on this topic suggests that approximately 1 in 5 patients with pneumonia is readmitted to the hospital within 30 days of discharge and that most readmissions are not because of pneumonia-related causes.},
 number={4},
 journal={Ochsner Journal},
 author={De Alba, Israel and Amin, Alpesh},
 year={2014},
 pages={649-54}
}

@article{10.1111/j.2517-6161.1964.tb00553.x,
    author = {Box, G. E. P. and Cox, D. R.},
    title = {An Analysis of Transformations},
    journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
    volume = {26},
    number = {2},
    pages = {211-243},
    year = {2018},
    month = {12},
    abstract = {In the analysis of data it is often assumed that observations y  1, y  2, …, yn are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters θ. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples.},
    issn = {0035-9246},
    doi = {10.1111/j.2517-6161.1964.tb00553.x},
    url = {https://doi.org/10.1111/j.2517-6161.1964.tb00553.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/26/2/211/49099371/jrsssb\_26\_2\_211.pdf},
}

@article{10.1093/biomet/87.4.954,
    author = {Yeo, In‐Kwon and Johnson, Richard A.},
    title = {A new family of power transformations to improve normality or symmetry},
    journal = {Biometrika},
    volume = {87},
    number = {4},
    pages = {954-959},
    year = {2000},
    month = {12},
    abstract = {We introduce a new power transformation family which is well defined on the whole real line and which is appropriate for reducing skewness and to approximate normality. It has properties similar to those of the Box–Cox transformation for positive variables. The large‐sample properties of the transformation are investigated in the contect of a single random sample.},
    issn = {0006-3444},
    doi = {10.1093/biomet/87.4.954},
    url = {https://doi.org/10.1093/biomet/87.4.954},
    eprint = {https://academic.oup.com/biomet/article-pdf/87/4/954/633221/870954.pdf},
}

@article{10.1111/j.1467-9868.2005.00503.x,
    author = {Zou, Hui and Hastie, Trevor},
    title = {Regularization and Variable Selection Via the Elastic Net},
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {67},
    number = {2},
    pages = {301-320},
    year = {2005},
    month = {03},
    abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
    issn = {1369-7412},
    doi = {10.1111/j.1467-9868.2005.00503.x},
    url = {https://doi.org/10.1111/j.1467-9868.2005.00503.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/67/2/301/49795094/jrsssb\_67\_2\_301.pdf},
}
