---
title: "Demo 1: Bioinformatics of Gene Expression"
subtitle: "Merrimack College DSE6630: Healthcare & Life Sciences Analytics"
author: "Katherine S. Geist, PhD"
date: "10 April 2024"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
bibliography: references.bib  
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      cache.comments = TRUE,
                      size = 13)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Turn off scientific notation
options(scipen=999)

# Set seed
set.seed(50009)


# Clean, set up, and load
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))

pacman::p_load(tidyverse, 
               ggplot2, 
               kableExtra,
               BiocManager,
               DESeq2,
               pheatmap,
               RColorBrewer,
               vsn,
               gridExtra,
               ggrepel,
               e1071,
               caret,
               randomForest,
               ranger,
               multtest
)
library("DBI")
library("DESeq2")
```

# Introduction

You have just been hired by a university to work on a team with other biological data scientists working on responses of organisms to climate change. Native pollinators are of special importance because of agriculture and food security; but some pollinators are likely to experience thermal intolerance, especially to changes in temperatures that alter their core physiology because of rapid temperature changes. 

Exposing bees and other insects to extreme cold puts them into what is often referred to as a __chill coma__: their body functions slow to the point that they go into torpor, which means they cannot move. Their physiology slows to the point that they are unable to move their flight muscles, for example. Here, we have differential gene expression data from the North American bumblebee, *Bombus impatiens*, which is both a native pollinator found throughout most of the U.S and Canada. It is also an important agricultural species because of its pollination services! Chill coma is essential to the survival of *B. impatiens* maiden queens as they overwinter; they need to emerge from their burrows in the ground in the spring to start their nests, lay eggs, rear larvae, and ultimately pollinate our food plants. Thus, understanding the genes underlying the species' physiological responses to shifts in cold temperature, especially in light of predicted temperature shifts in North America, could be particularly important. 

You were asked to work with other climate data scientists to re-analyze these chill coma experiment  expression data using machine learning methods. Your goal is to perform a __benchmarking study__ to see how your results compare to conventional methods. You will start with conventional methods and then apply machine learning methods after that to draw your comparisons.

**These data have never been explored this way before**. You can find the original paper [here]("https://drive.google.com/file/d/1X3z251hebpjBVdOXsQtYzDG-SaeCa6M1/view") by Verble et al. (2023). Not only are you contributing to what we understand about best practices for gene expression analysis but you are doing **novel research** too! The original results by Verble et al. (2023) were very compelling, but these data are a perfect opportunity to dp some benchmarking! You will see why...

## Study Overview

According to the study, *Bombus impatiens* colonies were reared indoors for 72 hours and then treated individuals put into chill coma (Verble et al., 2023) by exposing them to $\approx$ 0$^{\circ}$ C (32$^{\circ} F$ for 75 min. Individuals were then killed at the following intervals to isolate their RNA (and thus measure gene expression at those number of minutes post-coma): 0 min, 10 min, 30 min, 120 min, and 720 min. Bumblebees (cold-treated and control from the same colony) were sacrficied by flash freezing them at -80 to await RNA isolation. This is because RNA is very unstable at room temperature, thus instantaneous death is essential to stop gene expression changes that might be happening in the organism. Overall, you have been given information about the bumblebee's natal colony, whether the sample was cold-exposed or control, and the duration of chill coma.


## Sample Metadata
__Metadata__ refer to information about sequencing files that describe the samples. This can include information about how the samples were collected, the __phenotypes__ of the samples (the traits, physical or behavioral, that are thought to be at least partially controlled by gene expression diversity), and IDs for individuals or, in this case, colonies from which the data come.

```{r, echo=FALSE, fig.height=3}
# Load the metadata files
metadata <- read_csv("../../I. Bioinformatics/Demo/23524047/Complete_sampleinfo.csv",
            show_col_types = FALSE) %>% 
            as.data.frame() %>% 
            mutate_if(is.character, as.factor)

dict <- tribble(
  ~ Variable, ~ Description,
  "sampleID", "The original ID of the sequencing sample obtained",
  "Colony", "The bumblebee colony from which the indiduals sampled came",
  "Treatment", "Control vs. Chill Exposure",
  "Time", "Duration of the treatment"
)

kable(
    dict,
    format = "html",
    caption = "Table 1. Metadata of the gene expression samples.") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

```{r, echo =FALSE}
head(metadata) 
```

##### **Question 1** (TS): Looking at the [original study]("https://drive.google.com/file/d/1X3z251hebpjBVdOXsQtYzDG-SaeCa6M1/view"), method of RNA sequencing was performed? This is an unusual method of sequencing; you might need to look it up. Can you explain, approximately, how it works? (Give your best attempt)

> In the original study, RNA sequencing was performed using 3’ Tag-based RNA sequencing (TagSeq), which targets the 3’ ends of RNA transcripts to assess gene expression levels. This method involves isolating mRNA from the sample, fragmenting it, initiating cDNA synthesis with a specific oligo-dT primer targeting the 3’ end of mRNA molecules, amplifying the resulting cDNA fragments via PCR, and finally sequencing these fragments to focus on the 3’ ends for gene expression analysis.


RNA-sequencing data go through MANY pre-processing steps before they get to this point. This includes:

* Checking the quality of the FASTQ reads that come off the sequencer
* Removing ligated barcodes, if needed
* Performing __alignment__ to the genome to know which __genes__ are which!
* Counting the number of reads that __mapped__ (aligned) to the genome after ignoring any under a particular quality threshold

##### **Question 2** (TS): Looking at the [original study]("https://drive.google.com/file/d/1X3z251hebpjBVdOXsQtYzDG-SaeCa6M1/view") by  briefly describe in 3-4 sentences how the above steps were performed. We will have discussed this some in lecture too if you need help.

> In the original study, the RNA-sequencing data underwent several pre-processing steps. Initially, FastQC assessed the quality of the FASTQ reads. Any ligated barcodes were then removed from the reads. Next, STAR aligned the reads to the B. impatiens genome to determine gene mapping. The number of reads meeting a quality threshold and successfully mapped to the genome was counted to quantify gene expression levels. These steps are crucial for ensuring accurate and reliable downstream gene expression analyses.


## Read Counts

We will be working with the __raw count__ data as __CPM (read counts per million)__ from across all 74 samples to perform a __differential gene expression__ analysis, often referred to as a 'DE' analysis in abbreviation. I will call it a 'DE' analysis going forward and I will refer to __'differentially expressed genes'__ as 'DEGs' or just 'DEG'. 

##### **Question 3** (TS): Explain CPM for someone who might not know what that means. You may need to do a little extra reading on the subject!

> Counts Per Million (CPM) is a normalization method commonly used in RNA sequencing data analysis to quantify gene expression levels across samples. It calculates the expression of a gene as the number of reads mapped to that gene.

Let's read in the raw counts and get a sense of these data.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
counts <- read_csv(file = "../../I. Bioinformatics/Demo/23524047/ECK_CountData_Complete.csv") %>% as.data.frame()
head(counts[,1:6]) 
```

##### **Question 4** (TS): Why is the `geneID` something like, 'LOC100740276'? What does that mean?

> The geneID is ‘LOC100740276' because that is a common format used to label genes in genomic databases. ‘LOC’ stands for locus which refers to a  specific location or position on a chromosome where a gene is located. ‘100740276’ is a unique identifier that distinguishes this gene from others in the database.

##### **Question 5** (TS): For column `COOECK6` gene 'LOC100740276' has a value of 6. What does that mean?

> A value of 6 for the gene 'LOC100740276' in the column 'C00ECK6'  indicates the number of RNA-seq reads that were detected for the gene 'LOC100740276’ in the sample represented by 'C00ECK6'.

# Conventional Method: Differential Expression Analysis using `DESeq2`

The Bioconductor package `DESeq2` is one of several such packages that exist in R, but has rapidly become one of the 'gold standards' for DE analyses. You can find more about how this package is used by looking at the [vignette](http://bioconductor.org/packages/devel/bioc/vignettes/DESeq2/inst/doc/DESeq2.html).

## Pre-processing the counts data and metadata

In short, `DESeq2` reads in the raw counts as a matrix, which must then be converted to a __design matrix__ which includes the sample metadata for each column.  Importantly, matrices should **not** contain the IDs in a column, but should be move to the __row names__ of the matrix instead. 

#### 1. Assign the `geneID` column as the rownames and then remove the `geneID` column from raw counts.
```{r, echo = TRUE}
rownames(counts) <- counts$geneID
counts <- counts[ ,-1]
```

#### 2. Assign the `sampleID` column as the rownames and then remove the `sampleID` column from metadata.
Just as it was picky about the `geneID` being the __row names__ of the counts data, it `DeSeq2` requires the `sampleID` to be the row names of the metadata. 

```{r, echo = TRUE}
rownames(metadata) <- metadata$sampleID
metadata <- metadata[ ,-1]
```

#### 3. Configure the metadata with specific column headings.
`DeSEq2` is very picky about the headings of the metadata columns before you make your design matrix. It is critical that the __columns__ of the count matrix match the __rows__ of the metadata matrix. The order must be identical! If they are not, we need to rearrange them. Note, too, that they must match verbatim.

Let's check that the column names of the `counts` matrix are (1) the same and (2) in the same order as the metadata matrix. Returns `TRUE` for both if these cases are satisfied.

```{r, echo = TRUE, results='asis'}
print(all(colnames(counts) %in% rownames(metadata)))
print(all(colnames(counts) == rownames(metadata)))
```

#### 4. Make sure the levels of the factor(s) of interest in the metadata are using the correct reference level.
There is a fundamental difference in the `character` and `factor` data types, and note that when we read in the original __metadata__ dataframe we converted all character type to factor type. This is to allow us to make sure that our levels are in the order we want, with the reference level listed first. 

Let's start by taking a look at the levels of `Treatment` - by alphanumeric ordering, `Cold` comes before `Control`, so we need to fix that before we make the __design object__ downstream. This is because we want the `Control` condition to be the reference condition!

```{r, results='hide', echo = TRUE}
levels(as.factor(metadata$Treatment))
metadata$Treatment <- factor(metadata$Treatment, levels = c("Control", "Cold"))
```


## Make the `DESeq` Design Object

Once your __raw counts__ and __metadata__ matrices have been formatted correctly, you are ready to make the __design object__. The design object contains your __model formula__ for analysis, which includes both treatment effects and batch effect(s). Notably, the __design object__ controls for 'batch differences' -- these are random effects due to differences between individuals sampled, sequencing runs, and/or RNA quantities extracted before sequencing. All of these things could ultimately affect our ability to __statistically detect__ differences in gene expression, so controlling for batch effects is essential.

##### **Question 6** (TS): Which variable is the 'batch' variable in this analysis and why? Hint: Look at the design formula in the next code chunk! 

> The 'Colony’ variable is the batch variable because it represents a potential source of variation that is unrelated to the treatment or experimental condition being studied.

The design formula is used to estimate the dispersions and to estimate gene expression changes (measured in log-fold) per the model. For future reference, **you should put the variable of interest at the end of the formula** and make sure the control level is the first level of the factor. E.g., in our case, `Treatment` is placed at the end of the formula as it is the variable of interest. `Time` should also be included after `Treatment`; we expect there to be a difference among the durations of chill coma, potentially.

```{r, message=FALSE, warning=FALSE, results='hide'}
dsgnObject <- DESeqDataSetFromMatrix(countData = counts, 
                                     colData = metadata,
                                     design = ~ Colony + Treatment + Time)
dim(dsgnObject)
```

```{r}
print(metadata)



```


Notice that, at the same time that we set the __design formula__, we created the actual __design object__ upon which the work will actually be run. The `DESeqDataSet` is an object class used by `DeSeq2` to store the read counts, metadata, and design formula. 

##### **Question 7** (TS): You may only be used to working with data frames in R, so objects could be a foreign beast. Take a moment to look at the `dsgnObject` we just made. What differences do you notice about the structure vs. a dataframe?

> The differences I noticed about the structure vs. a data frame are that structures are specifically designed for handling RNA-Seq count data and associated metadata in the context of differential expression analysis using DESeq2. Structures contain raw count data, sample metadata, and design formula information required for differential expression analysis. While data frames are a general-purpose data structure used for storing tabular data, suitable for various types of analysis in R. A data frame contains tabular data with columns of potentially different types, suitable for a wide range of analysis tasks.


## Normalization: Variance-Stabilizing Transformation

We currently still have __raw counts__ in the design object, which haven't been normalized in any way. We actually have several options for normalization, including regularized-$log_2$ transformation or __variance stabilizing__ transformation.

Below is a function that runs a variance stabilizing transform (VST). Note that here I am running a parametric transformation with `blind = FALSE`. We do not want the transformations to be blind to the experimental design (treatment) at this stage; we expect large differential expression in some genes. Thus, we need to control for outliers by setting `blind = FALSE`.

Take a moment to look through the function and try to understand everything it does, including the parameters it takes.

#### Figure 1. Effect of parametric variance-stabilizing transformation on gene expression.
```{r, echo = F, warning=FALSE, message=FALSE}
runVST <- function(dsgnObject, blind, fitType, makePlot = TRUE, writeTable = FALSE, writeRData = FALSE) {
  ## Perform the VST
  
  # Check if the fitType is the regularized log:
  if(fitType == "rlog") {
    vsData <- rlog(dsgnObject, blind = blind)
  }
  ## Otherwise:
  else {
    vsData <- varianceStabilizingTransformation(dsgnObject, 
                                              blind = blind, 
                                              fitType = fitType)
  }
  
  if(makePlot == TRUE) {
    # Plot the effect of the VS transform:
    p1 <- meanSdPlot(assay(dsgnObject), plot = F)
    p1 <- p1$gg + ggtitle("Before Variance Stabilization") + 
      scale_fill_gradient(low = "cadetblue", high = "purple") + 
      theme_bw() + theme(legend.position = "bottom")
    p2 <- meanSdPlot(assay(vsData), plot = F)
    p2 <- p2$gg + ggtitle("After Variance Stabilization") + 
      scale_fill_gradient(low = "cadetblue", high = "purple") + 
      theme_bw() + theme(legend.position = "bottom")
    grid.arrange(p1, p2, nrow=1)
  }
  
  if(writeTable == TRUE) {
    # Write the data for future use, if needed:
    write.table(assay(vsData),
              file = "vst.txt",
              sep="\t", 
              quote=F, 
              row.names=T)
  }
  if(writeRData == TRUE) {
    save(vsData, file="vst_all_timepoints.Rdata")
  }
  return(vsData)
}

runVST(dsgnObject, blind = FALSE, fitType = "parametric", makePlot = TRUE, writeTable = FALSE, writeRData = FALSE)

```

##### **Question 8** (TS): What do you notice about the plot on the left (the `dsgnObject` without any transformation) vs. the plot on the right after the variance stabilizing transform? What do you think the VST is doing?

> The plot on the left has a y-axis up to 60,000, indicating that the original data might have had some highly expressed genes with very large counts. The plot on the after the variance stabilizing transform now had a y-axis of 3. The transformation has likely stabilized the variance, resulting in a more consistent distribution of gene expression values across different expression levels.

##### **Question 9** (TS): This might seem like a lot of work. Why? Think back to other analyses where you have needed to normalize your data. What is the reason we do it, and why does it matter so much that we get it 'right' for our data?

> Normalization of gene expression data, as with any biological data, is a crucial step in analysis. Ensuring proper normalization is essential because it directly influences the validity and reliability of subsequent analyses and biological interpretations.  If normalization is not performed properly or if biases are not corrected, it can lead to incorrect conclusions, misinterpretation of results, and unreliable biological insights.

### Digging Deeper: Estimating Gene-Wise Dispersion

DESeq2 uses a specific measure of dispersion ($\alpha$) related to the mean ($\mu$) and variance of the data: $\sigma^2 = \mu + \alpha * \mu^2$. This means that for genes with moderate to high counts, the square root of dispersion ($\alpha$) equals the Coefficient of Variation ($CV = \sigma^2 / \mu$). Thus, a 0.01 dispersion means there is 10% variation around the mean expected across biological replicates.

#### Figure 2. Gene expression variance by means.
```{r, echo=FALSE}
meanCounts <- rowMeans(assay(dsgnObject))      ## Per locus, what is the average expression
varCounts <- apply(assay(dsgnObject), 1, var)  ## Apply the variance function by margin = 1, which is rows

plot(log(varCounts) ~ log(meanCounts), 
     ylab = "Natural-log Variance in Gene Expression", 
     xlab = "Natural-log Mean Expression", 
     main = "\nLog-Log plot of variance by mean for each gene\n should be approximately linear.\n", 
     pch = 16, 
     cex = 0.75)
abline(lm(log(varCounts+0.0001) ~ log((meanCounts+0.0001))), 
       col = "#a8325e", 
       lty = 2, 
       lwd = 2)
```


The relationship between mean and variance should be linear on the log scale, and in gene expression data we predict that for higher means, we can more accurately predict the variance (i.e., it is more "fanned out" at lower means and a tighter linear relationship at higher means). We expect that for low mean counts, the variance estimates have a much larger spread, such that the dispersion estimates will differ much more between genes with small means. When this pattern holds, a variance-stabilizing transformation will help to resolve this issue. This is because the variance stabilizing transformation (VST) functions provided by `DESeq2` attempt to shrink the gene-wise dispersion. 

##### **Question 10** (TS): Does this plot support that a VST is (1) needed, (2) not needed, or (3) likely to be ineffective?

> This plot supports that a VST is needed because of its fan-out pattern and the fact that there is not a tight linear relationship or a random/erratic relationship.

### Other VST Functions 
Above, we set the `fitType = "parametric"` when we ran the initial VST. But is this really the best choice? The first rule in data science is **never just use the defaults.**

Other functions you can try:
- `fitType = "local"`

- `fitType = "mean"`

- `fitType = "rlog"`

##### **Question 11** (MS): Use our custom `runVST()` function, changing out the `fitType` parameter to do tuning. Heuristically, which `fitType` seems to be most appropriate here?
> Your answer here.

```{r}
# <Your code here>
```

## Numbers of DE Genes Based on Fold-Change (Wald Tests)

You're only seeing the tail-end of what is a **very** long process to uncover differential expression. Yet, your work is only just beginning! The first important thing to understand when we are doing __conventional gene expression__ analyses is the concept of __fold-change__. We're going to explore this a little bit here before we test for significant differential expression of the genes.

The `DESeq()` function does two things roughly simultaneously. It:

1. __Performs normalization (median of ratios).__ It does this by correcting for variance in read sequencing depth as well as for inter-library dispersion in counts (for each gene).

2. __Calculates the significance of coefficients with a negative binomial generalized linear model.__

Note that DESeq2 does not actually use the normalized counts but rather uses the raw counts and models the normalization in the negative binomial model. One of the ways that we then make sense of the __magnitude__ of significant differential expression is with the __fold-change__.

### What is log-fold change (LFC)?
__Log-fold change__ refers to the fold-change on a logarithmic scale to indicate a positive or negative change in expression between two conditions. As a reminder, here we will be comparing the __cold-exposed__ treatment to the __control__ treatment. Thus, a positive fold change value correlated with increased expression in cold-exposed vs. control whereas a negative fold change indicates a decrease in expression in the cold-exposed treatment relative to control.

However, this value is typically reported in $log_2$, and our human brains do not do on-the-fly conversions of logarithms very well. Let's make ourselves a little cheatsheet to help with this. For example, a $log_2$-fold change of 2 for any given gene in our study would mean that the expression of that gene is increased in  __cold-exposed__ bees relative to __control__ bees, on average, by a multiplicative factor of $2^2 \approx 4$!

#### Figure 3. Relationship between log-fold and linear change.
```{r, echo =FALSE}
lfc <- c(1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5)
linearChange <- round(2^lfc, 3)
folds <- cbind.data.frame(lfc, linearChange)

folds %>% 
  ggplot(aes(y = linearChange, x = lfc)) +
  geom_col(fill = "cadetblue", color = "black") +
  labs(x = expression(log[2]-fold),
       y = "Equivalent Linear Change") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        panel.border = element_blank()) + 
  scale_x_continuous(breaks = seq(0, 5, 0.5))

kable(
    folds,
    format = "html",
    col.names = c("log2-fold Change", "Linear Equivalent"),
    caption = "Table 2. Cheatsheet of log-fold change with linear scale equivalents.") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

### Wald Tests for significant differential expression between conditions

We have technically performed the contrasts, but we still **do not yet know which genes are differentially expressed!** We will want a standardized way to retrieve DE genes for both the treatments, so we will do hypothesis tests using set significance thresholds. This is a more conservative approach, so you will receive fewer genes than with a post-hoc filtering method, as some papers choose to do. For benchmarking, we will use a conservative method and we can look at the log-fold change to see what thresholds it would correspond to, had we used that method.

This performs a __Wald test__ of the specified contrast with a set significance threshold and log-fold change (LFC) threshold. Here, we are using a threshold of $p = 0.05$ since we are applying a Benjamini-Hochberg adjustment to the p-value. We are also using a series of LFC values rather than a single one; otherwise, __choosing one would be rather arbitrary__ even if it is often done in the scientific literature! This will allow us to see whether we get very different numbers of differentially expressed genes between the treatment conditions for each of the possible log-fold change cutoffs.

#### 1. Do the normalization you selected as most appropriate in Question 11. 
Here I am reading it back in so as not to give away the correct answer! Otherwise, you can just use the `runVST()` function!
```{r, include = TRUE}
#runVST(dsgnObject, ...)
load("../../I. Bioinformatics/Demo/vst_all_timepoints.Rdata")
```

#### 2. Estimate the size factors.
A __size factor__ is effectively a count of how many reads there are in each library, sample, or batch (it can depend on the design). This is to account for those batch-level effects we discussed earlier! In other words, it's kind of like estimating the degree of proportionality or __weights__. But estimating the size factors is more than merely dividing the counts by the total number of reads in each batch. Here, `DESeq` is scaling the read counts using the third quantile of the distribution of read counts for each sample. It then uses that as a scaling factor to ensure that the third quantile is the __same across all samples__.

```{r, echo=TRUE}
dsgnObject <- estimateSizeFactors(dsgnObject)        ## Yes, we are overwriting our object after scaling for convenience.
# dsgnObject@colData$sizeFactor                      ## If we wanted to see the resulting size factors employed for each sample
```

Let's examine how well the scaling/normalization has worked across all of the samples:

#### Figure 4. Effect of scaling / normalization across all 74 samples.
```{r, fig.width=8, fig.height=8, echo = FALSE}
temp <- metadata %>% 
  mutate(sampleID = rownames(metadata))

normalized_counts_long <- assay(vsData) %>% 
  data.frame() %>%
  mutate(Locus = rownames(assay(vsData))) %>% 
  pivot_longer(cols = -75, values_to = "Variance-Stabilized Expression", names_to = "sampleID")  %>% 
  full_join(temp, by = "sampleID") 

ggplot(normalized_counts_long, aes(x = sampleID, y = `Variance-Stabilized Expression`, color = Treatment)) +
  geom_boxplot() + 
  theme_classic() + 
  coord_flip() + 
  geom_hline(aes(yintercept = median(assay(vsData))), col="black", show.legend = T) +
  scale_color_manual(values=c("cadetblue", "#a8325e"))
```

#### 3. Let's call significantly DE genes!
```{r, echo = TRUE, message=FALSE, warning=FALSE}
alpha <- 0.05                                                         ## Setting this for a False Discovery Rate of 5%
dispObject <- estimateDispersions(dsgnObject)                         ## Estimate the dispersions
waldObject <- nbinomWaldTest(dispObject)                              ## Use that to perform the negative binomial Wald tests
resultsDESeq <- results(waldObject,
                        alpha = alpha, 
                        pAdjustMethod = "BH")                         ## Uses Benjamini-Hochberg / FDR adjusted p-values
summary(resultsDESeq)
#save(resultsDESeq, file = "DESeqResults.Rdata")                       ## Save the results
```

##### **Question 12** (MS): You know what a p-value is but you may not be familiar with an __adjusted p-value__. What is it? Specifically, what's a __False Discovery Rate__? 
> Your answer here.

##### **Question 13** (MS): Why is it critical we employ a p-value adjustment here, like the FDR?
> Your answer here.

##### **Question 14** (MS): How many differentially expressed genes did we find with the __Wald test__?
> Your answer here.

##### **Question 15** (MS): Looking at the Verble et al. (2023) study, how do our results differ from their `DESeq2` results and why? 
> Your answer here.

##### Hint 1: What is different about our __design formula__ versus what they describe verbally? 
##### Hint 2: Look at what we did with `Time`!

# Exploratory Data Analysis

After all that hard work, it feels pretty disappointing to only find **2 genes differentially expressed** between cold-exposed and control treatments across all the time points. Let's start to visually explore the data to see if there is enough here to apply a machine-learning approach to help us out. After all, __Wald tests__ are very conservative! They are also not going to do as good a job at cutting through noise, regardless of scaling & normalization. 

## Summary statistics on the raw gene (feature) counts
Let's first go back to the raw data to see if there's anything diagnostic here.

```{r, echo = FALSE}
countsSummStats <- function(df, meta) {
  header <- c("Total Genes", 
              "Number Genes with >1 read count", 
              "Proportion with Nonzero Expression", 
              "Proportion with Low Expression", 
              "Number Reads: Cold-Expossed Treatment", 
              "Number Reads: Control Treatment", 
              "Total Reads Mapped", 
              "N Samples", 
              "Mean CPM per Sample")
  
  totalGenes <- nrow(df)          ## Total genes that were in the genome used for mapping
  totalCounts <- rowSums(df)
  ## Number of genes with at least 1 count:
  numNonzeroExp <- length(totalCounts[totalCounts > 0])
  propNonzeroExp <- numNonzeroExp / totalGenes
  ## How about how many genes where there are fewer than 10 reads in 90% of samples?
  lowExpression <- length(totalCounts[totalCounts < 10*ncol(df)*0.9]) 
  propLowExp <- sum(lowExpression) / totalGenes

  ## Read count by treatment
  A <- meta %>% 
    filter(Treatment == "Cold") %>% 
    rownames()
  countA <- df %>% 
    select(any_of(A)) %>% 
    sum()

  B <- meta %>% 
    filter(Treatment == "Control") %>% 
    rownames()
  countB <- df %>% 
    select(any_of(B)) %>% 
    sum()

  totalReads <- sum(df)       # Total read count
  samples <- ncol(df)         # Number of samples
  
  summStats <- as.data.frame(rbind(sprintf("%1.0f", totalGenes), 
                                   sprintf("%1.0f", numNonzeroExp), 
                                   sprintf("%1.4f", propNonzeroExp), 
                                   sprintf("%1.4f", propLowExp), 
                                   sprintf("%1.0f", countA), 
                                   sprintf("%1.0f", countB), 
                                   sprintf("%1.0f", totalReads), 
                                   sprintf("%1.0f", samples), 
                                   sprintf("%1.2f", totalReads/samples/10^6)))
  rownames(summStats) <- header
  names(summStats) <- "All Samples" 

  return(summStats)
}

summaryStatsDF <- countsSummStats(counts, metadata) 

summaryStatsDF %>% 
kable(
    format = "html",
    caption = "Table 3. Summary statistics of the raw gene expression counts") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

##### **Question 16** (MS): Look at the custom function I've written for generating the summary statistics. How is __low expression__ defined? 
> Your answer here.



We have nearly 40% low expression by this definition. That seems high -- but it would also make sense because these bees were put into a __chill coma__ (which means their physiological processes might have shut down). In other words, the low expression might be a byproduct of the type of treatment that was applied, and could be really interesting in and of itself. Conventional methods typically __remove lowly expressed genes__, but we are doing our best at this stage to preserve all the data.


##### **Question 17** (MS): Given that it might be indicative of the study itself, do you think it could be **ill-advised** in this case to filter for low expression?
> Your answer here.


## Heatmap of gene expression across treatments & time points
First, we will calculate the sample distances using a Pearson's pairwise correlation coefficient as you have typically done in your EDA to date. The `cor()` function can be applied to the matrix of normalized gene expression values and then we can make a heatmap using the `pheatmap` package. Notice that we are also applying __hierarchical clustering__, which means that we are clustering the samples by correlation coefficient. Note that to get the matrix of numeric values for input, we extract them from the VST object containing our normalized values using the `assay()` function.

#### Figure 5. Gene expression correlation among the samples with hierarchical clustering.
```{r, fig.width = 6, fig.height = 8.5, echo = FALSE}
## Compute  pairwise correlation values for samples:
pwCorr <- cor(assay(vsData))
## Let's rename the samples to something more meaningful to us using treatment and time
rownames(pwCorr) <- paste(vsData$Treatment, vsData$Time, sep="-" )
colnames(pwCorr) <- NULL        ## No column names, thanks.

## Let's set a monochromatic color palette
colors <- colorRampPalette(brewer.pal(9, "Purples"))(255)

## Let's make our heatmap!
pheatmap(pwCorr, 
         color = colors)
```

##### **Question 18** (MS): What do you notice about the arrangement of the treatments and time points (or lack thereof)?
> Your answer here.


##### **Question 19** (MS): Complete this code chunk to calculate the sample distances instead. You will use the `dist()`, which calculates the Euclidean distance between samples but to do it we must __transpose__ the matrix first with the `t()` function. This is actually somewhat similar to what is happening in a kMeans clustering algorithm. Does your assessment about the arrangement of the treatments and time points change any?
> Your answer here.

Hint: Make sure to update your figure caption!
```{r, fig.width = 6, fig.height = 8.5, echo = TRUE}
## Calculate the sample distances; fill in the blank and take off the comment to run
# sampleDistances <- dist(t(____)))     ## Hint: what did we do to extract a matrix from the VST object earlier?

## Make the heatmap of the sample distances; fill in any blanks and then take the comments off to run!
# sampleDistMatrix <- as.matrix(sampleDistances)

## Let's label by treatment and time again
# rownames(sampleDistMatrix) <- paste(vsData$Treatment, vsData$Time, sep="-" )
# colnames(sampleDistMatrix) <- NULL

## Let's set a monochromatic color palette
# colors <- colorRampPalette(rev(brewer.pal(9, "Purples")))(255)

# pheatmap(___,
         # clustering_distance_rows = sampleDistances,
         # clustering_distance_cols = sampleDistances,
         # col = colors)
```


## Principal Component Analysis (PCA)
PCA is a very commonly performed analysis in gene expression studies because it allows us visualize how well our different samples and conditions cluster together. We can also use PCA to identify which genes are likely to be more important to explain those differences, although we have not employed that here. Most importantly, given how **noisy the heatmap looks**, it would be nice to see if we can 'cut through the noise' with any other methods!

Notice that we are using a `plotPCA()` function which has been designed to work with gene expression data rather than `princomp()` or `prcomp`. 

##### **Question 20** (MS): Do you think we could still use `princomp()` or `prcomp`?
> Your answer here.

#### Figure 7. The first two principal components for the difference in gene expression across all samples, conditions, and time points.
```{r, echo = FALSE, warning=FALSE, message=FALSE, fig.width = 8}
#Create a PCA data frame
pca <- plotPCA(vsData, 
               intgroup = c("Treatment", "Colony", "Time"), 
               returnData = TRUE,
               ntop = 500)
percentVar <- round(100 * attr(pca, "percentVar"))

#Plot the PCA with the % variance attributable to PC1 and PC2
ggplot(pca, aes(PC1, PC2, color = Treatment, shape = Colony)) +
  geom_point(size=3, alpha = 0.85) +
  scale_color_manual(values=c("cadetblue", "#a8325e")) +
  labs(x = paste0("PC1: ",percentVar[1], "% variance"), 
       y = paste0("PC2: ",percentVar[2], "% variance"),
       title = "PCA of Gene Expression by Treatment + Colony + Time") +
  xlim(-30,30) +
  ylim(-15, 15) +
  theme_bw() +
  theme(axis.text = element_text(size = 13),
        legend.position = "right") +
  facet_wrap(~Time, nrow = 1)
```

##### **Question 21** (RC): What percent of the variance is attributable to PC1 and PC2? At which time point(s) do you see the most separation between the treatments? Is this consistent with what Verble et al. (2023) found?

> The first principal component can explain 39% of the variance and the second principal component can explain 11% of the variance. You see the most seperation at the 30 minute interval since there are no red or green outlier points on their opposign ends. I think this is not consistent with what Verble et al. (2023) found. In their article on page 7 they said that the variance was substantially different among time points unlike in our plot where they are more similiar.

##### **Question 22** (RC): Think about what you know about Support Vector Machines (we will review this in lecture a bit too). Do you see anything in these PCA plots that make you think that SVM might do a much better job of finding the boundary between the treatments?

> I think tha margins would help with classification. On most of the plots if you were to draw a line of best fit to determine the two treatments you would still have some missclassifications but having a hard margin would help to fix this.


### Volcano Plots

These are the most __canonical plots__ (along with clustered heatmaps) for visualizing the results of gene expression analyses. On the y-axis is the $-log_{10}$ p-value and the x-axis is the $log_2$-fold change. The purple lines show where $log_2$-fold change between the treatment conditions is $\geq 2$, which is would mean that expression is $2^2$ times greater in one condition over the other in linear change. Note that I have used `ggrepel()` to annotate as many of the points as possible with a $log_2$-fold change of greater than 2. 

#### Figure 8. Volcano plot of gene expression between cold-exposed and control bumblebees.
```{r, warning = FALSE, echo = FALSE, message = FALSE, fig.height=6, fig.width=8}
res <- resultsDESeq %>% 
  as.data.frame() %>% 
  mutate(DEG = ifelse(log2FoldChange > 0 & padj < 0.05, "Up DEG",
                       ifelse(log2FoldChange < 0 & padj < 0.05, "Down DEG", "N.S."))) %>% 
  drop_na()    ## We have to drop the NAs for Volcano plots, unfortunately!

# Next, we would like to annotate anything with greater then 2 log-fold change in expression:
res <- res %>% 
  mutate(locus = ifelse(abs(log2FoldChange) > 3, rownames(res), ""))

res %>% 
  ggplot(aes(x = log2FoldChange, y = -log10(pvalue), label=locus, color = DEG)) + 
  geom_point(alpha = 0.85, size = 1.5) +
  scale_color_manual(values=c("cadetblue", "gray", "#a8325e")) +
  geom_text_repel(show.legend = FALSE) +
  labs(y = expression(paste(-log[10], " p-value")),
       x = expression(paste(log[2], "-fold change"))) +
  theme_classic() +
  theme(axis.text = element_text(size = 12), 
        axis.title = element_text(size = 12),
        title = element_text(size = 15),
        legend.text = element_text(size = 12)) +
  geom_vline(xintercept = c(-2, 2), col="purple")                      ## Log-fold change of 2 times
#  geom_hline(yintercept = -log10(0.05), col="darkorange")             ## Corresponds to a p-value cutoff of 0.05; original not padj though, do not display
```


##### **Question 23** (RC): Is there anything you notice from the volcano plot? Does anything about this support that a machine learning approach might perform better?

> I think the main thing from interpting the plot is that most of the points do not meet the significance threshold, however from this plot we can see 5 samples that are significant with two of them being outliers. I do not think this suppoerts a better machine learning approach.

\newpage

# Machine Learning Analyses 
Thus far, you have probably been exposed to the __kitchen sink__ model of algorithm selection. That's pretty common. In some fields, however -- like biology and biomedicine -- folks can sometimes be a little more hesitant to just 'throw anything and everything' at their data. Why?

* It contrasts fundamentally with how biologists think about performing __"science"__.
* They want clear, methodologies that they can understand (transparency is key).
* The statistics involved were not taught to them.
* There is a very real (and sometimes justified) fear over [p-hacking]("https://royalsocietypublishing.org/doi/10.1098/rsos.220346").
* Every good analysis requires clearly justifed choices.

All of this is to say that there is nothing **wrong** with the conventional approaches -- when they work! I've published several times using `DESeq2` and I will continue to do so. So, why would I even consider switching?

One word: **NOISE**.


## Random Forest

Random Forest (RF) is a popular supervised learning algorithm, or learner, with few assumptions about the relationships among variables. RF is well-suited to capture complex interactions such as those commonly seen in biological systems, and can run either a regression or classification. 

It is based on the binary decision tree, which progressively splits the samples into two child nodes that maximize how much of the variance in the dependent variable is explained by the predictors. However, single decision trees tend to over fit the data, i.e., **they tend to generate much better predictions on the training set than on the test set**. Thus, RF uses an ensemble of trees (a forest!) to counteract this overfitting (although note that it NO algorithm is robust to overfitting!). Each tree in the forest uses a random sample of the observations, while only a subset of all features (columns) is assessed for each node split. 

Although sometimes called a 'black box' algorith, RF is more explainable than other deep-learning methods. One thing RF can give you is a scoring of features according to their influence; this is a measure of how __important__ the feature was in generating the trees in the forest. The result is a ranked list of features that were important for predicting the outcome. 

RF is being used increasingly with gene expression studies because, as an ensemble learning algorithm, it is thought to be useful in what is often referred to as the "large $P$ small $n$" paradigm, where $P$ is the number of predictors and $n$ is the number of observations. You may not fully realize it, but we have just such a $P >> n$ problem here: $n$ is actually the number of __samples__ whereas $P$ is the number of genes!  

So, let's see how a Random Forest performs out-of-the-box (OOB) on our data with minimal pre-processing, filtering, and no feature selection.


#### 1. Pre-processing the data. 
This time through, we are doing very minimal pre-processing. We must first extract and transpose the VST data that we got out of `DESeq2`. 
```{r, echo=TRUE}
## Extract the VST data
tVSdata <- t(assay(vsData))

# We do have gene names, e.g., TRNAV-UAC that are malformed for ranger and randomForest. We will fix that before proceeding:
for (c in 1:ncol(tVSdata)) {
  colName <- colnames(tVSdata)[c]
  colName <- gsub("-", "_", colName)
  colName -> colnames(tVSdata)[c]
}
```

#### 2. We do need to put our other features (Colony, Time) and our outcome (Treatment) into the transposed matrix.
Make sure to put Treatment as the last column; that will be useful downstream in the analysis pipeline.
```{r, echo=TRUE}
df1 <- cbind(colData(dsgnObject)[1], colData(dsgnObject)[3], colData(dsgnObject)[2])       ## We don't need the size factors
tVSdata <- merge(tVSdata, df1, by = "row.names")

## The merge turns back into a dataframe and removes the sample names from the rows; let's put them back:
rownames(tVSdata) <- tVSdata[,1]
tVSdata <- tVSdata[,-1]
```

#### 3. Create the test-train partitions using the `createDataPartition()` function from the `caret` package. This allows us to split with respect to the response variable, Treatment, to ensure that it is even distributed throughout the test-train split.
You will see that, with so few samples, I am opting for a 80%-20% split. 

#### Figure 9. Distribution of samples between the training and testing sets.
```{r, echo = FALSE, message = FALSE, warning = FALSE}
## Make the train and test partitions:
ind <- createDataPartition(y = tVSdata[, c("Treatment")],     ## Treatment is evenly distributed
                           p = 0.8,                                    ## % into training
                           list = FALSE)                               ## don't return a list
train <- tVSdata[ind, ]
test <- tVSdata[-ind,]

## Check the distribution of the samples:
tab1 <- train %>% 
  group_by(Treatment, Time, Colony) %>% 
  summarise(Proportion = n()/nrow(train))
#tab1
tab1 %>% 
  ggplot(aes(y = Proportion, x = Treatment, fill = Colony)) +
  geom_col(color = "black") +
  scale_fill_manual(values=c("cadetblue", "#a8325e", "goldenrod")) +
  ggtitle("Training Set") +
  facet_wrap(~Time) + 
  theme_bw()

tab2 <- test %>% 
  group_by(Treatment, Time, Colony) %>% 
  summarise(Proportion = n()/nrow(test))
#tab2
tab2 %>% 
  ggplot(aes(y = Proportion, x = Treatment, fill = Colony)) +
  geom_col(color = "black") +
  scale_fill_manual(values=c("cadetblue", "#a8325e", "goldenrod")) +
  ggtitle("Testing Set") +
  facet_wrap(~Time) + 
  theme_bw()
```

##### **Question 24** (RC): How many features (genes) are in the train and test data sets? Do you notice anything little concerning about the train and test splits. Does adjusting the split seem to make it better, worse, or stay the same?

> There are 3 genes in the training set at each time interval, each interval has all three genes: dahila, daisy, and violets. The testing set has all three as well, but only in two intervals the 120 and 720m intervals. For the rest many of them only have one or two genes for each treatment mainly being dasy or violet. 


#### 4. Fitting an initial Random Forest model. 
Let's take a look at the out-of-box (OOB) performance. **This may take your computer a minute (or three) to run. Be patient.**
```{r, echo = TRUE}
rfOOB <- randomForest::randomForest(
  Treatment ~ ., 
  data = train)

rfOOB$confusion %>% 
  kable(
    format = "html",
    caption = "Table 4. Results of the OOB Random Forest") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F))
```
This is the OOB classification error. 

#### Figure 10. Result of out-of-box (OOB) random forest classifier on cold-exposed vs. control changes in gene expression.
```{r, echo = FALSE, fig.height = 6}
plot(rfOOB, 
     main = "OOB Random Forest", 
     lwd = 2)
```

##### **Question 25** (RC): Based on the plot, did the OOB manage to find complete separation between cold-exposed and control treatments?

> Yes OOB was abke to find complete seperation betwee the cold-exposed and control treatment after about 300 trees. Before then it was having trouble but after it made three distinct lines.

Let's look at the results of the training set when tested against the data (called __training error__):
```{r, echo = FALSE}
pred.train.rf <- predict(rfOOB, train, type = 'response')
confMat <- confusionMatrix(train$Treatment,
                pred.train.rf)

overall <- round(confMat$overall, 3)
data.frame(overall) %>% kable(
    format = "html",
    caption = "Table 5. OOB RF Train - Results of Confusion Matrix") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

Now let's look at the results of the training set when tested against the test set (a.k.a., __test error__ from which we derive **accuracy**):
```{r, echo = FALSE}
pred.test.rf <- predict(rfOOB, test, type = "response")
confMat <- confusionMatrix(pred.test.rf, test$Treatment)

overall <- round(confMat$overall, 3)
data.frame(overall) %>% kable(
    format = "html",
    caption = "Table 6. OOB RF Test - Results of Confusion Matrix") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

Lastly, let's extract the top __important features__:
```{r, echo=FALSE, message = FALSE, warning = FALSE}
importantRF <- rfOOB$importance     ## Store the important genes!

importantRF %>% 
  as.data.frame() %>% 
  arrange(desc(MeanDecreaseGini)) %>% 
  top_n(10) %>% 
  kable(
    format = "html",
    caption = "Table 7. Top 10 important genes identiftied by the OOB Random Forest") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F))
```


##### **Question 26** (RC): How did the OOB model perform? Which prediction (training or testing) had the highest accuracy?

> The OOB model had a 71% accuracy when predicting the testing set. The training set would have the higher accuracy, since the model was built on that data it has all the answers when prediciting the training set.

##### **Question 27** (RC): Why is it not a good idea to look at just the accuracy?

> Only looking at the accuracy is generally not a good idea because it doesn't account for balance between different classes. It also makes the model biased towards one side. Accuracy alone may not reflect how well the model does with new data. It's important to consider other metrics like precision, or recall, which give a better metric for  evaluation of the model's effectiveness.

##### **Question 28** (RC): How many of the important features (genes) overlap with the ones from the `DESeq2` analysis with log-fold change in expression > 1? **You must write a function to solve this.** Hint: Your function will be most flexible if it takes two parameters: (1) a log-fold change to cut off against in the `DESeq` results, and (2) a list of important genes from RF or another machine learning. Trust me, a function will make your life SO much easier!!! 

```{r, echo = FALSE}
findOverlappingGenes <- function(cutoff) {
 res <- resultsDESeq %>% 
  as.data.frame() %>% 
  mutate(DEG = ifelse(log2FoldChange > cutoff & padj < 0.05, "Up DEG",
                       ifelse(log2FoldChange < cutoff & padj < 0.05, "Down DEG", "N.S.")))%>% 
  drop_na()    ## We have to drop the NAs for Volcano plots, unfortunately!

# Next, we would like to annotate anything with greater then 2 log-fold change in expression:
res <- res %>% 
  mutate(locus = ifelse(abs(log2FoldChange) > 2, rownames(res), ""))

res %>% 
  ggplot(aes(x = log2FoldChange, y = -log10(pvalue), label=locus, color = DEG)) + 
  geom_point(alpha = 0.85, size = 1.5) +
  scale_color_manual(values=c("#28B463", "gray", "#FFB6C1")) +
  geom_text_repel(show.legend = FALSE) +
  labs(y = expression(paste(-log[10], " p-value")),
       x = expression(paste(log[2], "-fold change"))) +
  theme_classic() +
  theme(axis.text = element_text(size = 12), 
        axis.title = element_text(size = 12),
        title = element_text(size = 15),
        legend.text = element_text(size = 12)) +
  geom_vline(xintercept = c(-2, 2), col="blue")        
}


```


Although the OOB accuracy isn't the worst ever, especially considering how poorly `DESeq2` performed, it still looks like we may have quite a lot of overfitting and we probably aren't getting a reliable list of important genes. We fix overfitting of a model with __cross-validation__. We are going to come back to this for **PROJECT 1**.


## Support Vector Machines (SVM)

The other machine learning algorithm we are going to apply to the gene expression data is one that has been used for quite a long time now and is becoming quite conventional in its own right: SVM, or support vector machines. This algorithm is especially handy for gene expression data because it can also perform a regression or classification and it does not require (although it can handle) the boundary between discrete classes to be linear. 

Just like with RF, we will not pre-process the data heavily or filter it in any way just to get a sense of the OOB performance. Then, in **Project 1** we will continue this work.

#### 1. Pre-process the datasets
Normally, data for an SVM **must** be scaled; but we have already fed it scaled and normalized data! So we have no need to do that pre-processing here.

We also cannot give an SVM an ordered factor, so we need to fix that before moving forward. We are going to turn the ordered factors (Treatment, Colony, and Time) into unordered factors and then re-call our train-test split.

```{r, echo = TRUE}
tVSdata <- tVSdata %>% 
  mutate_if(is.ordered, factor, ordered = FALSE)

train <- tVSdata[ind, ]
test <- tVSdata[-ind, ]
```

#### 2. Run the OOB (default) SVM.
As with RF, be patient and let it run, if needed. Mine only takes ~ 20 seconds, though!
```{r, echo=TRUE, warning=FALSE, message=FALSE}
svmOOB <- svm(Treatment ~ ., 
  data = train,
  kernel = "linear",
  na.action = na.omit
)
```

```{r, echo = FALSE}
paste0("The total number of support vectors was: ", svmOOB$tot.nSV)
```
```{r, echo=TRUE, warning=FALSE, message=FALSE}
svmOOB <- svm(Treatment ~ ., 
  data = train,
  kernel = "radial",
  na.action = na.omit
)
```

```{r, echo = FALSE}
paste0("The total number of support vectors was: ", svmOOB$tot.nSV)
```

```{r, echo=TRUE, warning=FALSE, message=FALSE}
svmOOB <- svm(Treatment ~ ., 
  data = train,
  kernel = "sigmoid",
  na.action = na.omit
)
```

```{r, echo = FALSE}
paste0("The total number of support vectors was: ", svmOOB$tot.nSV)
```

#### 3. Find the __training error__.

```{r, echo = FALSE}
pred.train.svm <- predict(svmOOB, train, type = 'response')
confMat <- confusionMatrix(train$Treatment,
                pred.train.svm)

overall <- round(confMat$overall, 3)
data.frame(overall) %>% kable(
    format = "html",
    caption = "Table 8. OOB SVM Train - Results of Confusion Matrix") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

#### 4. Find the test accuracy.

```{r, echo = FALSE}
pred.test.svm <- predict(svmOOB, test, type = "response")
confMat <- confusionMatrix(pred.test.svm, test$Treatment)

overall <- round(confMat$overall, 3)
data.frame(overall) %>% kable(
    format = "html",
    caption = "Table 9. OOB SVM Test - Results of Confusion Matrix") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
)
```

#### 5. Extract the top __important features__.
Unlike RF, the `svm()` function in `e1071` does not estimate the importance of the predictors for you. However, we can use a different function, `mt.teststat` from the 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
labels <- as.integer(train$Treatment) - 1

tTrain <- train %>% 
  select(-Colony, -Time, -Treatment) %>%
  t() %>% 
  as.matrix()

tscores <- mt.teststat(tTrain, 
                       labels, 
                       test = "t")
```

## Store the important genes!

```{r, echo = FALSE}
geneID <- rownames(tTrain)
importantsvm_1 <- data.frame(geneID, tscores)

importantsvm_1 %>% 
  arrange(desc(tscores)) %>% 
  top_n(10) %>% 
  kable(
    format = "html",
    caption = "Table 10. Top 10 important genes identiftied by the OOB SVM") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F))
```


##### **Question 29** (RC): Try quickly repeating these analyses with `kernel = "radial"` as well as `kernel = "sigmoid"` in Step #2 above. Follow through with your predictions - which kernel yields the lowest training error and the highest test accuracy?

> The linear kernel had the highest accuracy of have 100% accuracy for the training and 78% accuracy for the testing. It also has the lowest training error.

##### **Question 30** (RC): Use your function from question 28 - do you see an overlap between the `DESeq2` genes with greater than 2 log-fold expression? 
```{r}
findOverlappingGenes(1)
```

> Yes there is some over lap with the greater then 2 log-fold expression 4 out of the 5 samples that are significan are shared between the two models

##### **Question 31** (RC): At this point, which model is performing better on the gene expression data and why? 

> From all the analysis between the random OOB forest, the SVM and then DEseq2 model, it appears the the svm model is the most accurate. When evaulating the model with the training data both had 100% accuracy, but when looking at the testing data the SVM had an accuracy of 78% while the OOB RF had an accuracy 71%. I think the SVM does the best because of the margins it has along its regression line. Margins are good with SVM because they help make a better boundary when making classification decisons and reduce overfitting.

# Final Comments
At this stage, it should feel clear that we need to do feature selection. If not, stop and reflect on that statement. We know there is a lot of __noise__ in the data, and we have worked hard to preserve all of the observations and columns. However, we know that feature selection and tuning can both have DRASTIC effects on the performance of our models. Which one will your team choose to tune for **Project 1**?

**N.B.:** Did you find this module fun or interesting? Would you like to work with genes and gene expression data more deeply? Interested in a research opportunity? Talk to Dr. Geist about opportunities to work on and potentially publish benchmarking studies like this!

# References

